/opt/conda/envs/layoutlmv3/lib/python3.7/site-packages/torch/distributed/launch.py:188: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  FutureWarning,
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Namespace(batch_size=12, datasize=None, device='cuda', dist_url='env://', input_file='/nlp_group/wuxing/yuhuimu/AB-layoutlmv3/data/data_processed_ori/', learning_rate=1e-05, max_epochs=20, model_name='microsoft/layoutlmv3-base-chinese', model_params=None, output_model_dir='/nlp_group/wuxing/yuhuimu/AB-layoutlmv3/output/pretrain_wpa_lr1e-5_bs_12/', pretrained='/nlp_group/wuxing/yuhuimu/AB-layoutlmv3/pytorch_model.bin', ratio_train=0.9, seed=42, tokenizer_vocab_dir='/nlp_group/wuxing/yuhuimu/AB-layoutlmv3/data/tokenizer/', world_size=1)
Namespace(batch_size=12, datasize=None, device='cuda', dist_url='env://', input_file='/nlp_group/wuxing/yuhuimu/AB-layoutlmv3/data/data_processed_ori/', learning_rate=1e-05, max_epochs=20, model_name='microsoft/layoutlmv3-base-chinese', model_params=None, output_model_dir='/nlp_group/wuxing/yuhuimu/AB-layoutlmv3/output/pretrain_wpa_lr1e-5_bs_12/', pretrained='/nlp_group/wuxing/yuhuimu/AB-layoutlmv3/pytorch_model.bin', ratio_train=0.9, seed=42, tokenizer_vocab_dir='/nlp_group/wuxing/yuhuimu/AB-layoutlmv3/data/tokenizer/', world_size=1)
Namespace(batch_size=12, datasize=None, device='cuda', dist_url='env://', input_file='/nlp_group/wuxing/yuhuimu/AB-layoutlmv3/data/data_processed_ori/', learning_rate=1e-05, max_epochs=20, model_name='microsoft/layoutlmv3-base-chinese', model_params=None, output_model_dir='/nlp_group/wuxing/yuhuimu/AB-layoutlmv3/output/pretrain_wpa_lr1e-5_bs_12/', pretrained='/nlp_group/wuxing/yuhuimu/AB-layoutlmv3/pytorch_model.bin', ratio_train=0.9, seed=42, tokenizer_vocab_dir='/nlp_group/wuxing/yuhuimu/AB-layoutlmv3/data/tokenizer/', world_size=1)
Namespace(batch_size=12, datasize=None, device='cuda', dist_url='env://', input_file='/nlp_group/wuxing/yuhuimu/AB-layoutlmv3/data/data_processed_ori/', learning_rate=1e-05, max_epochs=20, model_name='microsoft/layoutlmv3-base-chinese', model_params=None, output_model_dir='/nlp_group/wuxing/yuhuimu/AB-layoutlmv3/output/pretrain_wpa_lr1e-5_bs_12/', pretrained='/nlp_group/wuxing/yuhuimu/AB-layoutlmv3/pytorch_model.bin', ratio_train=0.9, seed=42, tokenizer_vocab_dir='/nlp_group/wuxing/yuhuimu/AB-layoutlmv3/data/tokenizer/', world_size=1)
| distributed init (rank 2, word 4): env://
| distributed init (rank 3, word 4): env://
| distributed init (rank 0, word 4): env://
| distributed init (rank 1, word 4): env://
ai-platform-wlf1-ge10-1:35105:35105 [0] NCCL INFO Bootstrap : Using eth01:10.80.205.179<0>
ai-platform-wlf1-ge10-1:35105:35105 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
ai-platform-wlf1-ge10-1:35105:35105 [0] NCCL INFO cudaDriverVersion 11040
NCCL version 2.14.3+cuda11.7
ai-platform-wlf1-ge10-1:35106:35106 [1] NCCL INFO cudaDriverVersion 11040
ai-platform-wlf1-ge10-1:35109:35109 [3] NCCL INFO cudaDriverVersion 11040
ai-platform-wlf1-ge10-1:35107:35107 [2] NCCL INFO cudaDriverVersion 11040
ai-platform-wlf1-ge10-1:35105:35142 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
ai-platform-wlf1-ge10-1:35105:35142 [0] NCCL INFO Failed to open libibverbs.so[.1]
ai-platform-wlf1-ge10-1:35106:35106 [1] NCCL INFO Bootstrap : Using eth01:10.80.205.179<0>
ai-platform-wlf1-ge10-1:35107:35107 [2] NCCL INFO Bootstrap : Using eth01:10.80.205.179<0>
ai-platform-wlf1-ge10-1:35109:35109 [3] NCCL INFO Bootstrap : Using eth01:10.80.205.179<0>
ai-platform-wlf1-ge10-1:35106:35106 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
ai-platform-wlf1-ge10-1:35107:35107 [2] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
ai-platform-wlf1-ge10-1:35109:35109 [3] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
ai-platform-wlf1-ge10-1:35105:35142 [0] NCCL INFO NET/Socket : Using [0]eth01:10.80.205.179<0>
ai-platform-wlf1-ge10-1:35105:35142 [0] NCCL INFO Using network Socket
ai-platform-wlf1-ge10-1:35107:35143 [2] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
ai-platform-wlf1-ge10-1:35109:35144 [3] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
ai-platform-wlf1-ge10-1:35107:35143 [2] NCCL INFO Failed to open libibverbs.so[.1]
ai-platform-wlf1-ge10-1:35106:35145 [1] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
ai-platform-wlf1-ge10-1:35109:35144 [3] NCCL INFO Failed to open libibverbs.so[.1]
ai-platform-wlf1-ge10-1:35106:35145 [1] NCCL INFO Failed to open libibverbs.so[.1]
ai-platform-wlf1-ge10-1:35107:35143 [2] NCCL INFO NET/Socket : Using [0]eth01:10.80.205.179<0>
ai-platform-wlf1-ge10-1:35107:35143 [2] NCCL INFO Using network Socket
ai-platform-wlf1-ge10-1:35109:35144 [3] NCCL INFO NET/Socket : Using [0]eth01:10.80.205.179<0>
ai-platform-wlf1-ge10-1:35109:35144 [3] NCCL INFO Using network Socket
ai-platform-wlf1-ge10-1:35106:35145 [1] NCCL INFO NET/Socket : Using [0]eth01:10.80.205.179<0>
ai-platform-wlf1-ge10-1:35106:35145 [1] NCCL INFO Using network Socket
ai-platform-wlf1-ge10-1:35107:35143 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:35107:35143 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:35107:35143 [2] NCCL INFO Could not enable P2P between dev 1(=24000) and dev 0(=1000)
ai-platform-wlf1-ge10-1:35107:35143 [2] NCCL INFO Could not enable P2P between dev 2(=81000) and dev 0(=1000)
ai-platform-wlf1-ge10-1:35107:35143 [2] NCCL INFO Could not enable P2P between dev 3(=e1000) and dev 0(=1000)
ai-platform-wlf1-ge10-1:35107:35143 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:35107:35143 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:35107:35143 [2] NCCL INFO Could not enable P2P between dev 0(=1000) and dev 1(=24000)
ai-platform-wlf1-ge10-1:35107:35143 [2] NCCL INFO Could not enable P2P between dev 2(=81000) and dev 1(=24000)
ai-platform-wlf1-ge10-1:35107:35143 [2] NCCL INFO Could not enable P2P between dev 3(=e1000) and dev 1(=24000)
ai-platform-wlf1-ge10-1:35107:35143 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:35107:35143 [2] NCCL INFO Could not enable P2P between dev 0(=1000) and dev 2(=81000)
ai-platform-wlf1-ge10-1:35107:35143 [2] NCCL INFO Could not enable P2P between dev 1(=24000) and dev 2(=81000)
ai-platform-wlf1-ge10-1:35107:35143 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:35107:35143 [2] NCCL INFO Could not enable P2P between dev 3(=e1000) and dev 2(=81000)
ai-platform-wlf1-ge10-1:35107:35143 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:35107:35143 [2] NCCL INFO Could not enable P2P between dev 0(=1000) and dev 3(=e1000)
ai-platform-wlf1-ge10-1:35107:35143 [2] NCCL INFO Could not enable P2P between dev 1(=24000) and dev 3(=e1000)
ai-platform-wlf1-ge10-1:35107:35143 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:35107:35143 [2] NCCL INFO Could not enable P2P between dev 2(=81000) and dev 3(=e1000)
ai-platform-wlf1-ge10-1:35107:35143 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:35107:35143 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:35107:35143 [2] NCCL INFO Could not enable P2P between dev 1(=24000) and dev 0(=1000)
ai-platform-wlf1-ge10-1:35107:35143 [2] NCCL INFO Could not enable P2P between dev 2(=81000) and dev 0(=1000)
ai-platform-wlf1-ge10-1:35107:35143 [2] NCCL INFO Could not enable P2P between dev 3(=e1000) and dev 0(=1000)
ai-platform-wlf1-ge10-1:35107:35143 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:35107:35143 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:35107:35143 [2] NCCL INFO Could not enable P2P between dev 0(=1000) and dev 1(=24000)
ai-platform-wlf1-ge10-1:35107:35143 [2] NCCL INFO Could not enable P2P between dev 2(=81000) and dev 1(=24000)
ai-platform-wlf1-ge10-1:35107:35143 [2] NCCL INFO Could not enable P2P between dev 3(=e1000) and dev 1(=24000)
ai-platform-wlf1-ge10-1:35107:35143 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:35107:35143 [2] NCCL INFO Could not enable P2P between dev 0(=1000) and dev 2(=81000)
ai-platform-wlf1-ge10-1:35109:35144 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:35107:35143 [2] NCCL INFO Could not enable P2P between dev 1(=24000) and dev 2(=81000)
ai-platform-wlf1-ge10-1:35109:35144 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:35107:35143 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:35109:35144 [3] NCCL INFO Could not enable P2P between dev 1(=24000) and dev 0(=1000)
ai-platform-wlf1-ge10-1:35107:35143 [2] NCCL INFO Could not enable P2P between dev 3(=e1000) and dev 2(=81000)
ai-platform-wlf1-ge10-1:35107:35143 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:35109:35144 [3] NCCL INFO Could not enable P2P between dev 2(=81000) and dev 0(=1000)
ai-platform-wlf1-ge10-1:35107:35143 [2] NCCL INFO Could not enable P2P between dev 0(=1000) and dev 3(=e1000)
ai-platform-wlf1-ge10-1:35109:35144 [3] NCCL INFO Could not enable P2P between dev 3(=e1000) and dev 0(=1000)
ai-platform-wlf1-ge10-1:35107:35143 [2] NCCL INFO Could not enable P2P between dev 1(=24000) and dev 3(=e1000)
ai-platform-wlf1-ge10-1:35109:35144 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:35107:35143 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:35109:35144 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:35107:35143 [2] NCCL INFO Could not enable P2P between dev 2(=81000) and dev 3(=e1000)
ai-platform-wlf1-ge10-1:35109:35144 [3] NCCL INFO Could not enable P2P between dev 0(=1000) and dev 1(=24000)
ai-platform-wlf1-ge10-1:35109:35144 [3] NCCL INFO Could not enable P2P between dev 2(=81000) and dev 1(=24000)
ai-platform-wlf1-ge10-1:35109:35144 [3] NCCL INFO Could not enable P2P between dev 3(=e1000) and dev 1(=24000)
ai-platform-wlf1-ge10-1:35109:35144 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:35107:35143 [2] NCCL INFO Setting affinity for GPU 2 to ffffffff,00000000,ffffffff,00000000
ai-platform-wlf1-ge10-1:35109:35144 [3] NCCL INFO Could not enable P2P between dev 0(=1000) and dev 2(=81000)
ai-platform-wlf1-ge10-1:35109:35144 [3] NCCL INFO Could not enable P2P between dev 1(=24000) and dev 2(=81000)
ai-platform-wlf1-ge10-1:35109:35144 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:35109:35144 [3] NCCL INFO Could not enable P2P between dev 3(=e1000) and dev 2(=81000)
ai-platform-wlf1-ge10-1:35109:35144 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:35109:35144 [3] NCCL INFO Could not enable P2P between dev 0(=1000) and dev 3(=e1000)
ai-platform-wlf1-ge10-1:35109:35144 [3] NCCL INFO Could not enable P2P between dev 1(=24000) and dev 3(=e1000)
ai-platform-wlf1-ge10-1:35109:35144 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:35109:35144 [3] NCCL INFO Could not enable P2P between dev 2(=81000) and dev 3(=e1000)
ai-platform-wlf1-ge10-1:35109:35144 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:35109:35144 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:35109:35144 [3] NCCL INFO Could not enable P2P between dev 1(=24000) and dev 0(=1000)
ai-platform-wlf1-ge10-1:35109:35144 [3] NCCL INFO Could not enable P2P between dev 2(=81000) and dev 0(=1000)
ai-platform-wlf1-ge10-1:35109:35144 [3] NCCL INFO Could not enable P2P between dev 3(=e1000) and dev 0(=1000)
ai-platform-wlf1-ge10-1:35109:35144 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:35109:35144 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:35109:35144 [3] NCCL INFO Could not enable P2P between dev 0(=1000) and dev 1(=24000)
ai-platform-wlf1-ge10-1:35109:35144 [3] NCCL INFO Could not enable P2P between dev 2(=81000) and dev 1(=24000)
ai-platform-wlf1-ge10-1:35109:35144 [3] NCCL INFO Could not enable P2P between dev 3(=e1000) and dev 1(=24000)
ai-platform-wlf1-ge10-1:35109:35144 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:35109:35144 [3] NCCL INFO Could not enable P2P between dev 0(=1000) and dev 2(=81000)
ai-platform-wlf1-ge10-1:35109:35144 [3] NCCL INFO Could not enable P2P between dev 1(=24000) and dev 2(=81000)
ai-platform-wlf1-ge10-1:35109:35144 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:35109:35144 [3] NCCL INFO Could not enable P2P between dev 3(=e1000) and dev 2(=81000)
ai-platform-wlf1-ge10-1:35109:35144 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:35109:35144 [3] NCCL INFO Could not enable P2P between dev 0(=1000) and dev 3(=e1000)
ai-platform-wlf1-ge10-1:35109:35144 [3] NCCL INFO Could not enable P2P between dev 1(=24000) and dev 3(=e1000)
ai-platform-wlf1-ge10-1:35109:35144 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:35109:35144 [3] NCCL INFO Could not enable P2P between dev 2(=81000) and dev 3(=e1000)
ai-platform-wlf1-ge10-1:35109:35144 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff,00000000,ffffffff,00000000
ai-platform-wlf1-ge10-1:35105:35142 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:35105:35142 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:35105:35142 [0] NCCL INFO Could not enable P2P between dev 1(=24000) and dev 0(=1000)
ai-platform-wlf1-ge10-1:35105:35142 [0] NCCL INFO Could not enable P2P between dev 2(=81000) and dev 0(=1000)
ai-platform-wlf1-ge10-1:35105:35142 [0] NCCL INFO Could not enable P2P between dev 3(=e1000) and dev 0(=1000)
ai-platform-wlf1-ge10-1:35105:35142 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:35105:35142 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:35105:35142 [0] NCCL INFO Could not enable P2P between dev 0(=1000) and dev 1(=24000)
ai-platform-wlf1-ge10-1:35105:35142 [0] NCCL INFO Could not enable P2P between dev 2(=81000) and dev 1(=24000)
ai-platform-wlf1-ge10-1:35105:35142 [0] NCCL INFO Could not enable P2P between dev 3(=e1000) and dev 1(=24000)
ai-platform-wlf1-ge10-1:35105:35142 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:35105:35142 [0] NCCL INFO Could not enable P2P between dev 0(=1000) and dev 2(=81000)
ai-platform-wlf1-ge10-1:35105:35142 [0] NCCL INFO Could not enable P2P between dev 1(=24000) and dev 2(=81000)
ai-platform-wlf1-ge10-1:35105:35142 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:35105:35142 [0] NCCL INFO Could not enable P2P between dev 3(=e1000) and dev 2(=81000)
ai-platform-wlf1-ge10-1:35105:35142 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:35105:35142 [0] NCCL INFO Could not enable P2P between dev 0(=1000) and dev 3(=e1000)
ai-platform-wlf1-ge10-1:35105:35142 [0] NCCL INFO Could not enable P2P between dev 1(=24000) and dev 3(=e1000)
ai-platform-wlf1-ge10-1:35105:35142 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:35105:35142 [0] NCCL INFO Could not enable P2P between dev 2(=81000) and dev 3(=e1000)
ai-platform-wlf1-ge10-1:35105:35142 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:35105:35142 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:35105:35142 [0] NCCL INFO Could not enable P2P between dev 1(=24000) and dev 0(=1000)
ai-platform-wlf1-ge10-1:35105:35142 [0] NCCL INFO Could not enable P2P between dev 2(=81000) and dev 0(=1000)
ai-platform-wlf1-ge10-1:35105:35142 [0] NCCL INFO Could not enable P2P between dev 3(=e1000) and dev 0(=1000)
ai-platform-wlf1-ge10-1:35105:35142 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:35105:35142 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:35105:35142 [0] NCCL INFO Could not enable P2P between dev 0(=1000) and dev 1(=24000)
ai-platform-wlf1-ge10-1:35105:35142 [0] NCCL INFO Could not enable P2P between dev 2(=81000) and dev 1(=24000)
ai-platform-wlf1-ge10-1:35105:35142 [0] NCCL INFO Could not enable P2P between dev 3(=e1000) and dev 1(=24000)
ai-platform-wlf1-ge10-1:35105:35142 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:35105:35142 [0] NCCL INFO Could not enable P2P between dev 0(=1000) and dev 2(=81000)
ai-platform-wlf1-ge10-1:35106:35145 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:35105:35142 [0] NCCL INFO Could not enable P2P between dev 1(=24000) and dev 2(=81000)
ai-platform-wlf1-ge10-1:35106:35145 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:35105:35142 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:35106:35145 [1] NCCL INFO Could not enable P2P between dev 1(=24000) and dev 0(=1000)
ai-platform-wlf1-ge10-1:35105:35142 [0] NCCL INFO Could not enable P2P between dev 3(=e1000) and dev 2(=81000)
ai-platform-wlf1-ge10-1:35106:35145 [1] NCCL INFO Could not enable P2P between dev 2(=81000) and dev 0(=1000)
ai-platform-wlf1-ge10-1:35105:35142 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:35106:35145 [1] NCCL INFO Could not enable P2P between dev 3(=e1000) and dev 0(=1000)
ai-platform-wlf1-ge10-1:35105:35142 [0] NCCL INFO Could not enable P2P between dev 0(=1000) and dev 3(=e1000)
ai-platform-wlf1-ge10-1:35106:35145 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:35105:35142 [0] NCCL INFO Could not enable P2P between dev 1(=24000) and dev 3(=e1000)
ai-platform-wlf1-ge10-1:35106:35145 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:35105:35142 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:35106:35145 [1] NCCL INFO Could not enable P2P between dev 0(=1000) and dev 1(=24000)
ai-platform-wlf1-ge10-1:35105:35142 [0] NCCL INFO Could not enable P2P between dev 2(=81000) and dev 3(=e1000)
ai-platform-wlf1-ge10-1:35106:35145 [1] NCCL INFO Could not enable P2P between dev 2(=81000) and dev 1(=24000)
ai-platform-wlf1-ge10-1:35106:35145 [1] NCCL INFO Could not enable P2P between dev 3(=e1000) and dev 1(=24000)
ai-platform-wlf1-ge10-1:35106:35145 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:35106:35145 [1] NCCL INFO Could not enable P2P between dev 0(=1000) and dev 2(=81000)
ai-platform-wlf1-ge10-1:35106:35145 [1] NCCL INFO Could not enable P2P between dev 1(=24000) and dev 2(=81000)
ai-platform-wlf1-ge10-1:35105:35142 [0] NCCL INFO Setting affinity for GPU 0 to ffffffff,00000000,ffffffff
ai-platform-wlf1-ge10-1:35106:35145 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:35106:35145 [1] NCCL INFO Could not enable P2P between dev 3(=e1000) and dev 2(=81000)
ai-platform-wlf1-ge10-1:35106:35145 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:35106:35145 [1] NCCL INFO Could not enable P2P between dev 0(=1000) and dev 3(=e1000)
ai-platform-wlf1-ge10-1:35106:35145 [1] NCCL INFO Could not enable P2P between dev 1(=24000) and dev 3(=e1000)
ai-platform-wlf1-ge10-1:35106:35145 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:35106:35145 [1] NCCL INFO Could not enable P2P between dev 2(=81000) and dev 3(=e1000)
ai-platform-wlf1-ge10-1:35106:35145 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:35106:35145 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:35106:35145 [1] NCCL INFO Could not enable P2P between dev 1(=24000) and dev 0(=1000)
ai-platform-wlf1-ge10-1:35106:35145 [1] NCCL INFO Could not enable P2P between dev 2(=81000) and dev 0(=1000)
ai-platform-wlf1-ge10-1:35106:35145 [1] NCCL INFO Could not enable P2P between dev 3(=e1000) and dev 0(=1000)
ai-platform-wlf1-ge10-1:35106:35145 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:35106:35145 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:35106:35145 [1] NCCL INFO Could not enable P2P between dev 0(=1000) and dev 1(=24000)
ai-platform-wlf1-ge10-1:35106:35145 [1] NCCL INFO Could not enable P2P between dev 2(=81000) and dev 1(=24000)
ai-platform-wlf1-ge10-1:35106:35145 [1] NCCL INFO Could not enable P2P between dev 3(=e1000) and dev 1(=24000)
ai-platform-wlf1-ge10-1:35106:35145 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:35106:35145 [1] NCCL INFO Could not enable P2P between dev 0(=1000) and dev 2(=81000)
ai-platform-wlf1-ge10-1:35106:35145 [1] NCCL INFO Could not enable P2P between dev 1(=24000) and dev 2(=81000)
ai-platform-wlf1-ge10-1:35106:35145 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:35106:35145 [1] NCCL INFO Could not enable P2P between dev 3(=e1000) and dev 2(=81000)
ai-platform-wlf1-ge10-1:35106:35145 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:35106:35145 [1] NCCL INFO Could not enable P2P between dev 0(=1000) and dev 3(=e1000)
ai-platform-wlf1-ge10-1:35106:35145 [1] NCCL INFO Could not enable P2P between dev 1(=24000) and dev 3(=e1000)
ai-platform-wlf1-ge10-1:35106:35145 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:35106:35145 [1] NCCL INFO Could not enable P2P between dev 2(=81000) and dev 3(=e1000)
ai-platform-wlf1-ge10-1:35106:35145 [1] NCCL INFO Setting affinity for GPU 1 to ffffffff,00000000,ffffffff
ai-platform-wlf1-ge10-1:35109:35144 [3] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2 [2] -1/-1/-1->3->2 [3] -1/-1/-1->3->2
ai-platform-wlf1-ge10-1:35106:35145 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0 [2] 2/-1/-1->1->0 [3] 2/-1/-1->1->0
ai-platform-wlf1-ge10-1:35107:35143 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1 [2] 3/-1/-1->2->1 [3] 3/-1/-1->2->1
ai-platform-wlf1-ge10-1:35105:35142 [0] NCCL INFO Channel 00/04 :    0   1   2   3
ai-platform-wlf1-ge10-1:35105:35142 [0] NCCL INFO Channel 01/04 :    0   1   2   3
ai-platform-wlf1-ge10-1:35105:35142 [0] NCCL INFO Channel 02/04 :    0   1   2   3
ai-platform-wlf1-ge10-1:35105:35142 [0] NCCL INFO Channel 03/04 :    0   1   2   3
ai-platform-wlf1-ge10-1:35105:35142 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1 [2] 1/-1/-1->0->-1 [3] 1/-1/-1->0->-1
ai-platform-wlf1-ge10-1:35109:35144 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:35109:35144 [3] NCCL INFO Could not enable P2P between dev 3(=e1000) and dev 2(=81000)
ai-platform-wlf1-ge10-1:35106:35145 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:35106:35145 [1] NCCL INFO Could not enable P2P between dev 1(=24000) and dev 0(=1000)
ai-platform-wlf1-ge10-1:35107:35143 [2] NCCL INFO Could not enable P2P between dev 2(=81000) and dev 1(=24000)
ai-platform-wlf1-ge10-1:35105:35142 [0] NCCL INFO Could not enable P2P between dev 0(=1000) and dev 3(=e1000)
ai-platform-wlf1-ge10-1:35109:35144 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:35109:35144 [3] NCCL INFO Could not enable P2P between dev 3(=e1000) and dev 2(=81000)
ai-platform-wlf1-ge10-1:35107:35143 [2] NCCL INFO Could not enable P2P between dev 2(=81000) and dev 1(=24000)
ai-platform-wlf1-ge10-1:35106:35145 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:35106:35145 [1] NCCL INFO Could not enable P2P between dev 1(=24000) and dev 0(=1000)
ai-platform-wlf1-ge10-1:35105:35142 [0] NCCL INFO Could not enable P2P between dev 0(=1000) and dev 3(=e1000)
ai-platform-wlf1-ge10-1:35109:35144 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:35109:35144 [3] NCCL INFO Could not enable P2P between dev 3(=e1000) and dev 2(=81000)
ai-platform-wlf1-ge10-1:35107:35143 [2] NCCL INFO Could not enable P2P between dev 2(=81000) and dev 1(=24000)
ai-platform-wlf1-ge10-1:35106:35145 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:35106:35145 [1] NCCL INFO Could not enable P2P between dev 1(=24000) and dev 0(=1000)
ai-platform-wlf1-ge10-1:35105:35142 [0] NCCL INFO Could not enable P2P between dev 0(=1000) and dev 3(=e1000)
ai-platform-wlf1-ge10-1:35109:35144 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:35109:35144 [3] NCCL INFO Could not enable P2P between dev 3(=e1000) and dev 2(=81000)
ai-platform-wlf1-ge10-1:35107:35143 [2] NCCL INFO Could not enable P2P between dev 2(=81000) and dev 1(=24000)
ai-platform-wlf1-ge10-1:35106:35145 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:35106:35145 [1] NCCL INFO Could not enable P2P between dev 1(=24000) and dev 0(=1000)
ai-platform-wlf1-ge10-1:35105:35142 [0] NCCL INFO Could not enable P2P between dev 0(=1000) and dev 3(=e1000)
ai-platform-wlf1-ge10-1:35109:35144 [3] NCCL INFO Could not enable P2P between dev 3(=e1000) and dev 0(=1000)
ai-platform-wlf1-ge10-1:35107:35143 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:35107:35143 [2] NCCL INFO Could not enable P2P between dev 2(=81000) and dev 3(=e1000)
ai-platform-wlf1-ge10-1:35106:35145 [1] NCCL INFO Could not enable P2P between dev 1(=24000) and dev 2(=81000)
ai-platform-wlf1-ge10-1:35105:35142 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:35105:35142 [0] NCCL INFO Could not enable P2P between dev 0(=1000) and dev 1(=24000)
ai-platform-wlf1-ge10-1:35109:35144 [3] NCCL INFO Channel 00 : 3[e1000] -> 0[1000] via SHM/direct/direct
ai-platform-wlf1-ge10-1:35109:35144 [3] NCCL INFO Could not enable P2P between dev 3(=e1000) and dev 0(=1000)
ai-platform-wlf1-ge10-1:35107:35143 [2] NCCL INFO Channel 00 : 2[81000] -> 3[e1000] via SHM/direct/direct
ai-platform-wlf1-ge10-1:35107:35143 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:35107:35143 [2] NCCL INFO Could not enable P2P between dev 2(=81000) and dev 3(=e1000)
ai-platform-wlf1-ge10-1:35106:35145 [1] NCCL INFO Channel 00 : 1[24000] -> 2[81000] via SHM/direct/direct
ai-platform-wlf1-ge10-1:35106:35145 [1] NCCL INFO Could not enable P2P between dev 1(=24000) and dev 2(=81000)
ai-platform-wlf1-ge10-1:35105:35142 [0] NCCL INFO Channel 00 : 0[1000] -> 1[24000] via SHM/direct/direct
ai-platform-wlf1-ge10-1:35105:35142 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:35105:35142 [0] NCCL INFO Could not enable P2P between dev 0(=1000) and dev 1(=24000)
ai-platform-wlf1-ge10-1:35109:35144 [3] NCCL INFO Channel 01 : 3[e1000] -> 0[1000] via SHM/direct/direct
ai-platform-wlf1-ge10-1:35109:35144 [3] NCCL INFO Could not enable P2P between dev 3(=e1000) and dev 0(=1000)
ai-platform-wlf1-ge10-1:35107:35143 [2] NCCL INFO Channel 01 : 2[81000] -> 3[e1000] via SHM/direct/direct
ai-platform-wlf1-ge10-1:35107:35143 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:35107:35143 [2] NCCL INFO Could not enable P2P between dev 2(=81000) and dev 3(=e1000)
ai-platform-wlf1-ge10-1:35106:35145 [1] NCCL INFO Channel 01 : 1[24000] -> 2[81000] via SHM/direct/direct
ai-platform-wlf1-ge10-1:35106:35145 [1] NCCL INFO Could not enable P2P between dev 1(=24000) and dev 2(=81000)
ai-platform-wlf1-ge10-1:35105:35142 [0] NCCL INFO Channel 01 : 0[1000] -> 1[24000] via SHM/direct/direct
ai-platform-wlf1-ge10-1:35105:35142 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:35105:35142 [0] NCCL INFO Could not enable P2P between dev 0(=1000) and dev 1(=24000)
ai-platform-wlf1-ge10-1:35109:35144 [3] NCCL INFO Channel 02 : 3[e1000] -> 0[1000] via SHM/direct/direct
ai-platform-wlf1-ge10-1:35109:35144 [3] NCCL INFO Could not enable P2P between dev 3(=e1000) and dev 0(=1000)
ai-platform-wlf1-ge10-1:35107:35143 [2] NCCL INFO Channel 02 : 2[81000] -> 3[e1000] via SHM/direct/direct
ai-platform-wlf1-ge10-1:35107:35143 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:35107:35143 [2] NCCL INFO Could not enable P2P between dev 2(=81000) and dev 3(=e1000)
ai-platform-wlf1-ge10-1:35106:35145 [1] NCCL INFO Channel 02 : 1[24000] -> 2[81000] via SHM/direct/direct
ai-platform-wlf1-ge10-1:35106:35145 [1] NCCL INFO Could not enable P2P between dev 1(=24000) and dev 2(=81000)
ai-platform-wlf1-ge10-1:35105:35142 [0] NCCL INFO Channel 02 : 0[1000] -> 1[24000] via SHM/direct/direct
ai-platform-wlf1-ge10-1:35105:35142 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:35105:35142 [0] NCCL INFO Could not enable P2P between dev 0(=1000) and dev 1(=24000)
ai-platform-wlf1-ge10-1:35109:35144 [3] NCCL INFO Channel 03 : 3[e1000] -> 0[1000] via SHM/direct/direct
ai-platform-wlf1-ge10-1:35107:35143 [2] NCCL INFO Channel 03 : 2[81000] -> 3[e1000] via SHM/direct/direct
ai-platform-wlf1-ge10-1:35106:35145 [1] NCCL INFO Channel 03 : 1[24000] -> 2[81000] via SHM/direct/direct
ai-platform-wlf1-ge10-1:35105:35142 [0] NCCL INFO Channel 03 : 0[1000] -> 1[24000] via SHM/direct/direct
ai-platform-wlf1-ge10-1:35107:35143 [2] NCCL INFO Connected all rings
ai-platform-wlf1-ge10-1:35107:35143 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:35107:35143 [2] NCCL INFO Could not enable P2P between dev 2(=81000) and dev 3(=e1000)
ai-platform-wlf1-ge10-1:35105:35142 [0] NCCL INFO Connected all rings
ai-platform-wlf1-ge10-1:35105:35142 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:35105:35142 [0] NCCL INFO Could not enable P2P between dev 0(=1000) and dev 1(=24000)
ai-platform-wlf1-ge10-1:35109:35144 [3] NCCL INFO Connected all rings
ai-platform-wlf1-ge10-1:35109:35144 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:35109:35144 [3] NCCL INFO Could not enable P2P between dev 3(=e1000) and dev 2(=81000)
ai-platform-wlf1-ge10-1:35106:35145 [1] NCCL INFO Connected all rings
ai-platform-wlf1-ge10-1:35106:35145 [1] NCCL INFO Could not enable P2P between dev 1(=24000) and dev 2(=81000)
ai-platform-wlf1-ge10-1:35109:35144 [3] NCCL INFO Channel 00 : 3[e1000] -> 2[81000] via SHM/direct/direct
ai-platform-wlf1-ge10-1:35109:35144 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:35109:35144 [3] NCCL INFO Could not enable P2P between dev 3(=e1000) and dev 2(=81000)
ai-platform-wlf1-ge10-1:35109:35144 [3] NCCL INFO Channel 01 : 3[e1000] -> 2[81000] via SHM/direct/direct
ai-platform-wlf1-ge10-1:35109:35144 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:35109:35144 [3] NCCL INFO Could not enable P2P between dev 3(=e1000) and dev 2(=81000)
ai-platform-wlf1-ge10-1:35109:35144 [3] NCCL INFO Channel 02 : 3[e1000] -> 2[81000] via SHM/direct/direct
ai-platform-wlf1-ge10-1:35109:35144 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:35109:35144 [3] NCCL INFO Could not enable P2P between dev 3(=e1000) and dev 2(=81000)
ai-platform-wlf1-ge10-1:35109:35144 [3] NCCL INFO Channel 03 : 3[e1000] -> 2[81000] via SHM/direct/direct
ai-platform-wlf1-ge10-1:35107:35143 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:35107:35143 [2] NCCL INFO Could not enable P2P between dev 2(=81000) and dev 3(=e1000)
ai-platform-wlf1-ge10-1:35105:35142 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:35105:35142 [0] NCCL INFO Could not enable P2P between dev 0(=1000) and dev 1(=24000)
ai-platform-wlf1-ge10-1:35106:35145 [1] NCCL INFO Could not enable P2P between dev 1(=24000) and dev 2(=81000)
ai-platform-wlf1-ge10-1:35107:35143 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:35107:35143 [2] NCCL INFO Could not enable P2P between dev 2(=81000) and dev 3(=e1000)
ai-platform-wlf1-ge10-1:35105:35142 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:35105:35142 [0] NCCL INFO Could not enable P2P between dev 0(=1000) and dev 1(=24000)
ai-platform-wlf1-ge10-1:35106:35145 [1] NCCL INFO Could not enable P2P between dev 1(=24000) and dev 2(=81000)
ai-platform-wlf1-ge10-1:35107:35143 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:35107:35143 [2] NCCL INFO Could not enable P2P between dev 2(=81000) and dev 3(=e1000)
ai-platform-wlf1-ge10-1:35105:35142 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:35105:35142 [0] NCCL INFO Could not enable P2P between dev 0(=1000) and dev 1(=24000)
ai-platform-wlf1-ge10-1:35106:35145 [1] NCCL INFO Could not enable P2P between dev 1(=24000) and dev 2(=81000)
ai-platform-wlf1-ge10-1:35107:35143 [2] NCCL INFO Could not enable P2P between dev 2(=81000) and dev 1(=24000)
ai-platform-wlf1-ge10-1:35107:35143 [2] NCCL INFO Channel 00 : 2[81000] -> 1[24000] via SHM/direct/direct
ai-platform-wlf1-ge10-1:35107:35143 [2] NCCL INFO Could not enable P2P between dev 2(=81000) and dev 1(=24000)
ai-platform-wlf1-ge10-1:35107:35143 [2] NCCL INFO Channel 01 : 2[81000] -> 1[24000] via SHM/direct/direct
ai-platform-wlf1-ge10-1:35107:35143 [2] NCCL INFO Could not enable P2P between dev 2(=81000) and dev 1(=24000)
ai-platform-wlf1-ge10-1:35107:35143 [2] NCCL INFO Channel 02 : 2[81000] -> 1[24000] via SHM/direct/direct
ai-platform-wlf1-ge10-1:35107:35143 [2] NCCL INFO Could not enable P2P between dev 2(=81000) and dev 1(=24000)
ai-platform-wlf1-ge10-1:35107:35143 [2] NCCL INFO Channel 03 : 2[81000] -> 1[24000] via SHM/direct/direct
ai-platform-wlf1-ge10-1:35106:35145 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:35106:35145 [1] NCCL INFO Could not enable P2P between dev 1(=24000) and dev 0(=1000)
ai-platform-wlf1-ge10-1:35106:35145 [1] NCCL INFO Channel 00 : 1[24000] -> 0[1000] via SHM/direct/direct
ai-platform-wlf1-ge10-1:35106:35145 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:35106:35145 [1] NCCL INFO Could not enable P2P between dev 1(=24000) and dev 0(=1000)
ai-platform-wlf1-ge10-1:35106:35145 [1] NCCL INFO Channel 01 : 1[24000] -> 0[1000] via SHM/direct/direct
ai-platform-wlf1-ge10-1:35106:35145 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:35106:35145 [1] NCCL INFO Could not enable P2P between dev 1(=24000) and dev 0(=1000)
ai-platform-wlf1-ge10-1:35106:35145 [1] NCCL INFO Channel 02 : 1[24000] -> 0[1000] via SHM/direct/direct
ai-platform-wlf1-ge10-1:35106:35145 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:35106:35145 [1] NCCL INFO Could not enable P2P between dev 1(=24000) and dev 0(=1000)
ai-platform-wlf1-ge10-1:35106:35145 [1] NCCL INFO Channel 03 : 1[24000] -> 0[1000] via SHM/direct/direct
ai-platform-wlf1-ge10-1:35105:35142 [0] NCCL INFO Connected all trees
ai-platform-wlf1-ge10-1:35105:35142 [0] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
ai-platform-wlf1-ge10-1:35105:35142 [0] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer
ai-platform-wlf1-ge10-1:35109:35144 [3] NCCL INFO Connected all trees
ai-platform-wlf1-ge10-1:35109:35144 [3] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
ai-platform-wlf1-ge10-1:35109:35144 [3] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer
ai-platform-wlf1-ge10-1:35106:35145 [1] NCCL INFO Connected all trees
ai-platform-wlf1-ge10-1:35106:35145 [1] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
ai-platform-wlf1-ge10-1:35106:35145 [1] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer
ai-platform-wlf1-ge10-1:35107:35143 [2] NCCL INFO Connected all trees
ai-platform-wlf1-ge10-1:35107:35143 [2] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
ai-platform-wlf1-ge10-1:35107:35143 [2] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer
ai-platform-wlf1-ge10-1:35107:35143 [2] NCCL INFO comm 0x433f8950 rank 2 nranks 4 cudaDev 2 busId 81000 - Init COMPLETE
ai-platform-wlf1-ge10-1:35106:35145 [1] NCCL INFO comm 0x43a39030 rank 1 nranks 4 cudaDev 1 busId 24000 - Init COMPLETE
ai-platform-wlf1-ge10-1:35109:35144 [3] NCCL INFO comm 0x41c563d0 rank 3 nranks 4 cudaDev 3 busId e1000 - Init COMPLETE
ai-platform-wlf1-ge10-1:35105:35142 [0] NCCL INFO comm 0x44916d70 rank 0 nranks 4 cudaDev 0 busId 1000 - Init COMPLETE
The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. 
The tokenizer class you load from this checkpoint is 'XLMRobertaTokenizer'. 
The class this function is called from is 'LayoutLMv3Tokenizer_cn'.
The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. 
The tokenizer class you load from this checkpoint is 'XLMRobertaTokenizer'. 
The class this function is called from is 'LayoutLMv3Tokenizer_cn'.
The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. 
The tokenizer class you load from this checkpoint is 'XLMRobertaTokenizer'. 
The class this function is called from is 'LayoutLMv3Tokenizer_cn'.
The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. 
The tokenizer class you load from this checkpoint is 'XLMRobertaTokenizer'. 
The class this function is called from is 'LayoutLMv3Tokenizer_cn'.
Missing keys:  ['model.mask_token', 'HeadForWPA.bias', 'HeadForWPA.dense.weight', 'HeadForWPA.dense.bias', 'HeadForWPA.LayerNorm.weight', 'HeadForWPA.LayerNorm.bias', 'HeadForWPA.decoder.weight', 'HeadForWPA.decoder.bias']
Unexpected keys:  []
Create Dataset
Create Sampler
Create Dataloader
iter:  0
Start training
/opt/conda/envs/layoutlmv3/lib/python3.7/site-packages/transformers/modeling_utils.py:831: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.
  "The `device` argument is deprecated and will be removed in v5 of Transformers.", FutureWarning
/opt/conda/envs/layoutlmv3/lib/python3.7/site-packages/transformers/modeling_utils.py:831: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.
  "The `device` argument is deprecated and will be removed in v5 of Transformers.", FutureWarning
/opt/conda/envs/layoutlmv3/lib/python3.7/site-packages/transformers/modeling_utils.py:831: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.
  "The `device` argument is deprecated and will be removed in v5 of Transformers.", FutureWarning
/opt/conda/envs/layoutlmv3/lib/python3.7/site-packages/transformers/modeling_utils.py:831: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.
  "The `device` argument is deprecated and will be removed in v5 of Transformers.", FutureWarning
[W reducer.cpp:1298] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1298] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1298] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1298] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
Train Epoch: [0]  [   0/2023]  eta: 4:14:31  lr: 0.000000  wpa_loss: 0.6562  train_loss: 0.6562  time: 7.5487  data: 1.1752  max mem: 17266
Train Epoch: [0]  [  50/2023]  eta: 1:00:54  lr: 0.000000  wpa_loss: 0.6780  train_loss: 0.6780  time: 1.7404  data: 0.0001  max mem: 20466
Train Epoch: [0]  [ 100/2023]  eta: 0:57:33  lr: 0.000001  wpa_loss: 0.6437  train_loss: 0.6437  time: 1.7395  data: 0.0001  max mem: 20466
Train Epoch: [0]  [ 150/2023]  eta: 0:55:28  lr: 0.000001  wpa_loss: 0.5536  train_loss: 0.5536  time: 1.7408  data: 0.0001  max mem: 20466
Train Epoch: [0]  [ 200/2023]  eta: 0:53:42  lr: 0.000001  wpa_loss: 0.3555  train_loss: 0.3555  time: 1.7340  data: 0.0001  max mem: 20466
Train Epoch: [0]  [ 250/2023]  eta: 0:52:04  lr: 0.000001  wpa_loss: 0.2942  train_loss: 0.2942  time: 1.7403  data: 0.0001  max mem: 20466
Train Epoch: [0]  [ 300/2023]  eta: 0:50:29  lr: 0.000002  wpa_loss: 0.1898  train_loss: 0.1898  time: 1.7423  data: 0.0001  max mem: 20466
Train Epoch: [0]  [ 350/2023]  eta: 0:48:57  lr: 0.000002  wpa_loss: 0.2139  train_loss: 0.2139  time: 1.7381  data: 0.0001  max mem: 20466
Train Epoch: [0]  [ 400/2023]  eta: 0:47:25  lr: 0.000002  wpa_loss: 0.3410  train_loss: 0.3410  time: 1.7425  data: 0.0001  max mem: 20466
Train Epoch: [0]  [ 450/2023]  eta: 0:45:55  lr: 0.000002  wpa_loss: 0.1424  train_loss: 0.1424  time: 1.7344  data: 0.0001  max mem: 20466
Train Epoch: [0]  [ 500/2023]  eta: 0:44:26  lr: 0.000003  wpa_loss: 0.1244  train_loss: 0.1244  time: 1.7426  data: 0.0001  max mem: 20466
Train Epoch: [0]  [ 550/2023]  eta: 0:42:56  lr: 0.000003  wpa_loss: 0.1159  train_loss: 0.1159  time: 1.7375  data: 0.0001  max mem: 20466
Train Epoch: [0]  [ 600/2023]  eta: 0:41:28  lr: 0.000003  wpa_loss: 0.1778  train_loss: 0.1778  time: 1.7432  data: 0.0001  max mem: 20466
Train Epoch: [0]  [ 650/2023]  eta: 0:39:59  lr: 0.000003  wpa_loss: 0.2223  train_loss: 0.2223  time: 1.7353  data: 0.0001  max mem: 20466
Train Epoch: [0]  [ 700/2023]  eta: 0:38:31  lr: 0.000004  wpa_loss: 0.1381  train_loss: 0.1381  time: 1.7391  data: 0.0002  max mem: 20466
Train Epoch: [0]  [ 750/2023]  eta: 0:37:02  lr: 0.000004  wpa_loss: 0.0932  train_loss: 0.0932  time: 1.7371  data: 0.0001  max mem: 20466
Train Epoch: [0]  [ 800/2023]  eta: 0:35:34  lr: 0.000004  wpa_loss: 0.2896  train_loss: 0.2896  time: 1.7382  data: 0.0001  max mem: 20466
Train Epoch: [0]  [ 850/2023]  eta: 0:34:06  lr: 0.000004  wpa_loss: 0.0945  train_loss: 0.0945  time: 1.7377  data: 0.0001  max mem: 20466
Train Epoch: [0]  [ 900/2023]  eta: 0:32:39  lr: 0.000005  wpa_loss: 0.2074  train_loss: 0.2074  time: 1.7355  data: 0.0001  max mem: 20466
Train Epoch: [0]  [ 950/2023]  eta: 0:31:11  lr: 0.000005  wpa_loss: 0.0894  train_loss: 0.0894  time: 1.7376  data: 0.0001  max mem: 20466
Train Epoch: [0]  [1000/2023]  eta: 0:29:44  lr: 0.000005  wpa_loss: 0.0869  train_loss: 0.0869  time: 1.7403  data: 0.0002  max mem: 20466
Train Epoch: [0]  [1050/2023]  eta: 0:28:16  lr: 0.000005  wpa_loss: 0.0692  train_loss: 0.0692  time: 1.7388  data: 0.0002  max mem: 20466
Train Epoch: [0]  [1100/2023]  eta: 0:26:49  lr: 0.000006  wpa_loss: 0.1702  train_loss: 0.1702  time: 1.7372  data: 0.0002  max mem: 20466
Train Epoch: [0]  [1150/2023]  eta: 0:25:21  lr: 0.000006  wpa_loss: 0.0558  train_loss: 0.0558  time: 1.7395  data: 0.0002  max mem: 20466
Train Epoch: [0]  [1200/2023]  eta: 0:23:54  lr: 0.000006  wpa_loss: 0.0808  train_loss: 0.0808  time: 1.7382  data: 0.0002  max mem: 20466
Train Epoch: [0]  [1250/2023]  eta: 0:22:26  lr: 0.000006  wpa_loss: 0.0584  train_loss: 0.0584  time: 1.7365  data: 0.0002  max mem: 20466
Train Epoch: [0]  [1300/2023]  eta: 0:20:59  lr: 0.000007  wpa_loss: 0.0497  train_loss: 0.0497  time: 1.7395  data: 0.0002  max mem: 20466
Train Epoch: [0]  [1350/2023]  eta: 0:19:32  lr: 0.000007  wpa_loss: 0.0657  train_loss: 0.0657  time: 1.7364  data: 0.0002  max mem: 20466
Train Epoch: [0]  [1400/2023]  eta: 0:18:05  lr: 0.000007  wpa_loss: 0.0317  train_loss: 0.0317  time: 1.7190  data: 0.0001  max mem: 20466
Train Epoch: [0]  [1450/2023]  eta: 0:16:37  lr: 0.000007  wpa_loss: 0.0588  train_loss: 0.0588  time: 1.7346  data: 0.0001  max mem: 20466
Train Epoch: [0]  [1500/2023]  eta: 0:15:10  lr: 0.000008  wpa_loss: 0.0208  train_loss: 0.0208  time: 1.7395  data: 0.0001  max mem: 20466
Train Epoch: [0]  [1550/2023]  eta: 0:13:43  lr: 0.000008  wpa_loss: 0.0245  train_loss: 0.0245  time: 1.7357  data: 0.0001  max mem: 20466
Train Epoch: [0]  [1600/2023]  eta: 0:12:16  lr: 0.000008  wpa_loss: 0.0244  train_loss: 0.0244  time: 1.7381  data: 0.0001  max mem: 20466
Train Epoch: [0]  [1650/2023]  eta: 0:10:49  lr: 0.000008  wpa_loss: 0.0700  train_loss: 0.0700  time: 1.7385  data: 0.0001  max mem: 20466
Train Epoch: [0]  [1700/2023]  eta: 0:09:22  lr: 0.000009  wpa_loss: 0.0498  train_loss: 0.0498  time: 1.7371  data: 0.0001  max mem: 20466
Train Epoch: [0]  [1750/2023]  eta: 0:07:55  lr: 0.000009  wpa_loss: 0.1904  train_loss: 0.1904  time: 1.7363  data: 0.0001  max mem: 20466
Train Epoch: [0]  [1800/2023]  eta: 0:06:28  lr: 0.000009  wpa_loss: 0.0518  train_loss: 0.0518  time: 1.7387  data: 0.0001  max mem: 20466
Train Epoch: [0]  [1850/2023]  eta: 0:05:01  lr: 0.000010  wpa_loss: 0.0698  train_loss: 0.0698  time: 1.7379  data: 0.0001  max mem: 20466
Train Epoch: [0]  [1900/2023]  eta: 0:03:34  lr: 0.000010  wpa_loss: 0.0233  train_loss: 0.0233  time: 1.7368  data: 0.0001  max mem: 20466
Train Epoch: [0]  [1950/2023]  eta: 0:02:07  lr: 0.000010  wpa_loss: 0.0367  train_loss: 0.0367  time: 1.7367  data: 0.0001  max mem: 20466
Train Epoch: [0]  [2000/2023]  eta: 0:00:40  lr: 0.000010  wpa_loss: 0.0400  train_loss: 0.0400  time: 1.7368  data: 0.0001  max mem: 20466
Train Epoch: [0]  [2022/2023]  eta: 0:00:01  lr: 0.000010  wpa_loss: 0.0226  train_loss: 0.0226  time: 1.7390  data: 0.0008  max mem: 20466
Train Epoch: [0] Total time: 0:58:41 (1.7406 s / it)
Val Epoch: [0]  [  0/225]  eta: 0:04:50  wpa_loss: 0.1209  val_loss: 0.1209  time: 1.2897  data: 0.7949  max mem: 20466
Val Epoch: [0]  [ 50/225]  eta: 0:00:44  wpa_loss: 0.0475  val_loss: 0.0475  time: 0.2328  data: 0.0001  max mem: 20466
Val Epoch: [0]  [100/225]  eta: 0:00:30  wpa_loss: 0.0691  val_loss: 0.0691  time: 0.2329  data: 0.0001  max mem: 20466
Val Epoch: [0]  [150/225]  eta: 0:00:17  wpa_loss: 0.0229  val_loss: 0.0229  time: 0.2333  data: 0.0002  max mem: 20466
Val Epoch: [0]  [200/225]  eta: 0:00:05  wpa_loss: 0.0494  val_loss: 0.0494  time: 0.2335  data: 0.0002  max mem: 20466
Val Epoch: [0]  [224/225]  eta: 0:00:00  wpa_loss: 0.0146  val_loss: 0.0146  time: 0.2417  data: 0.0002  max mem: 20466
Val Epoch: [0] Total time: 0:00:54 (0.2427 s / it)
epoch:0, iter:2022, 2022,  train_loss: 0.022594228386878967, valid_loss: 0.05064470958947721, idiv_loss:0.05064470958947721
Averaged stats: lr: 0.0000  wpa_loss: 0.1596  train_loss: 0.1596
epoch 0 0.022594228386878967
Train Epoch: [1]  [   0/2023]  eta: 1:31:15  lr: 0.000010  wpa_loss: 0.0359  train_loss: 0.0359  time: 2.7069  data: 0.9616  max mem: 20466
Train Epoch: [1]  [  50/2023]  eta: 0:57:50  lr: 0.000010  wpa_loss: 0.0174  train_loss: 0.0174  time: 1.7397  data: 0.0001  max mem: 20467
Train Epoch: [1]  [ 100/2023]  eta: 0:56:03  lr: 0.000010  wpa_loss: 0.0049  train_loss: 0.0049  time: 1.7389  data: 0.0001  max mem: 20467
Train Epoch: [1]  [ 150/2023]  eta: 0:54:30  lr: 0.000010  wpa_loss: 0.0243  train_loss: 0.0243  time: 1.7407  data: 0.0001  max mem: 20467
Train Epoch: [1]  [ 200/2023]  eta: 0:53:00  lr: 0.000010  wpa_loss: 0.0141  train_loss: 0.0141  time: 1.7398  data: 0.0001  max mem: 20467
Train Epoch: [1]  [ 250/2023]  eta: 0:51:31  lr: 0.000010  wpa_loss: 0.0677  train_loss: 0.0677  time: 1.7394  data: 0.0001  max mem: 20467
Train Epoch: [1]  [ 300/2023]  eta: 0:50:02  lr: 0.000010  wpa_loss: 0.0100  train_loss: 0.0100  time: 1.7432  data: 0.0001  max mem: 20467
Train Epoch: [1]  [ 350/2023]  eta: 0:48:34  lr: 0.000010  wpa_loss: 0.0117  train_loss: 0.0117  time: 1.7393  data: 0.0001  max mem: 20467
Train Epoch: [1]  [ 400/2023]  eta: 0:47:07  lr: 0.000010  wpa_loss: 0.0056  train_loss: 0.0056  time: 1.7399  data: 0.0001  max mem: 20467
Train Epoch: [1]  [ 450/2023]  eta: 0:45:39  lr: 0.000010  wpa_loss: 0.0361  train_loss: 0.0361  time: 1.7387  data: 0.0001  max mem: 20467
Train Epoch: [1]  [ 500/2023]  eta: 0:44:11  lr: 0.000010  wpa_loss: 0.0394  train_loss: 0.0394  time: 1.7385  data: 0.0002  max mem: 20467
Train Epoch: [1]  [ 550/2023]  eta: 0:42:44  lr: 0.000010  wpa_loss: 0.0333  train_loss: 0.0333  time: 1.7346  data: 0.0001  max mem: 20467
Train Epoch: [1]  [ 600/2023]  eta: 0:41:16  lr: 0.000010  wpa_loss: 0.0090  train_loss: 0.0090  time: 1.7362  data: 0.0001  max mem: 20467
Train Epoch: [1]  [ 650/2023]  eta: 0:39:49  lr: 0.000010  wpa_loss: 0.0094  train_loss: 0.0094  time: 1.7379  data: 0.0002  max mem: 20467
Train Epoch: [1]  [ 700/2023]  eta: 0:38:22  lr: 0.000010  wpa_loss: 0.0138  train_loss: 0.0138  time: 1.7384  data: 0.0001  max mem: 20467
Train Epoch: [1]  [ 750/2023]  eta: 0:36:54  lr: 0.000010  wpa_loss: 0.0045  train_loss: 0.0045  time: 1.7385  data: 0.0001  max mem: 20467
Train Epoch: [1]  [ 800/2023]  eta: 0:35:27  lr: 0.000010  wpa_loss: 0.0055  train_loss: 0.0055  time: 1.7300  data: 0.0001  max mem: 20467
Train Epoch: [1]  [ 850/2023]  eta: 0:34:00  lr: 0.000010  wpa_loss: 0.0087  train_loss: 0.0087  time: 1.7415  data: 0.0001  max mem: 20467
Train Epoch: [1]  [ 900/2023]  eta: 0:32:33  lr: 0.000010  wpa_loss: 0.0448  train_loss: 0.0448  time: 1.7370  data: 0.0001  max mem: 20467
Train Epoch: [1]  [ 950/2023]  eta: 0:31:06  lr: 0.000010  wpa_loss: 0.0481  train_loss: 0.0481  time: 1.7396  data: 0.0001  max mem: 20467
Train Epoch: [1]  [1000/2023]  eta: 0:29:39  lr: 0.000010  wpa_loss: 0.0040  train_loss: 0.0040  time: 1.7390  data: 0.0001  max mem: 20467
Train Epoch: [1]  [1050/2023]  eta: 0:28:12  lr: 0.000010  wpa_loss: 0.0197  train_loss: 0.0197  time: 1.7347  data: 0.0001  max mem: 20467
Train Epoch: [1]  [1100/2023]  eta: 0:26:45  lr: 0.000010  wpa_loss: 0.0227  train_loss: 0.0227  time: 1.7403  data: 0.0001  max mem: 20467
Train Epoch: [1]  [1150/2023]  eta: 0:25:18  lr: 0.000010  wpa_loss: 0.0016  train_loss: 0.0016  time: 1.7338  data: 0.0001  max mem: 20467
Train Epoch: [1]  [1200/2023]  eta: 0:23:51  lr: 0.000010  wpa_loss: 0.0504  train_loss: 0.0504  time: 1.7392  data: 0.0001  max mem: 20467
Train Epoch: [1]  [1250/2023]  eta: 0:22:24  lr: 0.000010  wpa_loss: 0.0739  train_loss: 0.0739  time: 1.7406  data: 0.0001  max mem: 20467
Train Epoch: [1]  [1300/2023]  eta: 0:20:57  lr: 0.000010  wpa_loss: 0.0022  train_loss: 0.0022  time: 1.7417  data: 0.0002  max mem: 20467
Train Epoch: [1]  [1350/2023]  eta: 0:19:30  lr: 0.000010  wpa_loss: 0.0720  train_loss: 0.0720  time: 1.7358  data: 0.0002  max mem: 20467
Train Epoch: [1]  [1400/2023]  eta: 0:18:03  lr: 0.000010  wpa_loss: 0.0094  train_loss: 0.0094  time: 1.7407  data: 0.0002  max mem: 20467
Train Epoch: [1]  [1450/2023]  eta: 0:16:36  lr: 0.000010  wpa_loss: 0.0028  train_loss: 0.0028  time: 1.7388  data: 0.0002  max mem: 20467
Train Epoch: [1]  [1500/2023]  eta: 0:15:09  lr: 0.000010  wpa_loss: 0.0281  train_loss: 0.0281  time: 1.7345  data: 0.0002  max mem: 20467
Train Epoch: [1]  [1550/2023]  eta: 0:13:42  lr: 0.000010  wpa_loss: 0.0059  train_loss: 0.0059  time: 1.7366  data: 0.0002  max mem: 20467
Train Epoch: [1]  [1600/2023]  eta: 0:12:15  lr: 0.000010  wpa_loss: 0.0149  train_loss: 0.0149  time: 1.7339  data: 0.0001  max mem: 20467
Train Epoch: [1]  [1650/2023]  eta: 0:10:48  lr: 0.000010  wpa_loss: 0.0003  train_loss: 0.0003  time: 1.7387  data: 0.0001  max mem: 20467
Train Epoch: [1]  [1700/2023]  eta: 0:09:21  lr: 0.000010  wpa_loss: 0.0045  train_loss: 0.0045  time: 1.7378  data: 0.0001  max mem: 20467
Train Epoch: [1]  [1750/2023]  eta: 0:07:54  lr: 0.000010  wpa_loss: 0.0012  train_loss: 0.0012  time: 1.7372  data: 0.0001  max mem: 20467
Train Epoch: [1]  [1800/2023]  eta: 0:06:27  lr: 0.000010  wpa_loss: 0.0048  train_loss: 0.0048  time: 1.7385  data: 0.0001  max mem: 20467
Train Epoch: [1]  [1850/2023]  eta: 0:05:00  lr: 0.000010  wpa_loss: 0.0054  train_loss: 0.0054  time: 1.7395  data: 0.0001  max mem: 20467
Train Epoch: [1]  [1900/2023]  eta: 0:03:33  lr: 0.000010  wpa_loss: 0.0021  train_loss: 0.0021  time: 1.7364  data: 0.0002  max mem: 20467
Train Epoch: [1]  [1950/2023]  eta: 0:02:06  lr: 0.000010  wpa_loss: 0.0422  train_loss: 0.0422  time: 1.7354  data: 0.0002  max mem: 20467
Train Epoch: [1]  [2000/2023]  eta: 0:00:39  lr: 0.000010  wpa_loss: 0.0418  train_loss: 0.0418  time: 1.7398  data: 0.0002  max mem: 20467
Train Epoch: [1]  [2022/2023]  eta: 0:00:01  lr: 0.000010  wpa_loss: 0.0010  train_loss: 0.0010  time: 1.7365  data: 0.0009  max mem: 20467
Train Epoch: [1] Total time: 0:58:38 (1.7394 s / it)
Val Epoch: [1]  [  0/225]  eta: 0:03:51  wpa_loss: 0.0006  val_loss: 0.0006  time: 1.0276  data: 0.7910  max mem: 20467
Val Epoch: [1]  [ 50/225]  eta: 0:00:43  wpa_loss: 0.0001  val_loss: 0.0001  time: 0.2329  data: 0.0002  max mem: 20467
Val Epoch: [1]  [100/225]  eta: 0:00:30  wpa_loss: 0.0001  val_loss: 0.0001  time: 0.2331  data: 0.0001  max mem: 20467
Val Epoch: [1]  [150/225]  eta: 0:00:17  wpa_loss: 0.0024  val_loss: 0.0024  time: 0.2332  data: 0.0002  max mem: 20467
Val Epoch: [1]  [200/225]  eta: 0:00:05  wpa_loss: 0.0004  val_loss: 0.0004  time: 0.2339  data: 0.0002  max mem: 20467
Val Epoch: [1]  [224/225]  eta: 0:00:00  wpa_loss: 0.0006  val_loss: 0.0006  time: 0.2319  data: 0.0002  max mem: 20467
Val Epoch: [1] Total time: 0:00:54 (0.2409 s / it)
epoch:1, iter:4045, 2022,  train_loss: 0.0010230591287836432, valid_loss: 0.006885568831963206, idiv_loss:0.006885568831963206
Averaged stats: lr: 0.0000  wpa_loss: 0.0157  train_loss: 0.0157
epoch 1 0.0010230591287836432
Train Epoch: [2]  [   0/2023]  eta: 1:32:59  lr: 0.000010  wpa_loss: 0.0185  train_loss: 0.0185  time: 2.7581  data: 1.0036  max mem: 20467
Train Epoch: [2]  [  50/2023]  eta: 0:57:49  lr: 0.000010  wpa_loss: 0.0011  train_loss: 0.0011  time: 1.7384  data: 0.0001  max mem: 20467
Train Epoch: [2]  [ 100/2023]  eta: 0:56:02  lr: 0.000010  wpa_loss: 0.0043  train_loss: 0.0043  time: 1.7390  data: 0.0001  max mem: 20467
Train Epoch: [2]  [ 150/2023]  eta: 0:54:28  lr: 0.000010  wpa_loss: 0.0502  train_loss: 0.0502  time: 1.7424  data: 0.0001  max mem: 20467
Train Epoch: [2]  [ 200/2023]  eta: 0:52:57  lr: 0.000010  wpa_loss: 0.0029  train_loss: 0.0029  time: 1.7361  data: 0.0001  max mem: 20467
Train Epoch: [2]  [ 250/2023]  eta: 0:51:29  lr: 0.000010  wpa_loss: 0.0050  train_loss: 0.0050  time: 1.7383  data: 0.0002  max mem: 20467
Train Epoch: [2]  [ 300/2023]  eta: 0:50:01  lr: 0.000010  wpa_loss: 0.0023  train_loss: 0.0023  time: 1.7396  data: 0.0002  max mem: 20467
Train Epoch: [2]  [ 350/2023]  eta: 0:48:32  lr: 0.000010  wpa_loss: 0.0009  train_loss: 0.0009  time: 1.7363  data: 0.0004  max mem: 20467
Train Epoch: [2]  [ 400/2023]  eta: 0:47:05  lr: 0.000010  wpa_loss: 0.0011  train_loss: 0.0011  time: 1.7409  data: 0.0003  max mem: 20467
Train Epoch: [2]  [ 450/2023]  eta: 0:45:37  lr: 0.000010  wpa_loss: 0.0172  train_loss: 0.0172  time: 1.7344  data: 0.0002  max mem: 20467
Train Epoch: [2]  [ 500/2023]  eta: 0:44:10  lr: 0.000010  wpa_loss: 0.0058  train_loss: 0.0058  time: 1.7399  data: 0.0002  max mem: 20467
Train Epoch: [2]  [ 550/2023]  eta: 0:42:43  lr: 0.000010  wpa_loss: 0.0004  train_loss: 0.0004  time: 1.7367  data: 0.0004  max mem: 20467
Train Epoch: [2]  [ 600/2023]  eta: 0:41:16  lr: 0.000010  wpa_loss: 0.0086  train_loss: 0.0086  time: 1.7398  data: 0.0002  max mem: 20467
Train Epoch: [2]  [ 650/2023]  eta: 0:39:48  lr: 0.000010  wpa_loss: 0.0011  train_loss: 0.0011  time: 1.7356  data: 0.0002  max mem: 20467
Train Epoch: [2]  [ 700/2023]  eta: 0:38:21  lr: 0.000010  wpa_loss: 0.0047  train_loss: 0.0047  time: 1.7298  data: 0.0002  max mem: 20467
Train Epoch: [2]  [ 750/2023]  eta: 0:36:54  lr: 0.000010  wpa_loss: 0.0009  train_loss: 0.0009  time: 1.7368  data: 0.0002  max mem: 20467
Train Epoch: [2]  [ 800/2023]  eta: 0:35:26  lr: 0.000010  wpa_loss: 0.0003  train_loss: 0.0003  time: 1.7331  data: 0.0002  max mem: 20467
Train Epoch: [2]  [ 850/2023]  eta: 0:33:59  lr: 0.000010  wpa_loss: 0.0035  train_loss: 0.0035  time: 1.7378  data: 0.0002  max mem: 20467
Train Epoch: [2]  [ 900/2023]  eta: 0:32:32  lr: 0.000010  wpa_loss: 0.0124  train_loss: 0.0124  time: 1.7371  data: 0.0004  max mem: 20467
Train Epoch: [2]  [ 950/2023]  eta: 0:31:05  lr: 0.000010  wpa_loss: 0.0012  train_loss: 0.0012  time: 1.7389  data: 0.0002  max mem: 20467
Train Epoch: [2]  [1000/2023]  eta: 0:29:38  lr: 0.000010  wpa_loss: 0.0006  train_loss: 0.0006  time: 1.7369  data: 0.0002  max mem: 20467
Train Epoch: [2]  [1050/2023]  eta: 0:28:11  lr: 0.000010  wpa_loss: 0.0255  train_loss: 0.0255  time: 1.7385  data: 0.0002  max mem: 20467
Train Epoch: [2]  [1100/2023]  eta: 0:26:44  lr: 0.000010  wpa_loss: 0.0056  train_loss: 0.0056  time: 1.7396  data: 0.0002  max mem: 20467
Train Epoch: [2]  [1150/2023]  eta: 0:25:17  lr: 0.000010  wpa_loss: 0.0051  train_loss: 0.0051  time: 1.7366  data: 0.0002  max mem: 20467
Train Epoch: [2]  [1200/2023]  eta: 0:23:50  lr: 0.000010  wpa_loss: 0.0002  train_loss: 0.0002  time: 1.7364  data: 0.0002  max mem: 20467
Train Epoch: [2]  [1250/2023]  eta: 0:22:23  lr: 0.000010  wpa_loss: 0.0026  train_loss: 0.0026  time: 1.7356  data: 0.0002  max mem: 20467
Train Epoch: [2]  [1300/2023]  eta: 0:20:56  lr: 0.000010  wpa_loss: 0.0001  train_loss: 0.0001  time: 1.7383  data: 0.0002  max mem: 20467
Train Epoch: [2]  [1350/2023]  eta: 0:19:29  lr: 0.000010  wpa_loss: 0.0030  train_loss: 0.0030  time: 1.7335  data: 0.0002  max mem: 20467
Train Epoch: [2]  [1400/2023]  eta: 0:18:02  lr: 0.000010  wpa_loss: 0.0052  train_loss: 0.0052  time: 1.7333  data: 0.0001  max mem: 20467
Train Epoch: [2]  [1450/2023]  eta: 0:16:35  lr: 0.000010  wpa_loss: 0.0010  train_loss: 0.0010  time: 1.7382  data: 0.0001  max mem: 20467
Train Epoch: [2]  [1500/2023]  eta: 0:15:09  lr: 0.000010  wpa_loss: 0.0034  train_loss: 0.0034  time: 1.7381  data: 0.0003  max mem: 20467
Train Epoch: [2]  [1550/2023]  eta: 0:13:42  lr: 0.000010  wpa_loss: 0.0006  train_loss: 0.0006  time: 1.7376  data: 0.0001  max mem: 20467
Train Epoch: [2]  [1600/2023]  eta: 0:12:15  lr: 0.000010  wpa_loss: 0.0002  train_loss: 0.0002  time: 1.7381  data: 0.0001  max mem: 20467
Train Epoch: [2]  [1650/2023]  eta: 0:10:48  lr: 0.000010  wpa_loss: 0.0017  train_loss: 0.0017  time: 1.7376  data: 0.0002  max mem: 20467
Train Epoch: [2]  [1700/2023]  eta: 0:09:21  lr: 0.000010  wpa_loss: 0.0011  train_loss: 0.0011  time: 1.7397  data: 0.0001  max mem: 20467
Train Epoch: [2]  [1750/2023]  eta: 0:07:54  lr: 0.000010  wpa_loss: 0.0060  train_loss: 0.0060  time: 1.7373  data: 0.0001  max mem: 20467
Train Epoch: [2]  [1800/2023]  eta: 0:06:27  lr: 0.000010  wpa_loss: 0.0023  train_loss: 0.0023  time: 1.7371  data: 0.0001  max mem: 20467
Train Epoch: [2]  [1850/2023]  eta: 0:05:00  lr: 0.000010  wpa_loss: 0.0071  train_loss: 0.0071  time: 1.7393  data: 0.0001  max mem: 20467
Train Epoch: [2]  [1900/2023]  eta: 0:03:33  lr: 0.000010  wpa_loss: 0.0002  train_loss: 0.0002  time: 1.7357  data: 0.0001  max mem: 20467
Train Epoch: [2]  [1950/2023]  eta: 0:02:06  lr: 0.000010  wpa_loss: 0.0001  train_loss: 0.0001  time: 1.7369  data: 0.0001  max mem: 20467
Train Epoch: [2]  [2000/2023]  eta: 0:00:39  lr: 0.000010  wpa_loss: 0.0021  train_loss: 0.0021  time: 1.7321  data: 0.0001  max mem: 20467
Train Epoch: [2]  [2022/2023]  eta: 0:00:01  lr: 0.000010  wpa_loss: 0.0009  train_loss: 0.0009  time: 1.7296  data: 0.0011  max mem: 20467
Train Epoch: [2] Total time: 0:58:36 (1.7384 s / it)
Val Epoch: [2]  [  0/225]  eta: 0:04:52  wpa_loss: 0.0002  val_loss: 0.0002  time: 1.3017  data: 1.0646  max mem: 20467
Val Epoch: [2]  [ 50/225]  eta: 0:00:44  wpa_loss: 0.0000  val_loss: 0.0000  time: 0.2334  data: 0.0002  max mem: 20467
Val Epoch: [2]  [100/225]  eta: 0:00:30  wpa_loss: 0.0001  val_loss: 0.0001  time: 0.2337  data: 0.0003  max mem: 20467
Val Epoch: [2]  [150/225]  eta: 0:00:18  wpa_loss: 0.0020  val_loss: 0.0020  time: 0.2336  data: 0.0002  max mem: 20467
Val Epoch: [2]  [200/225]  eta: 0:00:05  wpa_loss: 0.0002  val_loss: 0.0002  time: 0.2342  data: 0.0002  max mem: 20467
Val Epoch: [2]  [224/225]  eta: 0:00:00  wpa_loss: 0.0001  val_loss: 0.0001  time: 0.2322  data: 0.0003  max mem: 20467
Val Epoch: [2] Total time: 0:00:54 (0.2431 s / it)
epoch:2, iter:6068, 2022,  train_loss: 0.0009318162919953465, valid_loss: 0.003470681530056431, idiv_loss:0.003470681530056431
Averaged stats: lr: 0.0000  wpa_loss: 0.0049  train_loss: 0.0049
epoch 2 0.0009318162919953465
Train Epoch: [3]  [   0/2023]  eta: 1:40:33  lr: 0.000010  wpa_loss: 0.0002  train_loss: 0.0002  time: 2.9824  data: 1.1207  max mem: 20467
Train Epoch: [3]  [  50/2023]  eta: 0:57:58  lr: 0.000010  wpa_loss: 0.0002  train_loss: 0.0002  time: 1.7390  data: 0.0002  max mem: 20467
Train Epoch: [3]  [ 100/2023]  eta: 0:56:03  lr: 0.000010  wpa_loss: 0.0010  train_loss: 0.0010  time: 1.7324  data: 0.0002  max mem: 20467
Train Epoch: [3]  [ 150/2023]  eta: 0:54:28  lr: 0.000010  wpa_loss: 0.0015  train_loss: 0.0015  time: 1.7373  data: 0.0002  max mem: 20467
Train Epoch: [3]  [ 200/2023]  eta: 0:52:58  lr: 0.000010  wpa_loss: 0.0003  train_loss: 0.0003  time: 1.7385  data: 0.0002  max mem: 20467
Train Epoch: [3]  [ 250/2023]  eta: 0:51:29  lr: 0.000010  wpa_loss: 0.0007  train_loss: 0.0007  time: 1.7403  data: 0.0002  max mem: 20467
Train Epoch: [3]  [ 300/2023]  eta: 0:50:01  lr: 0.000010  wpa_loss: 0.0045  train_loss: 0.0045  time: 1.7394  data: 0.0002  max mem: 20467
Train Epoch: [3]  [ 350/2023]  eta: 0:48:33  lr: 0.000010  wpa_loss: 0.0020  train_loss: 0.0020  time: 1.7368  data: 0.0002  max mem: 20467
Train Epoch: [3]  [ 400/2023]  eta: 0:47:05  lr: 0.000010  wpa_loss: 0.0057  train_loss: 0.0057  time: 1.7423  data: 0.0002  max mem: 20467
Train Epoch: [3]  [ 450/2023]  eta: 0:45:38  lr: 0.000010  wpa_loss: 0.0001  train_loss: 0.0001  time: 1.7375  data: 0.0002  max mem: 20467
Train Epoch: [3]  [ 500/2023]  eta: 0:44:11  lr: 0.000010  wpa_loss: 0.0002  train_loss: 0.0002  time: 1.7423  data: 0.0002  max mem: 20467
Train Epoch: [3]  [ 550/2023]  eta: 0:42:43  lr: 0.000010  wpa_loss: 0.0078  train_loss: 0.0078  time: 1.7377  data: 0.0002  max mem: 20467
Train Epoch: [3]  [ 600/2023]  eta: 0:41:15  lr: 0.000010  wpa_loss: 0.0005  train_loss: 0.0005  time: 1.7362  data: 0.0002  max mem: 20467
Train Epoch: [3]  [ 650/2023]  eta: 0:39:48  lr: 0.000010  wpa_loss: 0.0006  train_loss: 0.0006  time: 1.7376  data: 0.0002  max mem: 20467
Train Epoch: [3]  [ 700/2023]  eta: 0:38:21  lr: 0.000010  wpa_loss: 0.0002  train_loss: 0.0002  time: 1.7385  data: 0.0002  max mem: 20467
Train Epoch: [3]  [ 750/2023]  eta: 0:36:54  lr: 0.000010  wpa_loss: 0.0012  train_loss: 0.0012  time: 1.7371  data: 0.0002  max mem: 20467
Train Epoch: [3]  [ 800/2023]  eta: 0:35:27  lr: 0.000010  wpa_loss: 0.0035  train_loss: 0.0035  time: 1.7373  data: 0.0002  max mem: 20467
Train Epoch: [3]  [ 850/2023]  eta: 0:34:00  lr: 0.000010  wpa_loss: 0.0002  train_loss: 0.0002  time: 1.7406  data: 0.0002  max mem: 20467
Train Epoch: [3]  [ 900/2023]  eta: 0:32:32  lr: 0.000010  wpa_loss: 0.0003  train_loss: 0.0003  time: 1.7327  data: 0.0002  max mem: 20467
Train Epoch: [3]  [ 950/2023]  eta: 0:31:05  lr: 0.000010  wpa_loss: 0.0002  train_loss: 0.0002  time: 1.7379  data: 0.0002  max mem: 20467
Train Epoch: [3]  [1000/2023]  eta: 0:29:38  lr: 0.000010  wpa_loss: 0.0003  train_loss: 0.0003  time: 1.7383  data: 0.0002  max mem: 20467
Train Epoch: [3]  [1050/2023]  eta: 0:28:11  lr: 0.000010  wpa_loss: 0.0004  train_loss: 0.0004  time: 1.7386  data: 0.0002  max mem: 20467
Train Epoch: [3]  [1100/2023]  eta: 0:26:44  lr: 0.000010  wpa_loss: 0.0001  train_loss: 0.0001  time: 1.7409  data: 0.0002  max mem: 20467
Train Epoch: [3]  [1150/2023]  eta: 0:25:17  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7357  data: 0.0002  max mem: 20467
Train Epoch: [3]  [1200/2023]  eta: 0:23:51  lr: 0.000010  wpa_loss: 0.0024  train_loss: 0.0024  time: 1.7383  data: 0.0002  max mem: 20467
Train Epoch: [3]  [1250/2023]  eta: 0:22:24  lr: 0.000010  wpa_loss: 0.0001  train_loss: 0.0001  time: 1.7346  data: 0.0002  max mem: 20467
Train Epoch: [3]  [1300/2023]  eta: 0:20:57  lr: 0.000010  wpa_loss: 0.0020  train_loss: 0.0020  time: 1.7383  data: 0.0002  max mem: 20467
Train Epoch: [3]  [1350/2023]  eta: 0:19:30  lr: 0.000010  wpa_loss: 0.0008  train_loss: 0.0008  time: 1.7362  data: 0.0001  max mem: 20467
Train Epoch: [3]  [1400/2023]  eta: 0:18:03  lr: 0.000010  wpa_loss: 0.0002  train_loss: 0.0002  time: 1.7376  data: 0.0001  max mem: 20467
Train Epoch: [3]  [1450/2023]  eta: 0:16:36  lr: 0.000010  wpa_loss: 0.0014  train_loss: 0.0014  time: 1.7374  data: 0.0001  max mem: 20467
Train Epoch: [3]  [1500/2023]  eta: 0:15:09  lr: 0.000010  wpa_loss: 0.0020  train_loss: 0.0020  time: 1.7361  data: 0.0001  max mem: 20467
Train Epoch: [3]  [1550/2023]  eta: 0:13:42  lr: 0.000010  wpa_loss: 0.0002  train_loss: 0.0002  time: 1.7411  data: 0.0001  max mem: 20467
Train Epoch: [3]  [1600/2023]  eta: 0:12:15  lr: 0.000010  wpa_loss: 0.0009  train_loss: 0.0009  time: 1.7350  data: 0.0001  max mem: 20467
Train Epoch: [3]  [1650/2023]  eta: 0:10:48  lr: 0.000010  wpa_loss: 0.0002  train_loss: 0.0002  time: 1.7376  data: 0.0001  max mem: 20467
Train Epoch: [3]  [1700/2023]  eta: 0:09:21  lr: 0.000010  wpa_loss: 0.0003  train_loss: 0.0003  time: 1.7363  data: 0.0001  max mem: 20467
Train Epoch: [3]  [1750/2023]  eta: 0:07:54  lr: 0.000010  wpa_loss: 0.0026  train_loss: 0.0026  time: 1.7404  data: 0.0001  max mem: 20467
Train Epoch: [3]  [1800/2023]  eta: 0:06:27  lr: 0.000010  wpa_loss: 0.0026  train_loss: 0.0026  time: 1.7415  data: 0.0001  max mem: 20467
Train Epoch: [3]  [1850/2023]  eta: 0:05:00  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7408  data: 0.0001  max mem: 20467
Train Epoch: [3]  [1900/2023]  eta: 0:03:33  lr: 0.000010  wpa_loss: 0.0019  train_loss: 0.0019  time: 1.7416  data: 0.0001  max mem: 20467
Train Epoch: [3]  [1950/2023]  eta: 0:02:06  lr: 0.000010  wpa_loss: 0.0015  train_loss: 0.0015  time: 1.7374  data: 0.0001  max mem: 20467
Train Epoch: [3]  [2000/2023]  eta: 0:00:39  lr: 0.000010  wpa_loss: 0.0001  train_loss: 0.0001  time: 1.7391  data: 0.0001  max mem: 20467
Train Epoch: [3]  [2022/2023]  eta: 0:00:01  lr: 0.000010  wpa_loss: 0.0018  train_loss: 0.0018  time: 1.7431  data: 0.0010  max mem: 20467
Train Epoch: [3] Total time: 0:58:38 (1.7394 s / it)
Val Epoch: [3]  [  0/225]  eta: 0:04:37  wpa_loss: 0.0001  val_loss: 0.0001  time: 1.2340  data: 0.9965  max mem: 20467
Val Epoch: [3]  [ 50/225]  eta: 0:00:44  wpa_loss: 0.0001  val_loss: 0.0001  time: 0.2326  data: 0.0001  max mem: 20467
Val Epoch: [3]  [100/225]  eta: 0:00:30  wpa_loss: 0.0001  val_loss: 0.0001  time: 0.2332  data: 0.0002  max mem: 20467
Val Epoch: [3]  [150/225]  eta: 0:00:17  wpa_loss: 0.0002  val_loss: 0.0002  time: 0.2333  data: 0.0002  max mem: 20467
Val Epoch: [3]  [200/225]  eta: 0:00:05  wpa_loss: 0.0000  val_loss: 0.0000  time: 0.2337  data: 0.0002  max mem: 20467
Val Epoch: [3]  [224/225]  eta: 0:00:00  wpa_loss: 0.0003  val_loss: 0.0003  time: 0.2319  data: 0.0002  max mem: 20467
Val Epoch: [3] Total time: 0:00:54 (0.2419 s / it)
epoch:3, iter:8091, 2022,  train_loss: 0.0017826284747570753, valid_loss: 0.0035321025396681054, idiv_loss:0.0035321025396681054
Averaged stats: lr: 0.0000  wpa_loss: 0.0026  train_loss: 0.0026
epoch 3 0.0017826284747570753
Train Epoch: [4]  [   0/2023]  eta: 1:38:55  lr: 0.000010  wpa_loss: 0.0004  train_loss: 0.0004  time: 2.9341  data: 1.1100  max mem: 20467
Train Epoch: [4]  [  50/2023]  eta: 0:57:55  lr: 0.000010  wpa_loss: 0.0001  train_loss: 0.0001  time: 1.7368  data: 0.0001  max mem: 20467
Train Epoch: [4]  [ 100/2023]  eta: 0:56:04  lr: 0.000010  wpa_loss: 0.0001  train_loss: 0.0001  time: 1.7357  data: 0.0001  max mem: 20467
Train Epoch: [4]  [ 150/2023]  eta: 0:54:50  lr: 0.000010  wpa_loss: 0.0001  train_loss: 0.0001  time: 1.8234  data: 0.0001  max mem: 20467
Train Epoch: [4]  [ 200/2023]  eta: 0:53:14  lr: 0.000010  wpa_loss: 0.0001  train_loss: 0.0001  time: 1.7355  data: 0.0001  max mem: 20467
Train Epoch: [4]  [ 250/2023]  eta: 0:51:41  lr: 0.000010  wpa_loss: 0.0001  train_loss: 0.0001  time: 1.7393  data: 0.0001  max mem: 20467
Train Epoch: [4]  [ 300/2023]  eta: 0:50:10  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7387  data: 0.0001  max mem: 20467
Train Epoch: [4]  [ 350/2023]  eta: 0:48:40  lr: 0.000010  wpa_loss: 0.0001  train_loss: 0.0001  time: 1.7364  data: 0.0002  max mem: 20467
Train Epoch: [4]  [ 400/2023]  eta: 0:47:12  lr: 0.000010  wpa_loss: 0.0002  train_loss: 0.0002  time: 1.7366  data: 0.0001  max mem: 20467
Train Epoch: [4]  [ 450/2023]  eta: 0:45:43  lr: 0.000010  wpa_loss: 0.0066  train_loss: 0.0066  time: 1.7374  data: 0.0001  max mem: 20467
Train Epoch: [4]  [ 500/2023]  eta: 0:44:15  lr: 0.000010  wpa_loss: 0.0001  train_loss: 0.0001  time: 1.7388  data: 0.0001  max mem: 20467
Train Epoch: [4]  [ 550/2023]  eta: 0:42:47  lr: 0.000010  wpa_loss: 0.0067  train_loss: 0.0067  time: 1.7400  data: 0.0001  max mem: 20467
Train Epoch: [4]  [ 600/2023]  eta: 0:41:20  lr: 0.000010  wpa_loss: 0.0003  train_loss: 0.0003  time: 1.7382  data: 0.0002  max mem: 20467
Train Epoch: [4]  [ 650/2023]  eta: 0:39:52  lr: 0.000010  wpa_loss: 0.0008  train_loss: 0.0008  time: 1.7371  data: 0.0001  max mem: 20467
Train Epoch: [4]  [ 700/2023]  eta: 0:38:24  lr: 0.000010  wpa_loss: 0.0001  train_loss: 0.0001  time: 1.7386  data: 0.0001  max mem: 20467
Train Epoch: [4]  [ 750/2023]  eta: 0:36:57  lr: 0.000010  wpa_loss: 0.0001  train_loss: 0.0001  time: 1.7384  data: 0.0003  max mem: 20467
Train Epoch: [4]  [ 800/2023]  eta: 0:35:30  lr: 0.000010  wpa_loss: 0.0003  train_loss: 0.0003  time: 1.7418  data: 0.0003  max mem: 20467
Train Epoch: [4]  [ 850/2023]  eta: 0:34:02  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7378  data: 0.0003  max mem: 20467
Train Epoch: [4]  [ 900/2023]  eta: 0:32:35  lr: 0.000010  wpa_loss: 0.0001  train_loss: 0.0001  time: 1.7360  data: 0.0003  max mem: 20467
Train Epoch: [4]  [ 950/2023]  eta: 0:31:07  lr: 0.000010  wpa_loss: 0.0002  train_loss: 0.0002  time: 1.7304  data: 0.0003  max mem: 20467
Train Epoch: [4]  [1000/2023]  eta: 0:29:40  lr: 0.000010  wpa_loss: 0.0013  train_loss: 0.0013  time: 1.7357  data: 0.0003  max mem: 20467
Train Epoch: [4]  [1050/2023]  eta: 0:28:13  lr: 0.000010  wpa_loss: 0.0001  train_loss: 0.0001  time: 1.7330  data: 0.0003  max mem: 20467
Train Epoch: [4]  [1100/2023]  eta: 0:26:45  lr: 0.000010  wpa_loss: 0.0001  train_loss: 0.0001  time: 1.7378  data: 0.0003  max mem: 20467
Train Epoch: [4]  [1150/2023]  eta: 0:25:18  lr: 0.000010  wpa_loss: 0.0001  train_loss: 0.0001  time: 1.7369  data: 0.0001  max mem: 20467
Train Epoch: [4]  [1200/2023]  eta: 0:23:51  lr: 0.000010  wpa_loss: 0.0025  train_loss: 0.0025  time: 1.7308  data: 0.0001  max mem: 20467
Train Epoch: [4]  [1250/2023]  eta: 0:22:24  lr: 0.000010  wpa_loss: 0.0001  train_loss: 0.0001  time: 1.7374  data: 0.0001  max mem: 20467
Train Epoch: [4]  [1300/2023]  eta: 0:20:57  lr: 0.000010  wpa_loss: 0.0001  train_loss: 0.0001  time: 1.7390  data: 0.0001  max mem: 20467
Train Epoch: [4]  [1350/2023]  eta: 0:19:30  lr: 0.000010  wpa_loss: 0.0008  train_loss: 0.0008  time: 1.7382  data: 0.0001  max mem: 20467
Train Epoch: [4]  [1400/2023]  eta: 0:18:03  lr: 0.000010  wpa_loss: 0.0003  train_loss: 0.0003  time: 1.7396  data: 0.0002  max mem: 20467
Train Epoch: [4]  [1450/2023]  eta: 0:16:36  lr: 0.000010  wpa_loss: 0.0007  train_loss: 0.0007  time: 1.7372  data: 0.0002  max mem: 20467
Train Epoch: [4]  [1500/2023]  eta: 0:15:09  lr: 0.000010  wpa_loss: 0.0001  train_loss: 0.0001  time: 1.7358  data: 0.0003  max mem: 20467
Train Epoch: [4]  [1550/2023]  eta: 0:13:42  lr: 0.000010  wpa_loss: 0.0006  train_loss: 0.0006  time: 1.7385  data: 0.0003  max mem: 20467
Train Epoch: [4]  [1600/2023]  eta: 0:12:15  lr: 0.000010  wpa_loss: 0.0048  train_loss: 0.0048  time: 1.7332  data: 0.0003  max mem: 20467
Train Epoch: [4]  [1650/2023]  eta: 0:10:48  lr: 0.000010  wpa_loss: 0.0179  train_loss: 0.0179  time: 1.7336  data: 0.0002  max mem: 20467
Train Epoch: [4]  [1700/2023]  eta: 0:09:21  lr: 0.000010  wpa_loss: 0.0011  train_loss: 0.0011  time: 1.7334  data: 0.0003  max mem: 20467
Train Epoch: [4]  [1750/2023]  eta: 0:07:54  lr: 0.000010  wpa_loss: 0.0003  train_loss: 0.0003  time: 1.7319  data: 0.0003  max mem: 20467
Train Epoch: [4]  [1800/2023]  eta: 0:06:27  lr: 0.000010  wpa_loss: 0.0004  train_loss: 0.0004  time: 1.7331  data: 0.0002  max mem: 20467
Train Epoch: [4]  [1850/2023]  eta: 0:05:00  lr: 0.000010  wpa_loss: 0.0001  train_loss: 0.0001  time: 1.7331  data: 0.0003  max mem: 20467
Train Epoch: [4]  [1900/2023]  eta: 0:03:33  lr: 0.000010  wpa_loss: 0.0001  train_loss: 0.0001  time: 1.7373  data: 0.0002  max mem: 20467
Train Epoch: [4]  [1950/2023]  eta: 0:02:06  lr: 0.000010  wpa_loss: 0.0006  train_loss: 0.0006  time: 1.7344  data: 0.0003  max mem: 20467
Train Epoch: [4]  [2000/2023]  eta: 0:00:39  lr: 0.000010  wpa_loss: 0.0003  train_loss: 0.0003  time: 1.7358  data: 0.0002  max mem: 20467
Train Epoch: [4]  [2022/2023]  eta: 0:00:01  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7388  data: 0.0009  max mem: 20467
Train Epoch: [4] Total time: 0:58:36 (1.7385 s / it)
Val Epoch: [4]  [  0/225]  eta: 0:04:45  wpa_loss: 0.0000  val_loss: 0.0000  time: 1.2674  data: 1.0303  max mem: 20467
Val Epoch: [4]  [ 50/225]  eta: 0:00:44  wpa_loss: 0.0000  val_loss: 0.0000  time: 0.2325  data: 0.0001  max mem: 20467
Val Epoch: [4]  [100/225]  eta: 0:00:30  wpa_loss: 0.0000  val_loss: 0.0000  time: 0.2329  data: 0.0001  max mem: 20467
Val Epoch: [4]  [150/225]  eta: 0:00:17  wpa_loss: 0.0001  val_loss: 0.0001  time: 0.2335  data: 0.0002  max mem: 20467
Val Epoch: [4]  [200/225]  eta: 0:00:05  wpa_loss: 0.0000  val_loss: 0.0000  time: 0.2338  data: 0.0002  max mem: 20467
Val Epoch: [4]  [224/225]  eta: 0:00:00  wpa_loss: 0.0008  val_loss: 0.0008  time: 0.2321  data: 0.0002  max mem: 20467
Val Epoch: [4] Total time: 0:00:54 (0.2419 s / it)
epoch:4, iter:10114, 2022,  train_loss: 3.068879959755577e-05, valid_loss: 0.0014329514592625653, idiv_loss:0.0014329514592625653
Averaged stats: lr: 0.0000  wpa_loss: 0.0017  train_loss: 0.0017
epoch 4 3.068879959755577e-05
Train Epoch: [5]  [   0/2023]  eta: 1:39:02  lr: 0.000010  wpa_loss: 0.0001  train_loss: 0.0001  time: 2.9375  data: 1.1317  max mem: 20467
Train Epoch: [5]  [  50/2023]  eta: 0:57:54  lr: 0.000010  wpa_loss: 0.0001  train_loss: 0.0001  time: 1.7383  data: 0.0001  max mem: 20467
Train Epoch: [5]  [ 100/2023]  eta: 0:56:01  lr: 0.000010  wpa_loss: 0.0001  train_loss: 0.0001  time: 1.7358  data: 0.0001  max mem: 20467
Train Epoch: [5]  [ 150/2023]  eta: 0:54:26  lr: 0.000010  wpa_loss: 0.0003  train_loss: 0.0003  time: 1.7365  data: 0.0001  max mem: 20467
Train Epoch: [5]  [ 200/2023]  eta: 0:52:55  lr: 0.000010  wpa_loss: 0.0001  train_loss: 0.0001  time: 1.7348  data: 0.0001  max mem: 20467
Train Epoch: [5]  [ 250/2023]  eta: 0:51:26  lr: 0.000010  wpa_loss: 0.0001  train_loss: 0.0001  time: 1.7424  data: 0.0001  max mem: 20467
Train Epoch: [5]  [ 300/2023]  eta: 0:49:57  lr: 0.000010  wpa_loss: 0.0007  train_loss: 0.0007  time: 1.7362  data: 0.0001  max mem: 20467
Train Epoch: [5]  [ 350/2023]  eta: 0:48:30  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7381  data: 0.0001  max mem: 20467
Train Epoch: [5]  [ 400/2023]  eta: 0:47:03  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7382  data: 0.0001  max mem: 20467
Train Epoch: [5]  [ 450/2023]  eta: 0:45:35  lr: 0.000010  wpa_loss: 0.0009  train_loss: 0.0009  time: 1.7335  data: 0.0001  max mem: 20467
Train Epoch: [5]  [ 500/2023]  eta: 0:44:08  lr: 0.000010  wpa_loss: 0.0033  train_loss: 0.0033  time: 1.7373  data: 0.0001  max mem: 20467
Train Epoch: [5]  [ 550/2023]  eta: 0:42:41  lr: 0.000010  wpa_loss: 0.0011  train_loss: 0.0011  time: 1.7381  data: 0.0001  max mem: 20467
Train Epoch: [5]  [ 600/2023]  eta: 0:41:13  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7335  data: 0.0001  max mem: 20467
Train Epoch: [5]  [ 650/2023]  eta: 0:39:46  lr: 0.000010  wpa_loss: 0.0001  train_loss: 0.0001  time: 1.7369  data: 0.0001  max mem: 20467
Train Epoch: [5]  [ 700/2023]  eta: 0:38:19  lr: 0.000010  wpa_loss: 0.0005  train_loss: 0.0005  time: 1.7425  data: 0.0001  max mem: 20467
Train Epoch: [5]  [ 750/2023]  eta: 0:36:52  lr: 0.000010  wpa_loss: 0.0001  train_loss: 0.0001  time: 1.7372  data: 0.0001  max mem: 20467
Train Epoch: [5]  [ 800/2023]  eta: 0:35:26  lr: 0.000010  wpa_loss: 0.0003  train_loss: 0.0003  time: 1.7374  data: 0.0001  max mem: 20467
Train Epoch: [5]  [ 850/2023]  eta: 0:33:59  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7399  data: 0.0001  max mem: 20467
Train Epoch: [5]  [ 900/2023]  eta: 0:32:32  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7371  data: 0.0001  max mem: 20467
Train Epoch: [5]  [ 950/2023]  eta: 0:31:05  lr: 0.000010  wpa_loss: 0.0049  train_loss: 0.0049  time: 1.7392  data: 0.0001  max mem: 20467
Train Epoch: [5]  [1000/2023]  eta: 0:29:38  lr: 0.000010  wpa_loss: 0.0004  train_loss: 0.0004  time: 1.7363  data: 0.0001  max mem: 20467
Train Epoch: [5]  [1050/2023]  eta: 0:28:11  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7377  data: 0.0001  max mem: 20467
Train Epoch: [5]  [1100/2023]  eta: 0:26:44  lr: 0.000010  wpa_loss: 0.0001  train_loss: 0.0001  time: 1.7394  data: 0.0001  max mem: 20467
Train Epoch: [5]  [1150/2023]  eta: 0:25:17  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7382  data: 0.0001  max mem: 20467
Train Epoch: [5]  [1200/2023]  eta: 0:23:50  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7353  data: 0.0001  max mem: 20467
Train Epoch: [5]  [1250/2023]  eta: 0:22:23  lr: 0.000010  wpa_loss: 0.0001  train_loss: 0.0001  time: 1.7377  data: 0.0001  max mem: 20467
Train Epoch: [5]  [1300/2023]  eta: 0:20:56  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7367  data: 0.0001  max mem: 20467
Train Epoch: [5]  [1350/2023]  eta: 0:19:29  lr: 0.000010  wpa_loss: 0.0001  train_loss: 0.0001  time: 1.7345  data: 0.0001  max mem: 20467
Train Epoch: [5]  [1400/2023]  eta: 0:18:02  lr: 0.000010  wpa_loss: 0.0002  train_loss: 0.0002  time: 1.7364  data: 0.0001  max mem: 20467
Train Epoch: [5]  [1450/2023]  eta: 0:16:35  lr: 0.000010  wpa_loss: 0.0003  train_loss: 0.0003  time: 1.7387  data: 0.0001  max mem: 20467
Train Epoch: [5]  [1500/2023]  eta: 0:15:08  lr: 0.000010  wpa_loss: 0.0002  train_loss: 0.0002  time: 1.7364  data: 0.0001  max mem: 20467
Train Epoch: [5]  [1550/2023]  eta: 0:13:41  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7333  data: 0.0001  max mem: 20467
Train Epoch: [5]  [1600/2023]  eta: 0:12:15  lr: 0.000010  wpa_loss: 0.0002  train_loss: 0.0002  time: 1.7317  data: 0.0001  max mem: 20467
Train Epoch: [5]  [1650/2023]  eta: 0:10:48  lr: 0.000010  wpa_loss: 0.0001  train_loss: 0.0001  time: 1.7392  data: 0.0001  max mem: 20467
Train Epoch: [5]  [1700/2023]  eta: 0:09:21  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7329  data: 0.0001  max mem: 20467
Train Epoch: [5]  [1750/2023]  eta: 0:07:54  lr: 0.000010  wpa_loss: 0.0001  train_loss: 0.0001  time: 1.7348  data: 0.0001  max mem: 20467
Train Epoch: [5]  [1800/2023]  eta: 0:06:27  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7360  data: 0.0002  max mem: 20467
Train Epoch: [5]  [1850/2023]  eta: 0:05:00  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7368  data: 0.0002  max mem: 20467
Train Epoch: [5]  [1900/2023]  eta: 0:03:33  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7368  data: 0.0002  max mem: 20467
Train Epoch: [5]  [1950/2023]  eta: 0:02:06  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7375  data: 0.0002  max mem: 20467
Train Epoch: [5]  [2000/2023]  eta: 0:00:39  lr: 0.000010  wpa_loss: 0.0026  train_loss: 0.0026  time: 1.7354  data: 0.0002  max mem: 20467
Train Epoch: [5]  [2022/2023]  eta: 0:00:01  lr: 0.000010  wpa_loss: 0.0001  train_loss: 0.0001  time: 1.7351  data: 0.0008  max mem: 20467
Train Epoch: [5] Total time: 0:58:35 (1.7380 s / it)
Val Epoch: [5]  [  0/225]  eta: 0:04:47  wpa_loss: 0.0001  val_loss: 0.0001  time: 1.2794  data: 1.0420  max mem: 20467
Val Epoch: [5]  [ 50/225]  eta: 0:00:44  wpa_loss: 0.0000  val_loss: 0.0000  time: 0.2325  data: 0.0001  max mem: 20467
Val Epoch: [5]  [100/225]  eta: 0:00:30  wpa_loss: 0.0000  val_loss: 0.0000  time: 0.2329  data: 0.0001  max mem: 20467
Val Epoch: [5]  [150/225]  eta: 0:00:17  wpa_loss: 0.0008  val_loss: 0.0008  time: 0.2333  data: 0.0001  max mem: 20467
Val Epoch: [5]  [200/225]  eta: 0:00:05  wpa_loss: 0.0001  val_loss: 0.0001  time: 0.2336  data: 0.0001  max mem: 20467
Val Epoch: [5]  [224/225]  eta: 0:00:00  wpa_loss: 0.0001  val_loss: 0.0001  time: 0.2318  data: 0.0002  max mem: 20467
Val Epoch: [5] Total time: 0:00:54 (0.2419 s / it)
epoch:5, iter:12137, 2022,  train_loss: 0.00014561884745489806, valid_loss: 0.0031851550335947345, idiv_loss:0.0031851550335947345
Averaged stats: lr: 0.0000  wpa_loss: 0.0014  train_loss: 0.0014
epoch 5 0.00014561884745489806
Train Epoch: [6]  [   0/2023]  eta: 1:38:33  lr: 0.000010  wpa_loss: 0.0005  train_loss: 0.0005  time: 2.9230  data: 1.0147  max mem: 20467
Train Epoch: [6]  [  50/2023]  eta: 0:57:48  lr: 0.000010  wpa_loss: 0.0001  train_loss: 0.0001  time: 1.7344  data: 0.0001  max mem: 20467
Train Epoch: [6]  [ 100/2023]  eta: 0:56:00  lr: 0.000010  wpa_loss: 0.0001  train_loss: 0.0001  time: 1.7355  data: 0.0001  max mem: 20467
Train Epoch: [6]  [ 150/2023]  eta: 0:54:26  lr: 0.000010  wpa_loss: 0.0036  train_loss: 0.0036  time: 1.7349  data: 0.0002  max mem: 20467
Train Epoch: [6]  [ 200/2023]  eta: 0:52:56  lr: 0.000010  wpa_loss: 0.0008  train_loss: 0.0008  time: 1.7395  data: 0.0002  max mem: 20467
Train Epoch: [6]  [ 250/2023]  eta: 0:51:27  lr: 0.000010  wpa_loss: 0.0013  train_loss: 0.0013  time: 1.7380  data: 0.0001  max mem: 20467
Train Epoch: [6]  [ 300/2023]  eta: 0:49:58  lr: 0.000010  wpa_loss: 0.0006  train_loss: 0.0006  time: 1.7303  data: 0.0001  max mem: 20467
Train Epoch: [6]  [ 350/2023]  eta: 0:48:29  lr: 0.000010  wpa_loss: 0.0001  train_loss: 0.0001  time: 1.7342  data: 0.0001  max mem: 20467
Train Epoch: [6]  [ 400/2023]  eta: 0:47:01  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7324  data: 0.0001  max mem: 20467
Train Epoch: [6]  [ 450/2023]  eta: 0:45:34  lr: 0.000010  wpa_loss: 0.0008  train_loss: 0.0008  time: 1.7381  data: 0.0001  max mem: 20467
Train Epoch: [6]  [ 500/2023]  eta: 0:44:07  lr: 0.000010  wpa_loss: 0.0001  train_loss: 0.0001  time: 1.7379  data: 0.0001  max mem: 20467
Train Epoch: [6]  [ 550/2023]  eta: 0:42:40  lr: 0.000010  wpa_loss: 0.0002  train_loss: 0.0002  time: 1.7401  data: 0.0002  max mem: 20467
Train Epoch: [6]  [ 600/2023]  eta: 0:41:13  lr: 0.000010  wpa_loss: 0.0104  train_loss: 0.0104  time: 1.7345  data: 0.0001  max mem: 20467
Train Epoch: [6]  [ 650/2023]  eta: 0:39:46  lr: 0.000010  wpa_loss: 0.0001  train_loss: 0.0001  time: 1.7399  data: 0.0001  max mem: 20467
Train Epoch: [6]  [ 700/2023]  eta: 0:38:19  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7383  data: 0.0001  max mem: 20467
Train Epoch: [6]  [ 750/2023]  eta: 0:36:52  lr: 0.000010  wpa_loss: 0.0003  train_loss: 0.0003  time: 1.7395  data: 0.0001  max mem: 20467
Train Epoch: [6]  [ 800/2023]  eta: 0:35:25  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7364  data: 0.0001  max mem: 20467
Train Epoch: [6]  [ 850/2023]  eta: 0:33:58  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7356  data: 0.0001  max mem: 20467
Train Epoch: [6]  [ 900/2023]  eta: 0:32:31  lr: 0.000010  wpa_loss: 0.0010  train_loss: 0.0010  time: 1.7379  data: 0.0001  max mem: 20467
Train Epoch: [6]  [ 950/2023]  eta: 0:31:04  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7370  data: 0.0001  max mem: 20467
Train Epoch: [6]  [1000/2023]  eta: 0:29:37  lr: 0.000010  wpa_loss: 0.0003  train_loss: 0.0003  time: 1.7340  data: 0.0002  max mem: 20467
Train Epoch: [6]  [1050/2023]  eta: 0:28:10  lr: 0.000010  wpa_loss: 0.0007  train_loss: 0.0007  time: 1.7358  data: 0.0001  max mem: 20467
Train Epoch: [6]  [1100/2023]  eta: 0:26:43  lr: 0.000010  wpa_loss: 0.0029  train_loss: 0.0029  time: 1.7381  data: 0.0001  max mem: 20467
Train Epoch: [6]  [1150/2023]  eta: 0:25:16  lr: 0.000010  wpa_loss: 0.0003  train_loss: 0.0003  time: 1.7343  data: 0.0001  max mem: 20467
Train Epoch: [6]  [1200/2023]  eta: 0:23:49  lr: 0.000010  wpa_loss: 0.0001  train_loss: 0.0001  time: 1.7391  data: 0.0001  max mem: 20467
Train Epoch: [6]  [1250/2023]  eta: 0:22:22  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7355  data: 0.0002  max mem: 20467
Train Epoch: [6]  [1300/2023]  eta: 0:20:56  lr: 0.000010  wpa_loss: 0.0008  train_loss: 0.0008  time: 1.7343  data: 0.0001  max mem: 20467
Train Epoch: [6]  [1350/2023]  eta: 0:19:29  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7332  data: 0.0001  max mem: 20467
Train Epoch: [6]  [1400/2023]  eta: 0:18:02  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7366  data: 0.0001  max mem: 20467
Train Epoch: [6]  [1450/2023]  eta: 0:16:35  lr: 0.000010  wpa_loss: 0.0006  train_loss: 0.0006  time: 1.7364  data: 0.0001  max mem: 20467
Train Epoch: [6]  [1500/2023]  eta: 0:15:08  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7307  data: 0.0001  max mem: 20467
Train Epoch: [6]  [1550/2023]  eta: 0:13:41  lr: 0.000010  wpa_loss: 0.0001  train_loss: 0.0001  time: 1.7370  data: 0.0001  max mem: 20467
Train Epoch: [6]  [1600/2023]  eta: 0:12:14  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7345  data: 0.0001  max mem: 20467
Train Epoch: [6]  [1650/2023]  eta: 0:10:47  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7373  data: 0.0001  max mem: 20467
Train Epoch: [6]  [1700/2023]  eta: 0:09:20  lr: 0.000010  wpa_loss: 0.0007  train_loss: 0.0007  time: 1.7327  data: 0.0001  max mem: 20467
Train Epoch: [6]  [1750/2023]  eta: 0:07:54  lr: 0.000010  wpa_loss: 0.0008  train_loss: 0.0008  time: 1.7359  data: 0.0001  max mem: 20467
Train Epoch: [6]  [1800/2023]  eta: 0:06:27  lr: 0.000010  wpa_loss: 0.0007  train_loss: 0.0007  time: 1.7364  data: 0.0001  max mem: 20467
Train Epoch: [6]  [1850/2023]  eta: 0:05:00  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7308  data: 0.0001  max mem: 20467
Train Epoch: [6]  [1900/2023]  eta: 0:03:33  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7365  data: 0.0001  max mem: 20467
Train Epoch: [6]  [1950/2023]  eta: 0:02:06  lr: 0.000010  wpa_loss: 0.0017  train_loss: 0.0017  time: 1.7336  data: 0.0001  max mem: 20467
Train Epoch: [6]  [2000/2023]  eta: 0:00:39  lr: 0.000010  wpa_loss: 0.0001  train_loss: 0.0001  time: 1.7358  data: 0.0001  max mem: 20467
Train Epoch: [6]  [2022/2023]  eta: 0:00:01  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7365  data: 0.0008  max mem: 20467
Train Epoch: [6] Total time: 0:58:34 (1.7371 s / it)
Val Epoch: [6]  [  0/225]  eta: 0:05:01  wpa_loss: 0.0000  val_loss: 0.0000  time: 1.3382  data: 1.1005  max mem: 20467
Val Epoch: [6]  [ 50/225]  eta: 0:00:44  wpa_loss: 0.0000  val_loss: 0.0000  time: 0.2325  data: 0.0001  max mem: 20467
Val Epoch: [6]  [100/225]  eta: 0:00:30  wpa_loss: 0.0000  val_loss: 0.0000  time: 0.2328  data: 0.0001  max mem: 20467
Val Epoch: [6]  [150/225]  eta: 0:00:18  wpa_loss: 0.0000  val_loss: 0.0000  time: 0.2333  data: 0.0001  max mem: 20467
Val Epoch: [6]  [200/225]  eta: 0:00:05  wpa_loss: 0.0000  val_loss: 0.0000  time: 0.2337  data: 0.0002  max mem: 20467
Val Epoch: [6]  [224/225]  eta: 0:00:00  wpa_loss: 0.0000  val_loss: 0.0000  time: 0.2320  data: 0.0002  max mem: 20467
Val Epoch: [6] Total time: 0:00:54 (0.2421 s / it)
epoch:6, iter:14160, 2022,  train_loss: 1.2514499758253805e-05, valid_loss: 0.0013353751907319142, idiv_loss:0.0013353751907319142
Averaged stats: lr: 0.0000  wpa_loss: 0.0010  train_loss: 0.0010
epoch 6 1.2514499758253805e-05
Train Epoch: [7]  [   0/2023]  eta: 1:38:24  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 2.9186  data: 1.0215  max mem: 20467
Train Epoch: [7]  [  50/2023]  eta: 0:58:02  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7408  data: 0.0001  max mem: 20467
Train Epoch: [7]  [ 100/2023]  eta: 0:56:09  lr: 0.000010  wpa_loss: 0.0002  train_loss: 0.0002  time: 1.7394  data: 0.0001  max mem: 20467
Train Epoch: [7]  [ 150/2023]  eta: 0:54:35  lr: 0.000010  wpa_loss: 0.0003  train_loss: 0.0003  time: 1.7420  data: 0.0001  max mem: 20467
Train Epoch: [7]  [ 200/2023]  eta: 0:53:04  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7418  data: 0.0001  max mem: 20467
Train Epoch: [7]  [ 250/2023]  eta: 0:51:35  lr: 0.000010  wpa_loss: 0.0001  train_loss: 0.0001  time: 1.7409  data: 0.0001  max mem: 20467
Train Epoch: [7]  [ 300/2023]  eta: 0:50:06  lr: 0.000010  wpa_loss: 0.0007  train_loss: 0.0007  time: 1.7401  data: 0.0001  max mem: 20467
Train Epoch: [7]  [ 350/2023]  eta: 0:48:38  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7440  data: 0.0001  max mem: 20467
Train Epoch: [7]  [ 400/2023]  eta: 0:47:09  lr: 0.000010  wpa_loss: 0.0005  train_loss: 0.0005  time: 1.7413  data: 0.0001  max mem: 20467
Train Epoch: [7]  [ 450/2023]  eta: 0:45:42  lr: 0.000010  wpa_loss: 0.0001  train_loss: 0.0001  time: 1.7404  data: 0.0001  max mem: 20467
Train Epoch: [7]  [ 500/2023]  eta: 0:44:14  lr: 0.000010  wpa_loss: 0.0001  train_loss: 0.0001  time: 1.7432  data: 0.0001  max mem: 20467
Train Epoch: [7]  [ 550/2023]  eta: 0:42:47  lr: 0.000010  wpa_loss: 0.0004  train_loss: 0.0004  time: 1.7411  data: 0.0001  max mem: 20467
Train Epoch: [7]  [ 600/2023]  eta: 0:41:20  lr: 0.000010  wpa_loss: 0.0005  train_loss: 0.0005  time: 1.7417  data: 0.0001  max mem: 20467
Train Epoch: [7]  [ 650/2023]  eta: 0:39:52  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7390  data: 0.0001  max mem: 20467
Train Epoch: [7]  [ 700/2023]  eta: 0:38:25  lr: 0.000010  wpa_loss: 0.0005  train_loss: 0.0005  time: 1.7410  data: 0.0001  max mem: 20467
Train Epoch: [7]  [ 750/2023]  eta: 0:36:58  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7395  data: 0.0001  max mem: 20467
Train Epoch: [7]  [ 800/2023]  eta: 0:35:30  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7391  data: 0.0001  max mem: 20467
Train Epoch: [7]  [ 850/2023]  eta: 0:34:03  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7410  data: 0.0001  max mem: 20467
Train Epoch: [7]  [ 900/2023]  eta: 0:32:36  lr: 0.000010  wpa_loss: 0.0002  train_loss: 0.0002  time: 1.7406  data: 0.0001  max mem: 20467
Train Epoch: [7]  [ 950/2023]  eta: 0:31:09  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7395  data: 0.0001  max mem: 20467
Train Epoch: [7]  [1000/2023]  eta: 0:29:41  lr: 0.000010  wpa_loss: 0.0007  train_loss: 0.0007  time: 1.7367  data: 0.0001  max mem: 20467
Train Epoch: [7]  [1050/2023]  eta: 0:28:14  lr: 0.000010  wpa_loss: 0.0020  train_loss: 0.0020  time: 1.7356  data: 0.0001  max mem: 20467
Train Epoch: [7]  [1100/2023]  eta: 0:26:47  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7366  data: 0.0001  max mem: 20467
Train Epoch: [7]  [1150/2023]  eta: 0:25:20  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7405  data: 0.0001  max mem: 20467
Train Epoch: [7]  [1200/2023]  eta: 0:23:52  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7371  data: 0.0001  max mem: 20467
Train Epoch: [7]  [1250/2023]  eta: 0:22:25  lr: 0.000010  wpa_loss: 0.0045  train_loss: 0.0045  time: 1.7427  data: 0.0001  max mem: 20467
Train Epoch: [7]  [1300/2023]  eta: 0:20:58  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7402  data: 0.0001  max mem: 20467
Train Epoch: [7]  [1350/2023]  eta: 0:19:31  lr: 0.000010  wpa_loss: 0.0019  train_loss: 0.0019  time: 1.7397  data: 0.0001  max mem: 20467
Train Epoch: [7]  [1400/2023]  eta: 0:18:04  lr: 0.000010  wpa_loss: 0.0005  train_loss: 0.0005  time: 1.7376  data: 0.0001  max mem: 20467
Train Epoch: [7]  [1450/2023]  eta: 0:16:37  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7379  data: 0.0001  max mem: 20467
Train Epoch: [7]  [1500/2023]  eta: 0:15:10  lr: 0.000010  wpa_loss: 0.0001  train_loss: 0.0001  time: 1.7358  data: 0.0001  max mem: 20467
Train Epoch: [7]  [1550/2023]  eta: 0:13:43  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7389  data: 0.0001  max mem: 20467
Train Epoch: [7]  [1600/2023]  eta: 0:12:16  lr: 0.000010  wpa_loss: 0.0002  train_loss: 0.0002  time: 1.7408  data: 0.0001  max mem: 20467
Train Epoch: [7]  [1650/2023]  eta: 0:10:49  lr: 0.000010  wpa_loss: 0.0001  train_loss: 0.0001  time: 1.7377  data: 0.0001  max mem: 20467
Train Epoch: [7]  [1700/2023]  eta: 0:09:22  lr: 0.000010  wpa_loss: 0.0003  train_loss: 0.0003  time: 1.7403  data: 0.0001  max mem: 20467
Train Epoch: [7]  [1750/2023]  eta: 0:07:55  lr: 0.000010  wpa_loss: 0.0006  train_loss: 0.0006  time: 1.7375  data: 0.0001  max mem: 20467
Train Epoch: [7]  [1800/2023]  eta: 0:06:28  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7388  data: 0.0001  max mem: 20467
Train Epoch: [7]  [1850/2023]  eta: 0:05:01  lr: 0.000010  wpa_loss: 0.0001  train_loss: 0.0001  time: 1.7402  data: 0.0001  max mem: 20467
Train Epoch: [7]  [1900/2023]  eta: 0:03:34  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7380  data: 0.0001  max mem: 20467
Train Epoch: [7]  [1950/2023]  eta: 0:02:07  lr: 0.000010  wpa_loss: 0.0004  train_loss: 0.0004  time: 1.7390  data: 0.0001  max mem: 20467
Train Epoch: [7]  [2000/2023]  eta: 0:00:40  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7370  data: 0.0001  max mem: 20467
Train Epoch: [7]  [2022/2023]  eta: 0:00:01  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7409  data: 0.0008  max mem: 20467
Train Epoch: [7] Total time: 0:58:41 (1.7408 s / it)
Val Epoch: [7]  [  0/225]  eta: 0:03:55  wpa_loss: 0.0000  val_loss: 0.0000  time: 1.0455  data: 0.8090  max mem: 20467
Val Epoch: [7]  [ 50/225]  eta: 0:00:43  wpa_loss: 0.0000  val_loss: 0.0000  time: 0.2327  data: 0.0001  max mem: 20467
Val Epoch: [7]  [100/225]  eta: 0:00:30  wpa_loss: 0.0000  val_loss: 0.0000  time: 0.2330  data: 0.0001  max mem: 20467
Val Epoch: [7]  [150/225]  eta: 0:00:17  wpa_loss: 0.0001  val_loss: 0.0001  time: 0.2331  data: 0.0001  max mem: 20467
Val Epoch: [7]  [200/225]  eta: 0:00:05  wpa_loss: 0.0000  val_loss: 0.0000  time: 0.2334  data: 0.0002  max mem: 20467
Val Epoch: [7]  [224/225]  eta: 0:00:00  wpa_loss: 0.0004  val_loss: 0.0004  time: 0.2318  data: 0.0002  max mem: 20467
Val Epoch: [7] Total time: 0:00:54 (0.2410 s / it)
epoch:7, iter:16183, 2022,  train_loss: 1.3660370314028114e-05, valid_loss: 0.0010004675850480756, idiv_loss:0.0010004675850480756
Averaged stats: lr: 0.0000  wpa_loss: 0.0008  train_loss: 0.0008
epoch 7 1.3660370314028114e-05
Train Epoch: [8]  [   0/2023]  eta: 1:37:14  lr: 0.000010  wpa_loss: 0.0005  train_loss: 0.0005  time: 2.8843  data: 0.9907  max mem: 20467
Train Epoch: [8]  [  50/2023]  eta: 0:57:55  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7388  data: 0.0001  max mem: 20467
Train Epoch: [8]  [ 100/2023]  eta: 0:56:04  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7381  data: 0.0001  max mem: 20467
Train Epoch: [8]  [ 150/2023]  eta: 0:54:30  lr: 0.000010  wpa_loss: 0.0027  train_loss: 0.0027  time: 1.7399  data: 0.0001  max mem: 20467
Train Epoch: [8]  [ 200/2023]  eta: 0:53:00  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7378  data: 0.0001  max mem: 20467
Train Epoch: [8]  [ 250/2023]  eta: 0:51:31  lr: 0.000010  wpa_loss: 0.0026  train_loss: 0.0026  time: 1.7387  data: 0.0001  max mem: 20467
Train Epoch: [8]  [ 300/2023]  eta: 0:50:03  lr: 0.000010  wpa_loss: 0.0001  train_loss: 0.0001  time: 1.7417  data: 0.0001  max mem: 20467
Train Epoch: [8]  [ 350/2023]  eta: 0:48:35  lr: 0.000010  wpa_loss: 0.0001  train_loss: 0.0001  time: 1.7391  data: 0.0001  max mem: 20467
Train Epoch: [8]  [ 400/2023]  eta: 0:47:08  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7420  data: 0.0001  max mem: 20467
Train Epoch: [8]  [ 450/2023]  eta: 0:45:40  lr: 0.000010  wpa_loss: 0.0002  train_loss: 0.0002  time: 1.7423  data: 0.0001  max mem: 20467
Train Epoch: [8]  [ 500/2023]  eta: 0:44:12  lr: 0.000010  wpa_loss: 0.0001  train_loss: 0.0001  time: 1.7408  data: 0.0001  max mem: 20467
Train Epoch: [8]  [ 550/2023]  eta: 0:42:45  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7402  data: 0.0001  max mem: 20467
Train Epoch: [8]  [ 600/2023]  eta: 0:41:18  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7407  data: 0.0001  max mem: 20467
Train Epoch: [8]  [ 650/2023]  eta: 0:39:50  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7382  data: 0.0001  max mem: 20467
Train Epoch: [8]  [ 700/2023]  eta: 0:38:23  lr: 0.000010  wpa_loss: 0.0003  train_loss: 0.0003  time: 1.7408  data: 0.0001  max mem: 20467
Train Epoch: [8]  [ 750/2023]  eta: 0:36:56  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7426  data: 0.0001  max mem: 20467
Train Epoch: [8]  [ 800/2023]  eta: 0:35:29  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7374  data: 0.0001  max mem: 20467
Train Epoch: [8]  [ 850/2023]  eta: 0:34:02  lr: 0.000010  wpa_loss: 0.0001  train_loss: 0.0001  time: 1.7447  data: 0.0001  max mem: 20467
Train Epoch: [8]  [ 900/2023]  eta: 0:32:35  lr: 0.000010  wpa_loss: 0.0001  train_loss: 0.0001  time: 1.7399  data: 0.0001  max mem: 20467
Train Epoch: [8]  [ 950/2023]  eta: 0:31:08  lr: 0.000010  wpa_loss: 0.0001  train_loss: 0.0001  time: 1.7436  data: 0.0001  max mem: 20467
Train Epoch: [8]  [1000/2023]  eta: 0:29:41  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7392  data: 0.0001  max mem: 20467
Train Epoch: [8]  [1050/2023]  eta: 0:28:13  lr: 0.000010  wpa_loss: 0.0002  train_loss: 0.0002  time: 1.7388  data: 0.0001  max mem: 20467
Train Epoch: [8]  [1100/2023]  eta: 0:26:46  lr: 0.000010  wpa_loss: 0.0001  train_loss: 0.0001  time: 1.7390  data: 0.0001  max mem: 20467
Train Epoch: [8]  [1150/2023]  eta: 0:25:19  lr: 0.000010  wpa_loss: 0.0001  train_loss: 0.0001  time: 1.7377  data: 0.0001  max mem: 20467
Train Epoch: [8]  [1200/2023]  eta: 0:23:52  lr: 0.000010  wpa_loss: 0.0001  train_loss: 0.0001  time: 1.7385  data: 0.0001  max mem: 20467
Train Epoch: [8]  [1250/2023]  eta: 0:22:25  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7369  data: 0.0002  max mem: 20467
Train Epoch: [8]  [1300/2023]  eta: 0:20:58  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7379  data: 0.0002  max mem: 20467
Train Epoch: [8]  [1350/2023]  eta: 0:19:31  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7404  data: 0.0003  max mem: 20467
Train Epoch: [8]  [1400/2023]  eta: 0:18:04  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7425  data: 0.0003  max mem: 20467
Train Epoch: [8]  [1450/2023]  eta: 0:16:37  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7356  data: 0.0002  max mem: 20467
Train Epoch: [8]  [1500/2023]  eta: 0:15:10  lr: 0.000010  wpa_loss: 0.0002  train_loss: 0.0002  time: 1.7408  data: 0.0003  max mem: 20467
Train Epoch: [8]  [1550/2023]  eta: 0:13:43  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7364  data: 0.0002  max mem: 20467
Train Epoch: [8]  [1600/2023]  eta: 0:12:15  lr: 0.000010  wpa_loss: 0.0002  train_loss: 0.0002  time: 1.7330  data: 0.0003  max mem: 20467
Train Epoch: [8]  [1650/2023]  eta: 0:10:48  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7378  data: 0.0002  max mem: 20467
Train Epoch: [8]  [1700/2023]  eta: 0:09:21  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7364  data: 0.0003  max mem: 20467
Train Epoch: [8]  [1750/2023]  eta: 0:07:54  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7378  data: 0.0002  max mem: 20467
Train Epoch: [8]  [1800/2023]  eta: 0:06:27  lr: 0.000010  wpa_loss: 0.0001  train_loss: 0.0001  time: 1.7378  data: 0.0002  max mem: 20467
Train Epoch: [8]  [1850/2023]  eta: 0:05:00  lr: 0.000010  wpa_loss: 0.0001  train_loss: 0.0001  time: 1.7385  data: 0.0002  max mem: 20467
Train Epoch: [8]  [1900/2023]  eta: 0:03:33  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7370  data: 0.0002  max mem: 20467
Train Epoch: [8]  [1950/2023]  eta: 0:02:06  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7344  data: 0.0002  max mem: 20467
Train Epoch: [8]  [2000/2023]  eta: 0:00:40  lr: 0.000010  wpa_loss: 0.0002  train_loss: 0.0002  time: 1.7366  data: 0.0002  max mem: 20467
Train Epoch: [8]  [2022/2023]  eta: 0:00:01  lr: 0.000010  wpa_loss: 0.0001  train_loss: 0.0001  time: 1.7391  data: 0.0009  max mem: 20467
Train Epoch: [8] Total time: 0:58:39 (1.7399 s / it)
Val Epoch: [8]  [  0/225]  eta: 0:04:46  wpa_loss: 0.0000  val_loss: 0.0000  time: 1.2739  data: 1.0368  max mem: 20467
Val Epoch: [8]  [ 50/225]  eta: 0:00:44  wpa_loss: 0.0000  val_loss: 0.0000  time: 0.2327  data: 0.0001  max mem: 20467
Val Epoch: [8]  [100/225]  eta: 0:00:30  wpa_loss: 0.0000  val_loss: 0.0000  time: 0.2330  data: 0.0001  max mem: 20467
Val Epoch: [8]  [150/225]  eta: 0:00:17  wpa_loss: 0.0000  val_loss: 0.0000  time: 0.2332  data: 0.0001  max mem: 20467
Val Epoch: [8]  [200/225]  eta: 0:00:05  wpa_loss: 0.0001  val_loss: 0.0001  time: 0.2336  data: 0.0002  max mem: 20467
Val Epoch: [8]  [224/225]  eta: 0:00:00  wpa_loss: 0.0000  val_loss: 0.0000  time: 0.2318  data: 0.0002  max mem: 20467
Val Epoch: [8] Total time: 0:00:54 (0.2420 s / it)
epoch:8, iter:18206, 2022,  train_loss: 0.00012782089470420033, valid_loss: 0.0005558657291799641, idiv_loss:0.0005558657291799641
Averaged stats: lr: 0.0000  wpa_loss: 0.0006  train_loss: 0.0006
epoch 8 0.00012782089470420033
Train Epoch: [9]  [   0/2023]  eta: 1:36:37  lr: 0.000010  wpa_loss: 0.0001  train_loss: 0.0001  time: 2.8657  data: 0.9979  max mem: 20467
Train Epoch: [9]  [  50/2023]  eta: 0:57:54  lr: 0.000010  wpa_loss: 0.0029  train_loss: 0.0029  time: 1.7378  data: 0.0001  max mem: 20467
Train Epoch: [9]  [ 100/2023]  eta: 0:56:06  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7406  data: 0.0001  max mem: 20467
Train Epoch: [9]  [ 150/2023]  eta: 0:54:31  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7388  data: 0.0001  max mem: 20467
Train Epoch: [9]  [ 200/2023]  eta: 0:52:59  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7300  data: 0.0001  max mem: 20467
Train Epoch: [9]  [ 250/2023]  eta: 0:51:32  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7419  data: 0.0001  max mem: 20467
Train Epoch: [9]  [ 300/2023]  eta: 0:50:04  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7428  data: 0.0001  max mem: 20467
Train Epoch: [9]  [ 350/2023]  eta: 0:48:37  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7417  data: 0.0001  max mem: 20467
Train Epoch: [9]  [ 400/2023]  eta: 0:47:10  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7433  data: 0.0001  max mem: 20467
Train Epoch: [9]  [ 450/2023]  eta: 0:45:43  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7424  data: 0.0001  max mem: 20467
Train Epoch: [9]  [ 500/2023]  eta: 0:44:15  lr: 0.000010  wpa_loss: 0.0001  train_loss: 0.0001  time: 1.7405  data: 0.0001  max mem: 20467
Train Epoch: [9]  [ 550/2023]  eta: 0:42:48  lr: 0.000010  wpa_loss: 0.0007  train_loss: 0.0007  time: 1.7431  data: 0.0001  max mem: 20467
Train Epoch: [9]  [ 600/2023]  eta: 0:41:20  lr: 0.000010  wpa_loss: 0.0003  train_loss: 0.0003  time: 1.7400  data: 0.0001  max mem: 20467
Train Epoch: [9]  [ 650/2023]  eta: 0:39:53  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7406  data: 0.0001  max mem: 20467
Train Epoch: [9]  [ 700/2023]  eta: 0:38:25  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7342  data: 0.0001  max mem: 20467
Train Epoch: [9]  [ 750/2023]  eta: 0:36:58  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7394  data: 0.0001  max mem: 20467
Train Epoch: [9]  [ 800/2023]  eta: 0:35:30  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7386  data: 0.0001  max mem: 20467
Train Epoch: [9]  [ 850/2023]  eta: 0:34:03  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7407  data: 0.0001  max mem: 20467
Train Epoch: [9]  [ 900/2023]  eta: 0:32:36  lr: 0.000010  wpa_loss: 0.0001  train_loss: 0.0001  time: 1.7382  data: 0.0001  max mem: 20467
Train Epoch: [9]  [ 950/2023]  eta: 0:31:08  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7410  data: 0.0001  max mem: 20467
Train Epoch: [9]  [1000/2023]  eta: 0:29:41  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7422  data: 0.0001  max mem: 20467
Train Epoch: [9]  [1050/2023]  eta: 0:28:14  lr: 0.000010  wpa_loss: 0.0001  train_loss: 0.0001  time: 1.7387  data: 0.0001  max mem: 20467
Train Epoch: [9]  [1100/2023]  eta: 0:26:47  lr: 0.000010  wpa_loss: 0.0001  train_loss: 0.0001  time: 1.7388  data: 0.0001  max mem: 20467
Train Epoch: [9]  [1150/2023]  eta: 0:25:20  lr: 0.000010  wpa_loss: 0.0003  train_loss: 0.0003  time: 1.7422  data: 0.0001  max mem: 20467
Train Epoch: [9]  [1200/2023]  eta: 0:23:53  lr: 0.000010  wpa_loss: 0.0001  train_loss: 0.0001  time: 1.7354  data: 0.0001  max mem: 20467
Train Epoch: [9]  [1250/2023]  eta: 0:22:25  lr: 0.000010  wpa_loss: 0.0001  train_loss: 0.0001  time: 1.7361  data: 0.0001  max mem: 20467
Train Epoch: [9]  [1300/2023]  eta: 0:20:58  lr: 0.000010  wpa_loss: 0.0004  train_loss: 0.0004  time: 1.7369  data: 0.0001  max mem: 20467
Train Epoch: [9]  [1350/2023]  eta: 0:19:31  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7412  data: 0.0001  max mem: 20467
Train Epoch: [9]  [1400/2023]  eta: 0:18:04  lr: 0.000010  wpa_loss: 0.0001  train_loss: 0.0001  time: 1.7357  data: 0.0001  max mem: 20467
Train Epoch: [9]  [1450/2023]  eta: 0:16:37  lr: 0.000010  wpa_loss: 0.0021  train_loss: 0.0021  time: 1.7384  data: 0.0001  max mem: 20467
Train Epoch: [9]  [1500/2023]  eta: 0:15:10  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7386  data: 0.0001  max mem: 20467
Train Epoch: [9]  [1550/2023]  eta: 0:13:43  lr: 0.000010  wpa_loss: 0.0001  train_loss: 0.0001  time: 1.7413  data: 0.0001  max mem: 20467
Train Epoch: [9]  [1600/2023]  eta: 0:12:16  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7399  data: 0.0001  max mem: 20467
Train Epoch: [9]  [1650/2023]  eta: 0:10:49  lr: 0.000010  wpa_loss: 0.0009  train_loss: 0.0009  time: 1.7376  data: 0.0001  max mem: 20467
Train Epoch: [9]  [1700/2023]  eta: 0:09:22  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7397  data: 0.0001  max mem: 20467
Train Epoch: [9]  [1750/2023]  eta: 0:07:55  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7399  data: 0.0001  max mem: 20467
Train Epoch: [9]  [1800/2023]  eta: 0:06:28  lr: 0.000010  wpa_loss: 0.0003  train_loss: 0.0003  time: 1.7413  data: 0.0001  max mem: 20467
Train Epoch: [9]  [1850/2023]  eta: 0:05:01  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7391  data: 0.0001  max mem: 20467
Train Epoch: [9]  [1900/2023]  eta: 0:03:34  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7391  data: 0.0001  max mem: 20467
Train Epoch: [9]  [1950/2023]  eta: 0:02:07  lr: 0.000010  wpa_loss: 0.0060  train_loss: 0.0060  time: 1.7413  data: 0.0001  max mem: 20467
Train Epoch: [9]  [2000/2023]  eta: 0:00:40  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7428  data: 0.0001  max mem: 20467
Train Epoch: [9]  [2022/2023]  eta: 0:00:01  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7394  data: 0.0007  max mem: 20467
Train Epoch: [9] Total time: 0:58:42 (1.7413 s / it)
Val Epoch: [9]  [  0/225]  eta: 0:04:11  wpa_loss: 0.0000  val_loss: 0.0000  time: 1.1173  data: 0.8798  max mem: 20467
Val Epoch: [9]  [ 50/225]  eta: 0:00:43  wpa_loss: 0.0000  val_loss: 0.0000  time: 0.2328  data: 0.0001  max mem: 20467
Val Epoch: [9]  [100/225]  eta: 0:00:30  wpa_loss: 0.0000  val_loss: 0.0000  time: 0.2330  data: 0.0002  max mem: 20467
Val Epoch: [9]  [150/225]  eta: 0:00:17  wpa_loss: 0.0001  val_loss: 0.0001  time: 0.2331  data: 0.0001  max mem: 20467
Val Epoch: [9]  [200/225]  eta: 0:00:05  wpa_loss: 0.0000  val_loss: 0.0000  time: 0.2342  data: 0.0002  max mem: 20467
Val Epoch: [9]  [224/225]  eta: 0:00:00  wpa_loss: 0.0000  val_loss: 0.0000  time: 0.2320  data: 0.0003  max mem: 20467
Val Epoch: [9] Total time: 0:00:54 (0.2412 s / it)
epoch:9, iter:20229, 2022,  train_loss: 7.1918198045750614e-06, valid_loss: 0.0003314282504733354, idiv_loss:0.0003314282504733354
Averaged stats: lr: 0.0000  wpa_loss: 0.0005  train_loss: 0.0005
epoch 9 7.1918198045750614e-06
Train Epoch: [10]  [   0/2023]  eta: 1:36:35  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 2.8648  data: 0.9683  max mem: 20467
Train Epoch: [10]  [  50/2023]  eta: 0:57:57  lr: 0.000010  wpa_loss: 0.0006  train_loss: 0.0006  time: 1.7418  data: 0.0001  max mem: 20467
Train Epoch: [10]  [ 100/2023]  eta: 0:56:08  lr: 0.000010  wpa_loss: 0.0001  train_loss: 0.0001  time: 1.7370  data: 0.0001  max mem: 20467
Train Epoch: [10]  [ 150/2023]  eta: 0:54:34  lr: 0.000010  wpa_loss: 0.0006  train_loss: 0.0006  time: 1.7441  data: 0.0001  max mem: 20467
Train Epoch: [10]  [ 200/2023]  eta: 0:53:03  lr: 0.000010  wpa_loss: 0.0001  train_loss: 0.0001  time: 1.7399  data: 0.0001  max mem: 20467
Train Epoch: [10]  [ 250/2023]  eta: 0:51:33  lr: 0.000010  wpa_loss: 0.0004  train_loss: 0.0004  time: 1.7417  data: 0.0001  max mem: 20467
Train Epoch: [10]  [ 300/2023]  eta: 0:50:04  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7370  data: 0.0001  max mem: 20467
Train Epoch: [10]  [ 350/2023]  eta: 0:48:36  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7413  data: 0.0001  max mem: 20467
Train Epoch: [10]  [ 400/2023]  eta: 0:47:08  lr: 0.000010  wpa_loss: 0.0001  train_loss: 0.0001  time: 1.7393  data: 0.0001  max mem: 20467
Train Epoch: [10]  [ 450/2023]  eta: 0:45:40  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7353  data: 0.0001  max mem: 20467
Train Epoch: [10]  [ 500/2023]  eta: 0:44:12  lr: 0.000010  wpa_loss: 0.0001  train_loss: 0.0001  time: 1.7391  data: 0.0001  max mem: 20467
Train Epoch: [10]  [ 550/2023]  eta: 0:42:45  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7388  data: 0.0001  max mem: 20467
Train Epoch: [10]  [ 600/2023]  eta: 0:41:18  lr: 0.000010  wpa_loss: 0.0001  train_loss: 0.0001  time: 1.7412  data: 0.0001  max mem: 20467
Train Epoch: [10]  [ 650/2023]  eta: 0:39:51  lr: 0.000010  wpa_loss: 0.0001  train_loss: 0.0001  time: 1.7387  data: 0.0001  max mem: 20467
Train Epoch: [10]  [ 700/2023]  eta: 0:38:23  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7396  data: 0.0001  max mem: 20467
Train Epoch: [10]  [ 750/2023]  eta: 0:36:56  lr: 0.000010  wpa_loss: 0.0002  train_loss: 0.0002  time: 1.7403  data: 0.0001  max mem: 20467
Train Epoch: [10]  [ 800/2023]  eta: 0:35:29  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7385  data: 0.0002  max mem: 20467
Train Epoch: [10]  [ 850/2023]  eta: 0:34:02  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7359  data: 0.0001  max mem: 20467
Train Epoch: [10]  [ 900/2023]  eta: 0:32:34  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7402  data: 0.0001  max mem: 20467
Train Epoch: [10]  [ 950/2023]  eta: 0:31:07  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7366  data: 0.0002  max mem: 20467
Train Epoch: [10]  [1000/2023]  eta: 0:29:40  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7391  data: 0.0001  max mem: 20467
Train Epoch: [10]  [1050/2023]  eta: 0:28:13  lr: 0.000010  wpa_loss: 0.0001  train_loss: 0.0001  time: 1.7436  data: 0.0001  max mem: 20467
Train Epoch: [10]  [1100/2023]  eta: 0:26:46  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7380  data: 0.0001  max mem: 20467
Train Epoch: [10]  [1150/2023]  eta: 0:25:19  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7389  data: 0.0001  max mem: 20467
Train Epoch: [10]  [1200/2023]  eta: 0:23:52  lr: 0.000010  wpa_loss: 0.0001  train_loss: 0.0001  time: 1.7393  data: 0.0001  max mem: 20467
Train Epoch: [10]  [1250/2023]  eta: 0:22:25  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7364  data: 0.0001  max mem: 20467
Train Epoch: [10]  [1300/2023]  eta: 0:20:58  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7413  data: 0.0002  max mem: 20467
Train Epoch: [10]  [1350/2023]  eta: 0:19:31  lr: 0.000010  wpa_loss: 0.0010  train_loss: 0.0010  time: 1.7389  data: 0.0001  max mem: 20467
Train Epoch: [10]  [1400/2023]  eta: 0:18:04  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7411  data: 0.0001  max mem: 20467
Train Epoch: [10]  [1450/2023]  eta: 0:16:37  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7392  data: 0.0001  max mem: 20467
Train Epoch: [10]  [1500/2023]  eta: 0:15:10  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7406  data: 0.0001  max mem: 20467
Train Epoch: [10]  [1550/2023]  eta: 0:13:43  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7366  data: 0.0001  max mem: 20467
Train Epoch: [10]  [1600/2023]  eta: 0:12:16  lr: 0.000010  wpa_loss: 0.0001  train_loss: 0.0001  time: 1.7389  data: 0.0001  max mem: 20467
Train Epoch: [10]  [1650/2023]  eta: 0:10:48  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7369  data: 0.0002  max mem: 20467
Train Epoch: [10]  [1700/2023]  eta: 0:09:21  lr: 0.000010  wpa_loss: 0.0005  train_loss: 0.0005  time: 1.7365  data: 0.0001  max mem: 20467
Train Epoch: [10]  [1750/2023]  eta: 0:07:54  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7343  data: 0.0001  max mem: 20467
Train Epoch: [10]  [1800/2023]  eta: 0:06:27  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7372  data: 0.0001  max mem: 20467
Train Epoch: [10]  [1850/2023]  eta: 0:05:00  lr: 0.000010  wpa_loss: 0.0003  train_loss: 0.0003  time: 1.7382  data: 0.0001  max mem: 20467
Train Epoch: [10]  [1900/2023]  eta: 0:03:33  lr: 0.000010  wpa_loss: 0.0005  train_loss: 0.0005  time: 1.7350  data: 0.0001  max mem: 20467
Train Epoch: [10]  [1950/2023]  eta: 0:02:06  lr: 0.000010  wpa_loss: 0.0058  train_loss: 0.0058  time: 1.7384  data: 0.0001  max mem: 20467
Train Epoch: [10]  [2000/2023]  eta: 0:00:40  lr: 0.000010  wpa_loss: 0.0001  train_loss: 0.0001  time: 1.8355  data: 0.0001  max mem: 20467
Train Epoch: [10]  [2022/2023]  eta: 0:00:01  lr: 0.000010  wpa_loss: 0.0015  train_loss: 0.0015  time: 1.7364  data: 0.0008  max mem: 20467
Train Epoch: [10] Total time: 0:58:44 (1.7420 s / it)
Val Epoch: [10]  [  0/225]  eta: 0:04:59  wpa_loss: 0.0001  val_loss: 0.0001  time: 1.3308  data: 1.0949  max mem: 20467
Val Epoch: [10]  [ 50/225]  eta: 0:00:44  wpa_loss: 0.0001  val_loss: 0.0001  time: 0.2326  data: 0.0002  max mem: 20467
Val Epoch: [10]  [100/225]  eta: 0:00:30  wpa_loss: 0.0000  val_loss: 0.0000  time: 0.2328  data: 0.0001  max mem: 20467
Val Epoch: [10]  [150/225]  eta: 0:00:18  wpa_loss: 0.0001  val_loss: 0.0001  time: 0.2330  data: 0.0001  max mem: 20467
Val Epoch: [10]  [200/225]  eta: 0:00:05  wpa_loss: 0.0002  val_loss: 0.0002  time: 0.2335  data: 0.0001  max mem: 20467
Val Epoch: [10]  [224/225]  eta: 0:00:00  wpa_loss: 0.0001  val_loss: 0.0001  time: 0.2318  data: 0.0002  max mem: 20467
Val Epoch: [10] Total time: 0:00:54 (0.2421 s / it)
epoch:10, iter:22252, 2022,  train_loss: 0.001529033645056188, valid_loss: 0.0008429755685372382, idiv_loss:0.0008429755685372382
Averaged stats: lr: 0.0000  wpa_loss: 0.0005  train_loss: 0.0005
./src/pretrain_wpa.py:58: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig = plt.figure()
epoch 10 0.001529033645056188
Train Epoch: [11]  [   0/2023]  eta: 1:35:47  lr: 0.000010  wpa_loss: 0.0005  train_loss: 0.0005  time: 2.8413  data: 0.9859  max mem: 20467
Train Epoch: [11]  [  50/2023]  eta: 0:57:56  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7416  data: 0.0001  max mem: 20467
Train Epoch: [11]  [ 100/2023]  eta: 0:56:08  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7417  data: 0.0001  max mem: 20467
Train Epoch: [11]  [ 150/2023]  eta: 0:54:32  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7375  data: 0.0001  max mem: 20467
Train Epoch: [11]  [ 200/2023]  eta: 0:53:00  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7384  data: 0.0001  max mem: 20467
Train Epoch: [11]  [ 250/2023]  eta: 0:51:31  lr: 0.000010  wpa_loss: 0.0004  train_loss: 0.0004  time: 1.7394  data: 0.0001  max mem: 20467
Train Epoch: [11]  [ 300/2023]  eta: 0:50:02  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7374  data: 0.0001  max mem: 20467
Train Epoch: [11]  [ 350/2023]  eta: 0:48:34  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7386  data: 0.0001  max mem: 20467
Train Epoch: [11]  [ 400/2023]  eta: 0:47:06  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7379  data: 0.0001  max mem: 20467
Train Epoch: [11]  [ 450/2023]  eta: 0:45:38  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7376  data: 0.0001  max mem: 20467
Train Epoch: [11]  [ 500/2023]  eta: 0:44:11  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7376  data: 0.0001  max mem: 20467
Train Epoch: [11]  [ 550/2023]  eta: 0:42:43  lr: 0.000010  wpa_loss: 0.0002  train_loss: 0.0002  time: 1.7377  data: 0.0001  max mem: 20467
Train Epoch: [11]  [ 600/2023]  eta: 0:41:16  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7391  data: 0.0001  max mem: 20467
Train Epoch: [11]  [ 650/2023]  eta: 0:39:48  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7380  data: 0.0001  max mem: 20467
Train Epoch: [11]  [ 700/2023]  eta: 0:38:21  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7405  data: 0.0001  max mem: 20467
Train Epoch: [11]  [ 750/2023]  eta: 0:36:54  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7357  data: 0.0001  max mem: 20467
Train Epoch: [11]  [ 800/2023]  eta: 0:35:27  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7371  data: 0.0001  max mem: 20467
Train Epoch: [11]  [ 850/2023]  eta: 0:34:00  lr: 0.000010  wpa_loss: 0.0007  train_loss: 0.0007  time: 1.7378  data: 0.0001  max mem: 20467
Train Epoch: [11]  [ 900/2023]  eta: 0:32:33  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7378  data: 0.0001  max mem: 20467
Train Epoch: [11]  [ 950/2023]  eta: 0:31:05  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7371  data: 0.0001  max mem: 20467
Train Epoch: [11]  [1000/2023]  eta: 0:29:38  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7325  data: 0.0001  max mem: 20467
Train Epoch: [11]  [1050/2023]  eta: 0:28:11  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7378  data: 0.0001  max mem: 20467
Train Epoch: [11]  [1100/2023]  eta: 0:26:44  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7377  data: 0.0001  max mem: 20467
Train Epoch: [11]  [1150/2023]  eta: 0:25:17  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7371  data: 0.0001  max mem: 20467
Train Epoch: [11]  [1200/2023]  eta: 0:23:50  lr: 0.000010  wpa_loss: 0.0001  train_loss: 0.0001  time: 1.7361  data: 0.0001  max mem: 20467
Train Epoch: [11]  [1250/2023]  eta: 0:22:24  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7377  data: 0.0001  max mem: 20467
Train Epoch: [11]  [1300/2023]  eta: 0:20:57  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7364  data: 0.0001  max mem: 20467
Train Epoch: [11]  [1350/2023]  eta: 0:19:30  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7415  data: 0.0001  max mem: 20467
Train Epoch: [11]  [1400/2023]  eta: 0:18:03  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7430  data: 0.0001  max mem: 20467
Train Epoch: [11]  [1450/2023]  eta: 0:16:36  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7389  data: 0.0001  max mem: 20467
Train Epoch: [11]  [1500/2023]  eta: 0:15:09  lr: 0.000010  wpa_loss: 0.0022  train_loss: 0.0022  time: 1.7415  data: 0.0001  max mem: 20467
Train Epoch: [11]  [1550/2023]  eta: 0:13:42  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7377  data: 0.0001  max mem: 20467
Train Epoch: [11]  [1600/2023]  eta: 0:12:15  lr: 0.000010  wpa_loss: 0.0003  train_loss: 0.0003  time: 1.7373  data: 0.0001  max mem: 20467
Train Epoch: [11]  [1650/2023]  eta: 0:10:48  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7367  data: 0.0001  max mem: 20467
Train Epoch: [11]  [1700/2023]  eta: 0:09:21  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7337  data: 0.0001  max mem: 20467
Train Epoch: [11]  [1750/2023]  eta: 0:07:54  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7399  data: 0.0001  max mem: 20467
Train Epoch: [11]  [1800/2023]  eta: 0:06:27  lr: 0.000010  wpa_loss: 0.0006  train_loss: 0.0006  time: 1.7368  data: 0.0001  max mem: 20467
Train Epoch: [11]  [1850/2023]  eta: 0:05:00  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7375  data: 0.0001  max mem: 20467
Train Epoch: [11]  [1900/2023]  eta: 0:03:33  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7398  data: 0.0001  max mem: 20467
Train Epoch: [11]  [1950/2023]  eta: 0:02:06  lr: 0.000010  wpa_loss: 0.0002  train_loss: 0.0002  time: 1.7383  data: 0.0001  max mem: 20467
Train Epoch: [11]  [2000/2023]  eta: 0:00:39  lr: 0.000010  wpa_loss: 0.0002  train_loss: 0.0002  time: 1.7363  data: 0.0001  max mem: 20467
Train Epoch: [11]  [2022/2023]  eta: 0:00:01  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7386  data: 0.0007  max mem: 20467
Train Epoch: [11] Total time: 0:58:37 (1.7389 s / it)
Val Epoch: [11]  [  0/225]  eta: 0:04:53  wpa_loss: 0.0000  val_loss: 0.0000  time: 1.3032  data: 1.0663  max mem: 20467
Val Epoch: [11]  [ 50/225]  eta: 0:00:44  wpa_loss: 0.0000  val_loss: 0.0000  time: 0.2328  data: 0.0001  max mem: 20467
Val Epoch: [11]  [100/225]  eta: 0:00:30  wpa_loss: 0.0000  val_loss: 0.0000  time: 0.2330  data: 0.0001  max mem: 20467
Val Epoch: [11]  [150/225]  eta: 0:00:18  wpa_loss: 0.0000  val_loss: 0.0000  time: 0.2334  data: 0.0002  max mem: 20467
Val Epoch: [11]  [200/225]  eta: 0:00:05  wpa_loss: 0.0000  val_loss: 0.0000  time: 0.2337  data: 0.0002  max mem: 20467
Val Epoch: [11]  [224/225]  eta: 0:00:00  wpa_loss: 0.0000  val_loss: 0.0000  time: 0.2319  data: 0.0002  max mem: 20467
Val Epoch: [11] Total time: 0:00:54 (0.2438 s / it)
epoch:11, iter:24275, 2022,  train_loss: 1.9243278075009584e-05, valid_loss: 0.0005312628869180319, idiv_loss:0.0005312628869180319
Averaged stats: lr: 0.0000  wpa_loss: 0.0005  train_loss: 0.0005
epoch 11 1.9243278075009584e-05
Train Epoch: [12]  [   0/2023]  eta: 1:35:55  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 2.8453  data: 1.0660  max mem: 20467
Train Epoch: [12]  [  50/2023]  eta: 0:57:47  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7351  data: 0.0001  max mem: 20467
Train Epoch: [12]  [ 100/2023]  eta: 0:56:02  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7396  data: 0.0001  max mem: 20467
Train Epoch: [12]  [ 150/2023]  eta: 0:54:27  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7378  data: 0.0002  max mem: 20467
Train Epoch: [12]  [ 200/2023]  eta: 0:52:58  lr: 0.000010  wpa_loss: 0.0101  train_loss: 0.0101  time: 1.7398  data: 0.0002  max mem: 20467
Train Epoch: [12]  [ 250/2023]  eta: 0:51:29  lr: 0.000010  wpa_loss: 0.0001  train_loss: 0.0001  time: 1.7358  data: 0.0001  max mem: 20467
Train Epoch: [12]  [ 300/2023]  eta: 0:50:01  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7398  data: 0.0001  max mem: 20467
Train Epoch: [12]  [ 350/2023]  eta: 0:48:33  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7375  data: 0.0002  max mem: 20467
Train Epoch: [12]  [ 400/2023]  eta: 0:47:05  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7350  data: 0.0001  max mem: 20467
Train Epoch: [12]  [ 450/2023]  eta: 0:45:37  lr: 0.000010  wpa_loss: 0.0001  train_loss: 0.0001  time: 1.7381  data: 0.0001  max mem: 20467
Train Epoch: [12]  [ 500/2023]  eta: 0:44:09  lr: 0.000010  wpa_loss: 0.0002  train_loss: 0.0002  time: 1.7339  data: 0.0002  max mem: 20467
Train Epoch: [12]  [ 550/2023]  eta: 0:42:42  lr: 0.000010  wpa_loss: 0.0002  train_loss: 0.0002  time: 1.7401  data: 0.0001  max mem: 20467
Train Epoch: [12]  [ 600/2023]  eta: 0:41:15  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7366  data: 0.0001  max mem: 20467
Train Epoch: [12]  [ 650/2023]  eta: 0:39:47  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7356  data: 0.0001  max mem: 20467
Train Epoch: [12]  [ 700/2023]  eta: 0:38:20  lr: 0.000010  wpa_loss: 0.0001  train_loss: 0.0001  time: 1.7362  data: 0.0002  max mem: 20467
Train Epoch: [12]  [ 750/2023]  eta: 0:36:53  lr: 0.000010  wpa_loss: 0.0004  train_loss: 0.0004  time: 1.7391  data: 0.0003  max mem: 20467
Train Epoch: [12]  [ 800/2023]  eta: 0:35:26  lr: 0.000010  wpa_loss: 0.0001  train_loss: 0.0001  time: 1.7381  data: 0.0003  max mem: 20467
Train Epoch: [12]  [ 850/2023]  eta: 0:33:59  lr: 0.000010  wpa_loss: 0.0002  train_loss: 0.0002  time: 1.7348  data: 0.0003  max mem: 20467
Train Epoch: [12]  [ 900/2023]  eta: 0:32:32  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7393  data: 0.0003  max mem: 20467
Train Epoch: [12]  [ 950/2023]  eta: 0:31:05  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7360  data: 0.0003  max mem: 20467
Train Epoch: [12]  [1000/2023]  eta: 0:29:38  lr: 0.000010  wpa_loss: 0.0001  train_loss: 0.0001  time: 1.7354  data: 0.0003  max mem: 20467
Train Epoch: [12]  [1050/2023]  eta: 0:28:11  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7319  data: 0.0003  max mem: 20467
Train Epoch: [12]  [1100/2023]  eta: 0:26:44  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7345  data: 0.0003  max mem: 20467
Train Epoch: [12]  [1150/2023]  eta: 0:25:17  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7377  data: 0.0003  max mem: 20467
Train Epoch: [12]  [1200/2023]  eta: 0:23:50  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7351  data: 0.0003  max mem: 20467
Train Epoch: [12]  [1250/2023]  eta: 0:22:23  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7377  data: 0.0002  max mem: 20467
Train Epoch: [12]  [1300/2023]  eta: 0:20:56  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7361  data: 0.0003  max mem: 20467
Train Epoch: [12]  [1350/2023]  eta: 0:19:29  lr: 0.000010  wpa_loss: 0.0001  train_loss: 0.0001  time: 1.7340  data: 0.0002  max mem: 20467
Train Epoch: [12]  [1400/2023]  eta: 0:18:02  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7333  data: 0.0002  max mem: 20467
Train Epoch: [12]  [1450/2023]  eta: 0:16:35  lr: 0.000010  wpa_loss: 0.0002  train_loss: 0.0002  time: 1.7325  data: 0.0002  max mem: 20467
Train Epoch: [12]  [1500/2023]  eta: 0:15:08  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7345  data: 0.0003  max mem: 20467
Train Epoch: [12]  [1550/2023]  eta: 0:13:41  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7323  data: 0.0003  max mem: 20467
Train Epoch: [12]  [1600/2023]  eta: 0:12:14  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7348  data: 0.0002  max mem: 20467
Train Epoch: [12]  [1650/2023]  eta: 0:10:47  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7336  data: 0.0002  max mem: 20467
Train Epoch: [12]  [1700/2023]  eta: 0:09:21  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7335  data: 0.0002  max mem: 20467
Train Epoch: [12]  [1750/2023]  eta: 0:07:54  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7321  data: 0.0003  max mem: 20467
Train Epoch: [12]  [1800/2023]  eta: 0:06:27  lr: 0.000010  wpa_loss: 0.0001  train_loss: 0.0001  time: 1.7358  data: 0.0003  max mem: 20467
Train Epoch: [12]  [1850/2023]  eta: 0:05:00  lr: 0.000010  wpa_loss: 0.0001  train_loss: 0.0001  time: 1.7336  data: 0.0003  max mem: 20467
Train Epoch: [12]  [1900/2023]  eta: 0:03:33  lr: 0.000010  wpa_loss: 0.0001  train_loss: 0.0001  time: 1.7363  data: 0.0003  max mem: 20467
Train Epoch: [12]  [1950/2023]  eta: 0:02:06  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7355  data: 0.0003  max mem: 20467
Train Epoch: [12]  [2000/2023]  eta: 0:00:39  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7365  data: 0.0002  max mem: 20467
Train Epoch: [12]  [2022/2023]  eta: 0:00:01  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7371  data: 0.0008  max mem: 20467
Train Epoch: [12] Total time: 0:58:34 (1.7375 s / it)
Val Epoch: [12]  [  0/225]  eta: 0:04:12  wpa_loss: 0.0000  val_loss: 0.0000  time: 1.1220  data: 0.8848  max mem: 20467
Val Epoch: [12]  [ 50/225]  eta: 0:00:43  wpa_loss: 0.0000  val_loss: 0.0000  time: 0.2327  data: 0.0001  max mem: 20467
Val Epoch: [12]  [100/225]  eta: 0:00:30  wpa_loss: 0.0000  val_loss: 0.0000  time: 0.2330  data: 0.0001  max mem: 20467
Val Epoch: [12]  [150/225]  eta: 0:00:17  wpa_loss: 0.0000  val_loss: 0.0000  time: 0.2331  data: 0.0001  max mem: 20467
Val Epoch: [12]  [200/225]  eta: 0:00:05  wpa_loss: 0.0001  val_loss: 0.0001  time: 0.2334  data: 0.0001  max mem: 20467
Val Epoch: [12]  [224/225]  eta: 0:00:00  wpa_loss: 0.0000  val_loss: 0.0000  time: 0.2316  data: 0.0002  max mem: 20467
Val Epoch: [12] Total time: 0:00:54 (0.2411 s / it)
epoch:12, iter:26298, 2022,  train_loss: 4.811029612028506e-06, valid_loss: 0.0006526518874660623, idiv_loss:0.0006526518874660623
Averaged stats: lr: 0.0000  wpa_loss: 0.0004  train_loss: 0.0004
epoch 12 4.811029612028506e-06
Train Epoch: [13]  [   0/2023]  eta: 1:35:10  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 2.8227  data: 0.9755  max mem: 20467
Train Epoch: [13]  [  50/2023]  eta: 0:57:52  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7367  data: 0.0001  max mem: 20467
Train Epoch: [13]  [ 100/2023]  eta: 0:56:02  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7370  data: 0.0001  max mem: 20467
Train Epoch: [13]  [ 150/2023]  eta: 0:54:27  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7365  data: 0.0001  max mem: 20467
Train Epoch: [13]  [ 200/2023]  eta: 0:52:57  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7395  data: 0.0001  max mem: 20467
Train Epoch: [13]  [ 250/2023]  eta: 0:51:28  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7372  data: 0.0001  max mem: 20467
Train Epoch: [13]  [ 300/2023]  eta: 0:50:00  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7365  data: 0.0001  max mem: 20467
Train Epoch: [13]  [ 350/2023]  eta: 0:48:31  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7388  data: 0.0001  max mem: 20467
Train Epoch: [13]  [ 400/2023]  eta: 0:47:03  lr: 0.000010  wpa_loss: 0.0008  train_loss: 0.0008  time: 1.7356  data: 0.0001  max mem: 20467
Train Epoch: [13]  [ 450/2023]  eta: 0:45:35  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7350  data: 0.0001  max mem: 20467
Train Epoch: [13]  [ 500/2023]  eta: 0:44:08  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7371  data: 0.0001  max mem: 20467
Train Epoch: [13]  [ 550/2023]  eta: 0:42:41  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7393  data: 0.0001  max mem: 20467
Train Epoch: [13]  [ 600/2023]  eta: 0:41:13  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7358  data: 0.0001  max mem: 20467
Train Epoch: [13]  [ 650/2023]  eta: 0:39:46  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7350  data: 0.0001  max mem: 20467
Train Epoch: [13]  [ 700/2023]  eta: 0:38:19  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7389  data: 0.0001  max mem: 20467
Train Epoch: [13]  [ 750/2023]  eta: 0:36:52  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7381  data: 0.0001  max mem: 20467
Train Epoch: [13]  [ 800/2023]  eta: 0:35:25  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7380  data: 0.0001  max mem: 20467
Train Epoch: [13]  [ 850/2023]  eta: 0:33:58  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7384  data: 0.0001  max mem: 20467
Train Epoch: [13]  [ 900/2023]  eta: 0:32:31  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7362  data: 0.0001  max mem: 20467
Train Epoch: [13]  [ 950/2023]  eta: 0:31:04  lr: 0.000010  wpa_loss: 0.0007  train_loss: 0.0007  time: 1.7392  data: 0.0001  max mem: 20467
Train Epoch: [13]  [1000/2023]  eta: 0:29:37  lr: 0.000010  wpa_loss: 0.0001  train_loss: 0.0001  time: 1.7389  data: 0.0001  max mem: 20467
Train Epoch: [13]  [1050/2023]  eta: 0:28:11  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7361  data: 0.0001  max mem: 20467
Train Epoch: [13]  [1100/2023]  eta: 0:26:43  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7336  data: 0.0001  max mem: 20467
Train Epoch: [13]  [1150/2023]  eta: 0:25:17  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7330  data: 0.0001  max mem: 20467
Train Epoch: [13]  [1200/2023]  eta: 0:23:50  lr: 0.000010  wpa_loss: 0.0088  train_loss: 0.0088  time: 1.7372  data: 0.0001  max mem: 20467
Train Epoch: [13]  [1250/2023]  eta: 0:22:23  lr: 0.000010  wpa_loss: 0.0003  train_loss: 0.0003  time: 1.7360  data: 0.0001  max mem: 20467
Train Epoch: [13]  [1300/2023]  eta: 0:20:56  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7340  data: 0.0001  max mem: 20467
Train Epoch: [13]  [1350/2023]  eta: 0:19:29  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7378  data: 0.0001  max mem: 20467
Train Epoch: [13]  [1400/2023]  eta: 0:18:02  lr: 0.000010  wpa_loss: 0.0001  train_loss: 0.0001  time: 1.7325  data: 0.0001  max mem: 20467
Train Epoch: [13]  [1450/2023]  eta: 0:16:35  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7345  data: 0.0001  max mem: 20467
Train Epoch: [13]  [1500/2023]  eta: 0:15:08  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7377  data: 0.0001  max mem: 20467
Train Epoch: [13]  [1550/2023]  eta: 0:13:41  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7359  data: 0.0001  max mem: 20467
Train Epoch: [13]  [1600/2023]  eta: 0:12:14  lr: 0.000010  wpa_loss: 0.0001  train_loss: 0.0001  time: 1.7374  data: 0.0001  max mem: 20467
Train Epoch: [13]  [1650/2023]  eta: 0:10:47  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7351  data: 0.0001  max mem: 20467
Train Epoch: [13]  [1700/2023]  eta: 0:09:21  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7375  data: 0.0001  max mem: 20467
Train Epoch: [13]  [1750/2023]  eta: 0:07:54  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7344  data: 0.0001  max mem: 20467
Train Epoch: [13]  [1800/2023]  eta: 0:06:27  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7345  data: 0.0001  max mem: 20467
Train Epoch: [13]  [1850/2023]  eta: 0:05:00  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7397  data: 0.0001  max mem: 20467
Train Epoch: [13]  [1900/2023]  eta: 0:03:33  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7346  data: 0.0001  max mem: 20467
Train Epoch: [13]  [1950/2023]  eta: 0:02:06  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7372  data: 0.0001  max mem: 20467
Train Epoch: [13]  [2000/2023]  eta: 0:00:39  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7375  data: 0.0001  max mem: 20467
Train Epoch: [13]  [2022/2023]  eta: 0:00:01  lr: 0.000010  wpa_loss: 0.0018  train_loss: 0.0018  time: 1.7380  data: 0.0007  max mem: 20467
Train Epoch: [13] Total time: 0:58:35 (1.7377 s / it)
Val Epoch: [13]  [  0/225]  eta: 0:04:54  wpa_loss: 0.0000  val_loss: 0.0000  time: 1.3070  data: 1.0701  max mem: 20467
Val Epoch: [13]  [ 50/225]  eta: 0:00:44  wpa_loss: 0.0000  val_loss: 0.0000  time: 0.2327  data: 0.0001  max mem: 20467
Val Epoch: [13]  [100/225]  eta: 0:00:30  wpa_loss: 0.0000  val_loss: 0.0000  time: 0.2329  data: 0.0002  max mem: 20467
Val Epoch: [13]  [150/225]  eta: 0:00:17  wpa_loss: 0.0000  val_loss: 0.0000  time: 0.2332  data: 0.0001  max mem: 20467
Val Epoch: [13]  [200/225]  eta: 0:00:05  wpa_loss: 0.0000  val_loss: 0.0000  time: 0.2334  data: 0.0001  max mem: 20467
Val Epoch: [13]  [224/225]  eta: 0:00:00  wpa_loss: 0.0000  val_loss: 0.0000  time: 0.2317  data: 0.0002  max mem: 20467
Val Epoch: [13] Total time: 0:00:54 (0.2437 s / it)
epoch:13, iter:28321, 2022,  train_loss: 0.0017599216662347317, valid_loss: 0.0010401579006106153, idiv_loss:0.0010401579006106153
Averaged stats: lr: 0.0000  wpa_loss: 0.0003  train_loss: 0.0003
epoch 13 0.0017599216662347317
Train Epoch: [14]  [   0/2023]  eta: 1:34:43  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 2.8097  data: 1.0344  max mem: 20467
Train Epoch: [14]  [  50/2023]  eta: 0:57:52  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7397  data: 0.0001  max mem: 20467
Train Epoch: [14]  [ 100/2023]  eta: 0:56:04  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7383  data: 0.0001  max mem: 20467
Train Epoch: [14]  [ 150/2023]  eta: 0:54:31  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7401  data: 0.0001  max mem: 20467
Train Epoch: [14]  [ 200/2023]  eta: 0:53:01  lr: 0.000010  wpa_loss: 0.0034  train_loss: 0.0034  time: 1.7426  data: 0.0001  max mem: 20467
Train Epoch: [14]  [ 250/2023]  eta: 0:51:32  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7384  data: 0.0001  max mem: 20467
Train Epoch: [14]  [ 300/2023]  eta: 0:50:03  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7378  data: 0.0001  max mem: 20467
Train Epoch: [14]  [ 350/2023]  eta: 0:48:35  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7375  data: 0.0002  max mem: 20467
Train Epoch: [14]  [ 400/2023]  eta: 0:47:07  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7415  data: 0.0001  max mem: 20467
Train Epoch: [14]  [ 450/2023]  eta: 0:45:39  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7396  data: 0.0001  max mem: 20467
Train Epoch: [14]  [ 500/2023]  eta: 0:44:12  lr: 0.000010  wpa_loss: 0.0001  train_loss: 0.0001  time: 1.7394  data: 0.0001  max mem: 20467
Train Epoch: [14]  [ 550/2023]  eta: 0:42:45  lr: 0.000010  wpa_loss: 0.0047  train_loss: 0.0047  time: 1.7386  data: 0.0001  max mem: 20467
Train Epoch: [14]  [ 600/2023]  eta: 0:41:17  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7417  data: 0.0001  max mem: 20467
Train Epoch: [14]  [ 650/2023]  eta: 0:39:50  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7390  data: 0.0001  max mem: 20467
Train Epoch: [14]  [ 700/2023]  eta: 0:38:23  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7414  data: 0.0002  max mem: 20467
Train Epoch: [14]  [ 750/2023]  eta: 0:36:56  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7405  data: 0.0003  max mem: 20467
Train Epoch: [14]  [ 800/2023]  eta: 0:35:29  lr: 0.000010  wpa_loss: 0.0012  train_loss: 0.0012  time: 1.7391  data: 0.0002  max mem: 20467
Train Epoch: [14]  [ 850/2023]  eta: 0:34:01  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7371  data: 0.0002  max mem: 20467
Train Epoch: [14]  [ 900/2023]  eta: 0:32:34  lr: 0.000010  wpa_loss: 0.0001  train_loss: 0.0001  time: 1.7373  data: 0.0001  max mem: 20467
Train Epoch: [14]  [ 950/2023]  eta: 0:31:07  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7348  data: 0.0001  max mem: 20467
Train Epoch: [14]  [1000/2023]  eta: 0:29:40  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7386  data: 0.0001  max mem: 20467
Train Epoch: [14]  [1050/2023]  eta: 0:28:13  lr: 0.000010  wpa_loss: 0.0002  train_loss: 0.0002  time: 1.7351  data: 0.0001  max mem: 20467
Train Epoch: [14]  [1100/2023]  eta: 0:26:46  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7366  data: 0.0001  max mem: 20467
Train Epoch: [14]  [1150/2023]  eta: 0:25:18  lr: 0.000010  wpa_loss: 0.0005  train_loss: 0.0005  time: 1.7396  data: 0.0001  max mem: 20467
Train Epoch: [14]  [1200/2023]  eta: 0:23:51  lr: 0.000010  wpa_loss: 0.0003  train_loss: 0.0003  time: 1.7370  data: 0.0001  max mem: 20467
Train Epoch: [14]  [1250/2023]  eta: 0:22:24  lr: 0.000010  wpa_loss: 0.0005  train_loss: 0.0005  time: 1.7342  data: 0.0001  max mem: 20467
Train Epoch: [14]  [1300/2023]  eta: 0:20:57  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7378  data: 0.0001  max mem: 20467
Train Epoch: [14]  [1350/2023]  eta: 0:19:30  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7359  data: 0.0001  max mem: 20467
Train Epoch: [14]  [1400/2023]  eta: 0:18:03  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7350  data: 0.0001  max mem: 20467
Train Epoch: [14]  [1450/2023]  eta: 0:16:36  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7372  data: 0.0001  max mem: 20467
Train Epoch: [14]  [1500/2023]  eta: 0:15:09  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7339  data: 0.0001  max mem: 20467
Train Epoch: [14]  [1550/2023]  eta: 0:13:42  lr: 0.000010  wpa_loss: 0.0001  train_loss: 0.0001  time: 1.7365  data: 0.0001  max mem: 20467
Train Epoch: [14]  [1600/2023]  eta: 0:12:15  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7381  data: 0.0001  max mem: 20467
Train Epoch: [14]  [1650/2023]  eta: 0:10:48  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7354  data: 0.0001  max mem: 20467
Train Epoch: [14]  [1700/2023]  eta: 0:09:21  lr: 0.000010  wpa_loss: 0.0005  train_loss: 0.0005  time: 1.7371  data: 0.0001  max mem: 20467
Train Epoch: [14]  [1750/2023]  eta: 0:07:54  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7370  data: 0.0001  max mem: 20467
Train Epoch: [14]  [1800/2023]  eta: 0:06:27  lr: 0.000010  wpa_loss: 0.0047  train_loss: 0.0047  time: 1.7367  data: 0.0001  max mem: 20467
Train Epoch: [14]  [1850/2023]  eta: 0:05:00  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7387  data: 0.0001  max mem: 20467
Train Epoch: [14]  [1900/2023]  eta: 0:03:33  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7339  data: 0.0001  max mem: 20467
Train Epoch: [14]  [1950/2023]  eta: 0:02:06  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7368  data: 0.0001  max mem: 20467
Train Epoch: [14]  [2000/2023]  eta: 0:00:39  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7363  data: 0.0001  max mem: 20467
Train Epoch: [14]  [2022/2023]  eta: 0:00:01  lr: 0.000010  wpa_loss: 0.0001  train_loss: 0.0001  time: 1.7345  data: 0.0007  max mem: 20467
Train Epoch: [14] Total time: 0:58:37 (1.7388 s / it)
Val Epoch: [14]  [  0/225]  eta: 0:04:58  wpa_loss: 0.0005  val_loss: 0.0005  time: 1.3249  data: 1.0872  max mem: 20467
Val Epoch: [14]  [ 50/225]  eta: 0:00:44  wpa_loss: 0.0000  val_loss: 0.0000  time: 0.2327  data: 0.0001  max mem: 20467
Val Epoch: [14]  [100/225]  eta: 0:00:30  wpa_loss: 0.0000  val_loss: 0.0000  time: 0.2331  data: 0.0002  max mem: 20467
Val Epoch: [14]  [150/225]  eta: 0:00:18  wpa_loss: 0.0000  val_loss: 0.0000  time: 0.2334  data: 0.0001  max mem: 20467
Val Epoch: [14]  [200/225]  eta: 0:00:05  wpa_loss: 0.0000  val_loss: 0.0000  time: 0.2336  data: 0.0002  max mem: 20467
Val Epoch: [14]  [224/225]  eta: 0:00:00  wpa_loss: 0.0000  val_loss: 0.0000  time: 0.2316  data: 0.0002  max mem: 20467
Val Epoch: [14] Total time: 0:00:54 (0.2421 s / it)
epoch:14, iter:30344, 2022,  train_loss: 6.695695628877729e-05, valid_loss: 0.0013104799209688483, idiv_loss:0.0013104799209688483
Averaged stats: lr: 0.0000  wpa_loss: 0.0004  train_loss: 0.0004
epoch 14 6.695695628877729e-05
Train Epoch: [15]  [   0/2023]  eta: 1:35:11  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 2.8234  data: 1.0346  max mem: 20467
Train Epoch: [15]  [  50/2023]  eta: 0:57:50  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7383  data: 0.0001  max mem: 20467
Train Epoch: [15]  [ 100/2023]  eta: 0:56:03  lr: 0.000010  wpa_loss: 0.0001  train_loss: 0.0001  time: 1.7389  data: 0.0001  max mem: 20467
Train Epoch: [15]  [ 150/2023]  eta: 0:54:29  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7358  data: 0.0001  max mem: 20467
Train Epoch: [15]  [ 200/2023]  eta: 0:53:00  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7390  data: 0.0001  max mem: 20467
Train Epoch: [15]  [ 250/2023]  eta: 0:51:31  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7415  data: 0.0001  max mem: 20467
Train Epoch: [15]  [ 300/2023]  eta: 0:50:04  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7443  data: 0.0001  max mem: 20467
Train Epoch: [15]  [ 350/2023]  eta: 0:48:36  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7433  data: 0.0001  max mem: 20467
Train Epoch: [15]  [ 400/2023]  eta: 0:47:09  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7391  data: 0.0001  max mem: 20467
Train Epoch: [15]  [ 450/2023]  eta: 0:45:41  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7428  data: 0.0001  max mem: 20467
Train Epoch: [15]  [ 500/2023]  eta: 0:44:14  lr: 0.000010  wpa_loss: 0.0001  train_loss: 0.0001  time: 1.7370  data: 0.0001  max mem: 20467
Train Epoch: [15]  [ 550/2023]  eta: 0:42:47  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7426  data: 0.0001  max mem: 20467
Train Epoch: [15]  [ 600/2023]  eta: 0:41:19  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7389  data: 0.0001  max mem: 20467
Train Epoch: [15]  [ 650/2023]  eta: 0:39:52  lr: 0.000010  wpa_loss: 0.0061  train_loss: 0.0061  time: 1.7376  data: 0.0001  max mem: 20467
Train Epoch: [15]  [ 700/2023]  eta: 0:38:24  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7356  data: 0.0001  max mem: 20467
Train Epoch: [15]  [ 750/2023]  eta: 0:36:57  lr: 0.000010  wpa_loss: 0.0001  train_loss: 0.0001  time: 1.7380  data: 0.0001  max mem: 20467
Train Epoch: [15]  [ 800/2023]  eta: 0:35:30  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7413  data: 0.0001  max mem: 20467
Train Epoch: [15]  [ 850/2023]  eta: 0:34:02  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7380  data: 0.0002  max mem: 20467
Train Epoch: [15]  [ 900/2023]  eta: 0:32:35  lr: 0.000010  wpa_loss: 0.0001  train_loss: 0.0001  time: 1.7400  data: 0.0002  max mem: 20467
Train Epoch: [15]  [ 950/2023]  eta: 0:31:08  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7417  data: 0.0002  max mem: 20467
Train Epoch: [15]  [1000/2023]  eta: 0:29:41  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7362  data: 0.0002  max mem: 20467
Train Epoch: [15]  [1050/2023]  eta: 0:28:13  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7358  data: 0.0002  max mem: 20467
Train Epoch: [15]  [1100/2023]  eta: 0:26:46  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7395  data: 0.0002  max mem: 20467
Train Epoch: [15]  [1150/2023]  eta: 0:25:19  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7392  data: 0.0002  max mem: 20467
Train Epoch: [15]  [1200/2023]  eta: 0:23:52  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7358  data: 0.0002  max mem: 20467
Train Epoch: [15]  [1250/2023]  eta: 0:22:25  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7409  data: 0.0002  max mem: 20467
Train Epoch: [15]  [1300/2023]  eta: 0:20:58  lr: 0.000010  wpa_loss: 0.0020  train_loss: 0.0020  time: 1.7370  data: 0.0002  max mem: 20467
Train Epoch: [15]  [1350/2023]  eta: 0:19:31  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7388  data: 0.0002  max mem: 20467
Train Epoch: [15]  [1400/2023]  eta: 0:18:04  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7386  data: 0.0002  max mem: 20467
Train Epoch: [15]  [1450/2023]  eta: 0:16:37  lr: 0.000010  wpa_loss: 0.0002  train_loss: 0.0002  time: 1.7394  data: 0.0002  max mem: 20467
Train Epoch: [15]  [1500/2023]  eta: 0:15:10  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7417  data: 0.0002  max mem: 20467
Train Epoch: [15]  [1550/2023]  eta: 0:13:43  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7390  data: 0.0002  max mem: 20467
Train Epoch: [15]  [1600/2023]  eta: 0:12:16  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7427  data: 0.0002  max mem: 20467
Train Epoch: [15]  [1650/2023]  eta: 0:10:49  lr: 0.000010  wpa_loss: 0.0001  train_loss: 0.0001  time: 1.7379  data: 0.0002  max mem: 20467
Train Epoch: [15]  [1700/2023]  eta: 0:09:21  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7386  data: 0.0002  max mem: 20467
Train Epoch: [15]  [1750/2023]  eta: 0:07:55  lr: 0.000010  wpa_loss: 0.0001  train_loss: 0.0001  time: 1.7423  data: 0.0002  max mem: 20467
Train Epoch: [15]  [1800/2023]  eta: 0:06:28  lr: 0.000010  wpa_loss: 0.0008  train_loss: 0.0008  time: 1.7423  data: 0.0002  max mem: 20467
Train Epoch: [15]  [1850/2023]  eta: 0:05:01  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7386  data: 0.0002  max mem: 20467
Train Epoch: [15]  [1900/2023]  eta: 0:03:34  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7384  data: 0.0002  max mem: 20467
Train Epoch: [15]  [1950/2023]  eta: 0:02:07  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7410  data: 0.0002  max mem: 20467
Train Epoch: [15]  [2000/2023]  eta: 0:00:40  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7353  data: 0.0002  max mem: 20467
Train Epoch: [15]  [2022/2023]  eta: 0:00:01  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7411  data: 0.0007  max mem: 20467
Train Epoch: [15] Total time: 0:58:40 (1.7405 s / it)
Val Epoch: [15]  [  0/225]  eta: 0:04:44  wpa_loss: 0.0000  val_loss: 0.0000  time: 1.2658  data: 1.0294  max mem: 20467
Val Epoch: [15]  [ 50/225]  eta: 0:00:44  wpa_loss: 0.0000  val_loss: 0.0000  time: 0.2324  data: 0.0002  max mem: 20467
Val Epoch: [15]  [100/225]  eta: 0:00:30  wpa_loss: 0.0000  val_loss: 0.0000  time: 0.2334  data: 0.0002  max mem: 20467
Val Epoch: [15]  [150/225]  eta: 0:00:17  wpa_loss: 0.0000  val_loss: 0.0000  time: 0.2337  data: 0.0002  max mem: 20467
Val Epoch: [15]  [200/225]  eta: 0:00:05  wpa_loss: 0.0001  val_loss: 0.0001  time: 0.2338  data: 0.0002  max mem: 20467
Val Epoch: [15]  [224/225]  eta: 0:00:00  wpa_loss: 0.0000  val_loss: 0.0000  time: 0.2322  data: 0.0002  max mem: 20467
Val Epoch: [15] Total time: 0:00:54 (0.2419 s / it)
epoch:15, iter:32367, 2022,  train_loss: 1.2846351637563203e-05, valid_loss: 0.0013472236756751953, idiv_loss:0.0013472236756751953
Averaged stats: lr: 0.0000  wpa_loss: 0.0003  train_loss: 0.0003
epoch 15 1.2846351637563203e-05
Train Epoch: [16]  [   0/2023]  eta: 1:35:01  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 2.8183  data: 1.0112  max mem: 20467
Train Epoch: [16]  [  50/2023]  eta: 0:57:45  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7340  data: 0.0001  max mem: 20467
Train Epoch: [16]  [ 100/2023]  eta: 0:55:59  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7378  data: 0.0001  max mem: 20467
Train Epoch: [16]  [ 150/2023]  eta: 0:54:26  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7383  data: 0.0001  max mem: 20467
Train Epoch: [16]  [ 200/2023]  eta: 0:52:55  lr: 0.000010  wpa_loss: 0.0035  train_loss: 0.0035  time: 1.7362  data: 0.0001  max mem: 20467
Train Epoch: [16]  [ 250/2023]  eta: 0:51:28  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7425  data: 0.0001  max mem: 20467
Train Epoch: [16]  [ 300/2023]  eta: 0:50:00  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7400  data: 0.0001  max mem: 20467
Train Epoch: [16]  [ 350/2023]  eta: 0:48:33  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7416  data: 0.0001  max mem: 20467
Train Epoch: [16]  [ 400/2023]  eta: 0:47:05  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7415  data: 0.0001  max mem: 20467
Train Epoch: [16]  [ 450/2023]  eta: 0:45:38  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7393  data: 0.0001  max mem: 20467
Train Epoch: [16]  [ 500/2023]  eta: 0:44:11  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7427  data: 0.0001  max mem: 20467
Train Epoch: [16]  [ 550/2023]  eta: 0:42:44  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7413  data: 0.0002  max mem: 20467
Train Epoch: [16]  [ 600/2023]  eta: 0:41:17  lr: 0.000010  wpa_loss: 0.0007  train_loss: 0.0007  time: 1.7397  data: 0.0001  max mem: 20467
Train Epoch: [16]  [ 650/2023]  eta: 0:39:50  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7403  data: 0.0001  max mem: 20467
Train Epoch: [16]  [ 700/2023]  eta: 0:38:22  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7387  data: 0.0001  max mem: 20467
Train Epoch: [16]  [ 750/2023]  eta: 0:36:55  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7373  data: 0.0001  max mem: 20467
Train Epoch: [16]  [ 800/2023]  eta: 0:35:28  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7391  data: 0.0002  max mem: 20467
Train Epoch: [16]  [ 850/2023]  eta: 0:34:01  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7403  data: 0.0001  max mem: 20467
Train Epoch: [16]  [ 900/2023]  eta: 0:32:34  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7414  data: 0.0002  max mem: 20467
Train Epoch: [16]  [ 950/2023]  eta: 0:31:07  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7391  data: 0.0001  max mem: 20467
Train Epoch: [16]  [1000/2023]  eta: 0:29:40  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7408  data: 0.0002  max mem: 20467
Train Epoch: [16]  [1050/2023]  eta: 0:28:13  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7385  data: 0.0003  max mem: 20467
Train Epoch: [16]  [1100/2023]  eta: 0:26:46  lr: 0.000010  wpa_loss: 0.0006  train_loss: 0.0006  time: 1.7382  data: 0.0003  max mem: 20467
Train Epoch: [16]  [1150/2023]  eta: 0:25:18  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7378  data: 0.0003  max mem: 20467
Train Epoch: [16]  [1200/2023]  eta: 0:23:51  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7362  data: 0.0002  max mem: 20467
Train Epoch: [16]  [1250/2023]  eta: 0:22:24  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7398  data: 0.0002  max mem: 20467
Train Epoch: [16]  [1300/2023]  eta: 0:20:57  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7387  data: 0.0002  max mem: 20467
Train Epoch: [16]  [1350/2023]  eta: 0:19:30  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7389  data: 0.0002  max mem: 20467
Train Epoch: [16]  [1400/2023]  eta: 0:18:03  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7333  data: 0.0002  max mem: 20467
Train Epoch: [16]  [1450/2023]  eta: 0:16:36  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7377  data: 0.0002  max mem: 20467
Train Epoch: [16]  [1500/2023]  eta: 0:15:09  lr: 0.000010  wpa_loss: 0.0046  train_loss: 0.0046  time: 1.7372  data: 0.0003  max mem: 20467
Train Epoch: [16]  [1550/2023]  eta: 0:13:42  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7340  data: 0.0002  max mem: 20467
Train Epoch: [16]  [1600/2023]  eta: 0:12:15  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7384  data: 0.0002  max mem: 20467
Train Epoch: [16]  [1650/2023]  eta: 0:10:48  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7370  data: 0.0003  max mem: 20467
Train Epoch: [16]  [1700/2023]  eta: 0:09:21  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7389  data: 0.0002  max mem: 20467
Train Epoch: [16]  [1750/2023]  eta: 0:07:54  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7396  data: 0.0002  max mem: 20467
Train Epoch: [16]  [1800/2023]  eta: 0:06:27  lr: 0.000010  wpa_loss: 0.0003  train_loss: 0.0003  time: 1.7409  data: 0.0002  max mem: 20467
Train Epoch: [16]  [1850/2023]  eta: 0:05:00  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7390  data: 0.0002  max mem: 20467
Train Epoch: [16]  [1900/2023]  eta: 0:03:33  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7397  data: 0.0002  max mem: 20467
Train Epoch: [16]  [1950/2023]  eta: 0:02:06  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7377  data: 0.0002  max mem: 20467
Train Epoch: [16]  [2000/2023]  eta: 0:00:40  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7392  data: 0.0003  max mem: 20467
Train Epoch: [16]  [2022/2023]  eta: 0:00:01  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7365  data: 0.0008  max mem: 20467
Train Epoch: [16] Total time: 0:58:39 (1.7399 s / it)
Val Epoch: [16]  [  0/225]  eta: 0:05:00  wpa_loss: 0.0000  val_loss: 0.0000  time: 1.3354  data: 1.0975  max mem: 20467
Val Epoch: [16]  [ 50/225]  eta: 0:00:44  wpa_loss: 0.0000  val_loss: 0.0000  time: 0.2321  data: 0.0001  max mem: 20467
Val Epoch: [16]  [100/225]  eta: 0:00:30  wpa_loss: 0.0000  val_loss: 0.0000  time: 0.2328  data: 0.0001  max mem: 20467
Val Epoch: [16]  [150/225]  eta: 0:00:17  wpa_loss: 0.0000  val_loss: 0.0000  time: 0.2333  data: 0.0001  max mem: 20467
Val Epoch: [16]  [200/225]  eta: 0:00:05  wpa_loss: 0.0000  val_loss: 0.0000  time: 0.2336  data: 0.0002  max mem: 20467
Val Epoch: [16]  [224/225]  eta: 0:00:00  wpa_loss: 0.0000  val_loss: 0.0000  time: 0.2318  data: 0.0002  max mem: 20467
Val Epoch: [16] Total time: 0:00:54 (0.2420 s / it)
epoch:16, iter:34390, 2022,  train_loss: 4.126283783989493e-06, valid_loss: 0.0002512330622489066, idiv_loss:0.0002512330622489066
Averaged stats: lr: 0.0000  wpa_loss: 0.0003  train_loss: 0.0003
epoch 16 4.126283783989493e-06
Train Epoch: [17]  [   0/2023]  eta: 1:35:31  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 2.8333  data: 0.9504  max mem: 20467
Train Epoch: [17]  [  50/2023]  eta: 0:57:58  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7426  data: 0.0001  max mem: 20467
Train Epoch: [17]  [ 100/2023]  eta: 0:56:09  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7415  data: 0.0001  max mem: 20467
Train Epoch: [17]  [ 150/2023]  eta: 0:54:35  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7407  data: 0.0001  max mem: 20467
Train Epoch: [17]  [ 200/2023]  eta: 0:53:03  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7408  data: 0.0001  max mem: 20467
Train Epoch: [17]  [ 250/2023]  eta: 0:51:34  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7425  data: 0.0001  max mem: 20467
Train Epoch: [17]  [ 300/2023]  eta: 0:50:06  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7421  data: 0.0001  max mem: 20467
Train Epoch: [17]  [ 350/2023]  eta: 0:48:37  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7369  data: 0.0001  max mem: 20467
Train Epoch: [17]  [ 400/2023]  eta: 0:47:09  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7387  data: 0.0001  max mem: 20467
Train Epoch: [17]  [ 450/2023]  eta: 0:45:41  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7395  data: 0.0001  max mem: 20467
Train Epoch: [17]  [ 500/2023]  eta: 0:44:13  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7420  data: 0.0001  max mem: 20467
Train Epoch: [17]  [ 550/2023]  eta: 0:42:46  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7421  data: 0.0001  max mem: 20467
Train Epoch: [17]  [ 600/2023]  eta: 0:41:19  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7383  data: 0.0001  max mem: 20467
Train Epoch: [17]  [ 650/2023]  eta: 0:39:51  lr: 0.000010  wpa_loss: 0.0001  train_loss: 0.0001  time: 1.7403  data: 0.0001  max mem: 20467
Train Epoch: [17]  [ 700/2023]  eta: 0:38:24  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7387  data: 0.0001  max mem: 20467
Train Epoch: [17]  [ 750/2023]  eta: 0:36:56  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7356  data: 0.0001  max mem: 20467
Train Epoch: [17]  [ 800/2023]  eta: 0:35:29  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7373  data: 0.0001  max mem: 20467
Train Epoch: [17]  [ 850/2023]  eta: 0:34:02  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7356  data: 0.0001  max mem: 20467
Train Epoch: [17]  [ 900/2023]  eta: 0:32:35  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7387  data: 0.0001  max mem: 20467
Train Epoch: [17]  [ 950/2023]  eta: 0:31:07  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7373  data: 0.0001  max mem: 20467
Train Epoch: [17]  [1000/2023]  eta: 0:29:40  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7388  data: 0.0001  max mem: 20467
Train Epoch: [17]  [1050/2023]  eta: 0:28:13  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7367  data: 0.0001  max mem: 20467
Train Epoch: [17]  [1100/2023]  eta: 0:26:46  lr: 0.000010  wpa_loss: 0.0001  train_loss: 0.0001  time: 1.7403  data: 0.0001  max mem: 20467
Train Epoch: [17]  [1150/2023]  eta: 0:25:19  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7424  data: 0.0001  max mem: 20467
Train Epoch: [17]  [1200/2023]  eta: 0:23:52  lr: 0.000010  wpa_loss: 0.0002  train_loss: 0.0002  time: 1.7395  data: 0.0001  max mem: 20467
Train Epoch: [17]  [1250/2023]  eta: 0:22:25  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7396  data: 0.0001  max mem: 20467
Train Epoch: [17]  [1300/2023]  eta: 0:20:58  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7415  data: 0.0001  max mem: 20467
Train Epoch: [17]  [1350/2023]  eta: 0:19:31  lr: 0.000010  wpa_loss: 0.0015  train_loss: 0.0015  time: 1.7403  data: 0.0001  max mem: 20467
Train Epoch: [17]  [1400/2023]  eta: 0:18:04  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7393  data: 0.0001  max mem: 20467
Train Epoch: [17]  [1450/2023]  eta: 0:16:37  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7400  data: 0.0001  max mem: 20467
Train Epoch: [17]  [1500/2023]  eta: 0:15:10  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7399  data: 0.0001  max mem: 20467
Train Epoch: [17]  [1550/2023]  eta: 0:13:43  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7413  data: 0.0001  max mem: 20467
Train Epoch: [17]  [1600/2023]  eta: 0:12:16  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7378  data: 0.0001  max mem: 20467
Train Epoch: [17]  [1650/2023]  eta: 0:10:49  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7398  data: 0.0001  max mem: 20467
Train Epoch: [17]  [1700/2023]  eta: 0:09:22  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7415  data: 0.0001  max mem: 20467
Train Epoch: [17]  [1750/2023]  eta: 0:07:55  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7414  data: 0.0001  max mem: 20467
Train Epoch: [17]  [1800/2023]  eta: 0:06:28  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7389  data: 0.0001  max mem: 20467
Train Epoch: [17]  [1850/2023]  eta: 0:05:01  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7412  data: 0.0001  max mem: 20467
Train Epoch: [17]  [1900/2023]  eta: 0:03:34  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7391  data: 0.0001  max mem: 20467
Train Epoch: [17]  [1950/2023]  eta: 0:02:07  lr: 0.000010  wpa_loss: 0.0008  train_loss: 0.0008  time: 1.7393  data: 0.0001  max mem: 20467
Train Epoch: [17]  [2000/2023]  eta: 0:00:40  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7376  data: 0.0001  max mem: 20467
Train Epoch: [17]  [2022/2023]  eta: 0:00:01  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7369  data: 0.0008  max mem: 20467
Train Epoch: [17] Total time: 0:58:41 (1.7408 s / it)
Val Epoch: [17]  [  0/225]  eta: 0:04:57  wpa_loss: 0.0000  val_loss: 0.0000  time: 1.3206  data: 1.0838  max mem: 20467
Val Epoch: [17]  [ 50/225]  eta: 0:00:44  wpa_loss: 0.0000  val_loss: 0.0000  time: 0.2323  data: 0.0001  max mem: 20467
Val Epoch: [17]  [100/225]  eta: 0:00:30  wpa_loss: 0.0000  val_loss: 0.0000  time: 0.2330  data: 0.0002  max mem: 20467
Val Epoch: [17]  [150/225]  eta: 0:00:17  wpa_loss: 0.0000  val_loss: 0.0000  time: 0.2333  data: 0.0002  max mem: 20467
Val Epoch: [17]  [200/225]  eta: 0:00:05  wpa_loss: 0.0000  val_loss: 0.0000  time: 0.2337  data: 0.0002  max mem: 20467
Val Epoch: [17]  [224/225]  eta: 0:00:00  wpa_loss: 0.0000  val_loss: 0.0000  time: 0.2319  data: 0.0002  max mem: 20467
Val Epoch: [17] Total time: 0:00:54 (0.2419 s / it)
epoch:17, iter:36413, 2022,  train_loss: 7.634788744326215e-06, valid_loss: 0.0006665573282566583, idiv_loss:0.0006665573282566583
Averaged stats: lr: 0.0000  wpa_loss: 0.0002  train_loss: 0.0002
epoch 17 7.634788744326215e-06
Train Epoch: [18]  [   0/2023]  eta: 1:34:17  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 2.7967  data: 1.0091  max mem: 20467
Train Epoch: [18]  [  50/2023]  eta: 0:57:56  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7430  data: 0.0001  max mem: 20467
Train Epoch: [18]  [ 100/2023]  eta: 0:56:09  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7430  data: 0.0001  max mem: 20467
Train Epoch: [18]  [ 150/2023]  eta: 0:54:34  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7395  data: 0.0001  max mem: 20467
Train Epoch: [18]  [ 200/2023]  eta: 0:53:02  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7373  data: 0.0001  max mem: 20467
Train Epoch: [18]  [ 250/2023]  eta: 0:51:32  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7400  data: 0.0001  max mem: 20467
Train Epoch: [18]  [ 300/2023]  eta: 0:50:03  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7373  data: 0.0002  max mem: 20467
Train Epoch: [18]  [ 350/2023]  eta: 0:48:35  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7363  data: 0.0001  max mem: 20467
Train Epoch: [18]  [ 400/2023]  eta: 0:47:07  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7373  data: 0.0001  max mem: 20467
Train Epoch: [18]  [ 450/2023]  eta: 0:45:39  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7398  data: 0.0001  max mem: 20467
Train Epoch: [18]  [ 500/2023]  eta: 0:44:12  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7415  data: 0.0001  max mem: 20467
Train Epoch: [18]  [ 550/2023]  eta: 0:42:44  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7396  data: 0.0001  max mem: 20467
Train Epoch: [18]  [ 600/2023]  eta: 0:41:17  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7390  data: 0.0001  max mem: 20467
Train Epoch: [18]  [ 650/2023]  eta: 0:39:50  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7376  data: 0.0001  max mem: 20467
Train Epoch: [18]  [ 700/2023]  eta: 0:38:22  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7422  data: 0.0001  max mem: 20467
Train Epoch: [18]  [ 750/2023]  eta: 0:36:55  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7390  data: 0.0002  max mem: 20467
Train Epoch: [18]  [ 800/2023]  eta: 0:35:28  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7350  data: 0.0002  max mem: 20467
Train Epoch: [18]  [ 850/2023]  eta: 0:34:01  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7350  data: 0.0001  max mem: 20467
Train Epoch: [18]  [ 900/2023]  eta: 0:32:33  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7366  data: 0.0001  max mem: 20467
Train Epoch: [18]  [ 950/2023]  eta: 0:31:06  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7402  data: 0.0001  max mem: 20467
Train Epoch: [18]  [1000/2023]  eta: 0:29:39  lr: 0.000010  wpa_loss: 0.0001  train_loss: 0.0001  time: 1.7381  data: 0.0001  max mem: 20467
Train Epoch: [18]  [1050/2023]  eta: 0:28:12  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7373  data: 0.0001  max mem: 20467
Train Epoch: [18]  [1100/2023]  eta: 0:26:45  lr: 0.000010  wpa_loss: 0.0001  train_loss: 0.0001  time: 1.7377  data: 0.0001  max mem: 20467
Train Epoch: [18]  [1150/2023]  eta: 0:25:18  lr: 0.000010  wpa_loss: 0.0001  train_loss: 0.0001  time: 1.7405  data: 0.0001  max mem: 20467
Train Epoch: [18]  [1200/2023]  eta: 0:23:51  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7356  data: 0.0001  max mem: 20467
Train Epoch: [18]  [1250/2023]  eta: 0:22:24  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7353  data: 0.0001  max mem: 20467
Train Epoch: [18]  [1300/2023]  eta: 0:20:57  lr: 0.000010  wpa_loss: 0.0006  train_loss: 0.0006  time: 1.7373  data: 0.0001  max mem: 20467
Train Epoch: [18]  [1350/2023]  eta: 0:19:30  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7383  data: 0.0001  max mem: 20467
Train Epoch: [18]  [1400/2023]  eta: 0:18:03  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7343  data: 0.0001  max mem: 20467
Train Epoch: [18]  [1450/2023]  eta: 0:16:36  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7372  data: 0.0001  max mem: 20467
Train Epoch: [18]  [1500/2023]  eta: 0:15:09  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7356  data: 0.0001  max mem: 20467
Train Epoch: [18]  [1550/2023]  eta: 0:13:42  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7381  data: 0.0001  max mem: 20467
Train Epoch: [18]  [1600/2023]  eta: 0:12:15  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7371  data: 0.0001  max mem: 20467
Train Epoch: [18]  [1650/2023]  eta: 0:10:48  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7388  data: 0.0001  max mem: 20467
Train Epoch: [18]  [1700/2023]  eta: 0:09:21  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7407  data: 0.0001  max mem: 20467
Train Epoch: [18]  [1750/2023]  eta: 0:07:54  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7390  data: 0.0002  max mem: 20467
Train Epoch: [18]  [1800/2023]  eta: 0:06:27  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7377  data: 0.0001  max mem: 20467
Train Epoch: [18]  [1850/2023]  eta: 0:05:00  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7364  data: 0.0001  max mem: 20467
Train Epoch: [18]  [1900/2023]  eta: 0:03:33  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7383  data: 0.0001  max mem: 20467
Train Epoch: [18]  [1950/2023]  eta: 0:02:06  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7392  data: 0.0001  max mem: 20467
Train Epoch: [18]  [2000/2023]  eta: 0:00:39  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7361  data: 0.0001  max mem: 20467
Train Epoch: [18]  [2022/2023]  eta: 0:00:01  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7371  data: 0.0006  max mem: 20467
Train Epoch: [18] Total time: 0:58:38 (1.7394 s / it)
Val Epoch: [18]  [  0/225]  eta: 0:04:46  wpa_loss: 0.0000  val_loss: 0.0000  time: 1.2730  data: 1.0360  max mem: 20467
Val Epoch: [18]  [ 50/225]  eta: 0:00:44  wpa_loss: 0.0000  val_loss: 0.0000  time: 0.2321  data: 0.0002  max mem: 20467
Val Epoch: [18]  [100/225]  eta: 0:00:30  wpa_loss: 0.0000  val_loss: 0.0000  time: 0.2328  data: 0.0002  max mem: 20467
Val Epoch: [18]  [150/225]  eta: 0:00:17  wpa_loss: 0.0000  val_loss: 0.0000  time: 0.2331  data: 0.0001  max mem: 20467
Val Epoch: [18]  [200/225]  eta: 0:00:05  wpa_loss: 0.0000  val_loss: 0.0000  time: 0.2335  data: 0.0002  max mem: 20467
Val Epoch: [18]  [224/225]  eta: 0:00:00  wpa_loss: 0.0000  val_loss: 0.0000  time: 0.2318  data: 0.0002  max mem: 20467
Val Epoch: [18] Total time: 0:00:54 (0.2436 s / it)
epoch:18, iter:38436, 2022,  train_loss: 1.6791057078080485e-06, valid_loss: 0.00022239363013669492, idiv_loss:0.00022239363013669492
Averaged stats: lr: 0.0000  wpa_loss: 0.0002  train_loss: 0.0002
epoch 18 1.6791057078080485e-06
Train Epoch: [19]  [   0/2023]  eta: 1:34:25  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 2.8006  data: 1.0476  max mem: 20467
Train Epoch: [19]  [  50/2023]  eta: 0:57:53  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7397  data: 0.0001  max mem: 20467
Train Epoch: [19]  [ 100/2023]  eta: 0:56:04  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7371  data: 0.0001  max mem: 20467
Train Epoch: [19]  [ 150/2023]  eta: 0:54:29  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7357  data: 0.0001  max mem: 20467
Train Epoch: [19]  [ 200/2023]  eta: 0:52:58  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7366  data: 0.0001  max mem: 20467
Train Epoch: [19]  [ 250/2023]  eta: 0:51:30  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7406  data: 0.0001  max mem: 20467
Train Epoch: [19]  [ 300/2023]  eta: 0:50:02  lr: 0.000010  wpa_loss: 0.0001  train_loss: 0.0001  time: 1.7401  data: 0.0001  max mem: 20467
Train Epoch: [19]  [ 350/2023]  eta: 0:48:34  lr: 0.000010  wpa_loss: 0.0001  train_loss: 0.0001  time: 1.7397  data: 0.0001  max mem: 20467
Train Epoch: [19]  [ 400/2023]  eta: 0:47:06  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7391  data: 0.0001  max mem: 20467
Train Epoch: [19]  [ 450/2023]  eta: 0:45:39  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7380  data: 0.0001  max mem: 20467
Train Epoch: [19]  [ 500/2023]  eta: 0:44:11  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7398  data: 0.0001  max mem: 20467
Train Epoch: [19]  [ 550/2023]  eta: 0:42:44  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7422  data: 0.0001  max mem: 20467
Train Epoch: [19]  [ 600/2023]  eta: 0:41:17  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7387  data: 0.0001  max mem: 20467
Train Epoch: [19]  [ 650/2023]  eta: 0:39:50  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7413  data: 0.0001  max mem: 20467
Train Epoch: [19]  [ 700/2023]  eta: 0:38:22  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7395  data: 0.0001  max mem: 20467
Train Epoch: [19]  [ 750/2023]  eta: 0:36:55  lr: 0.000010  wpa_loss: 0.0001  train_loss: 0.0001  time: 1.7370  data: 0.0001  max mem: 20467
Train Epoch: [19]  [ 800/2023]  eta: 0:35:28  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7392  data: 0.0001  max mem: 20467
Train Epoch: [19]  [ 850/2023]  eta: 0:34:01  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7391  data: 0.0001  max mem: 20467
Train Epoch: [19]  [ 900/2023]  eta: 0:32:34  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7347  data: 0.0001  max mem: 20467
Train Epoch: [19]  [ 950/2023]  eta: 0:31:06  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7388  data: 0.0001  max mem: 20467
Train Epoch: [19]  [1000/2023]  eta: 0:29:39  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7378  data: 0.0001  max mem: 20467
Train Epoch: [19]  [1050/2023]  eta: 0:28:12  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7378  data: 0.0001  max mem: 20467
Train Epoch: [19]  [1100/2023]  eta: 0:26:45  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7373  data: 0.0001  max mem: 20467
Train Epoch: [19]  [1150/2023]  eta: 0:25:18  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7409  data: 0.0001  max mem: 20467
Train Epoch: [19]  [1200/2023]  eta: 0:23:51  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7408  data: 0.0001  max mem: 20467
Train Epoch: [19]  [1250/2023]  eta: 0:22:24  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7350  data: 0.0001  max mem: 20467
Train Epoch: [19]  [1300/2023]  eta: 0:20:57  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7349  data: 0.0001  max mem: 20467
Train Epoch: [19]  [1350/2023]  eta: 0:19:30  lr: 0.000010  wpa_loss: 0.0007  train_loss: 0.0007  time: 1.7379  data: 0.0001  max mem: 20467
Train Epoch: [19]  [1400/2023]  eta: 0:18:03  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7349  data: 0.0001  max mem: 20467
Train Epoch: [19]  [1450/2023]  eta: 0:16:36  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7359  data: 0.0001  max mem: 20467
Train Epoch: [19]  [1500/2023]  eta: 0:15:09  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7373  data: 0.0001  max mem: 20467
Train Epoch: [19]  [1550/2023]  eta: 0:13:42  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7346  data: 0.0001  max mem: 20467
Train Epoch: [19]  [1600/2023]  eta: 0:12:15  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7373  data: 0.0001  max mem: 20467
Train Epoch: [19]  [1650/2023]  eta: 0:10:48  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7364  data: 0.0001  max mem: 20467
Train Epoch: [19]  [1700/2023]  eta: 0:09:21  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7375  data: 0.0001  max mem: 20467
Train Epoch: [19]  [1750/2023]  eta: 0:07:54  lr: 0.000010  wpa_loss: 0.0008  train_loss: 0.0008  time: 1.7393  data: 0.0001  max mem: 20467
Train Epoch: [19]  [1800/2023]  eta: 0:06:27  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7374  data: 0.0001  max mem: 20467
Train Epoch: [19]  [1850/2023]  eta: 0:05:00  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7396  data: 0.0001  max mem: 20467
Train Epoch: [19]  [1900/2023]  eta: 0:03:33  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7381  data: 0.0001  max mem: 20467
Train Epoch: [19]  [1950/2023]  eta: 0:02:06  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7362  data: 0.0001  max mem: 20467
Train Epoch: [19]  [2000/2023]  eta: 0:00:39  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7356  data: 0.0001  max mem: 20467
Train Epoch: [19]  [2022/2023]  eta: 0:00:01  lr: 0.000010  wpa_loss: 0.0000  train_loss: 0.0000  time: 1.7360  data: 0.0007  max mem: 20467
Train Epoch: [19] Total time: 0:58:38 (1.7392 s / it)
Val Epoch: [19]  [  0/225]  eta: 0:04:46  wpa_loss: 0.0000  val_loss: 0.0000  time: 1.2752  data: 1.0391  max mem: 20467
Val Epoch: [19]  [ 50/225]  eta: 0:00:44  wpa_loss: 0.0000  val_loss: 0.0000  time: 0.2322  data: 0.0001  max mem: 20467
Val Epoch: [19]  [100/225]  eta: 0:00:30  wpa_loss: 0.0000  val_loss: 0.0000  time: 0.2329  data: 0.0002  max mem: 20467
Val Epoch: [19]  [150/225]  eta: 0:00:17  wpa_loss: 0.0000  val_loss: 0.0000  time: 0.2333  data: 0.0002  max mem: 20467
Val Epoch: [19]  [200/225]  eta: 0:00:05  wpa_loss: 0.0000  val_loss: 0.0000  time: 0.2337  data: 0.0002  max mem: 20467
Val Epoch: [19]  [224/225]  eta: 0:00:00  wpa_loss: 0.0000  val_loss: 0.0000  time: 0.2319  data: 0.0002  max mem: 20467
Val Epoch: [19] Total time: 0:00:54 (0.2431 s / it)
epoch:19, iter:40459, 2022,  train_loss: 2.5115675725828623e-06, valid_loss: 0.0007575589830675097, idiv_loss:0.0007575589830675097
Averaged stats: lr: 0.0000  wpa_loss: 0.0002  train_loss: 0.0002
epoch 19 2.5115675725828623e-06
Training time 19:52:30
ai-platform-wlf1-ge10-1:35106:35149 [1] NCCL INFO [Service thread] Connection closed by localRank 1
ai-platform-wlf1-ge10-1:35106:35106 [1] NCCL INFO comm 0x43a39030 rank 1 nranks 4 cudaDev 1 busId 24000 - Abort COMPLETE
ai-platform-wlf1-ge10-1:35109:35147 [3] NCCL INFO [Service thread] Connection closed by localRank 3
ai-platform-wlf1-ge10-1:35107:35146 [2] NCCL INFO [Service thread] Connection closed by localRank 2
ai-platform-wlf1-ge10-1:35109:35109 [3] NCCL INFO comm 0x41c563d0 rank 3 nranks 4 cudaDev 3 busId e1000 - Abort COMPLETE
ai-platform-wlf1-ge10-1:35107:35107 [2] NCCL INFO comm 0x433f8950 rank 2 nranks 4 cudaDev 2 busId 81000 - Abort COMPLETE
ai-platform-wlf1-ge10-1:35105:35148 [0] NCCL INFO [Service thread] Connection closed by localRank 0
ai-platform-wlf1-ge10-1:35105:35105 [0] NCCL INFO comm 0x44916d70 rank 0 nranks 4 cudaDev 0 busId 1000 - Abort COMPLETE
