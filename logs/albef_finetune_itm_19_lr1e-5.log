/opt/conda/envs/albef-ab/lib/python3.8/site-packages/torch/distributed/launch.py:181: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use-env is set by default in torchrun.
If your script expects `--local-rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
| distributed init (rank 0): env://
| distributed init (rank 3): env://
| distributed init (rank 2): env://
| distributed init (rank 1): env://
Creating retrieval dataset
Creating model
load checkpoint from /nlp_group/wuxing/yuhuimu/AB-layoutlmv3/output/albef/pretrain/checkpoint_19.pth
_IncompatibleKeys(missing_keys=[], unexpected_keys=['image_queue', 'text_queue', 'queue_ptr', 'vision_proj.weight', 'vision_proj.bias', 'text_proj.weight', 'text_proj.bias', 'visual_encoder_m.cls_token', 'visual_encoder_m.pos_embed', 'visual_encoder_m.patch_embed.proj.weight', 'visual_encoder_m.patch_embed.proj.bias', 'visual_encoder_m.blocks.0.norm1.weight', 'visual_encoder_m.blocks.0.norm1.bias', 'visual_encoder_m.blocks.0.attn.qkv.weight', 'visual_encoder_m.blocks.0.attn.qkv.bias', 'visual_encoder_m.blocks.0.attn.proj.weight', 'visual_encoder_m.blocks.0.attn.proj.bias', 'visual_encoder_m.blocks.0.norm2.weight', 'visual_encoder_m.blocks.0.norm2.bias', 'visual_encoder_m.blocks.0.mlp.fc1.weight', 'visual_encoder_m.blocks.0.mlp.fc1.bias', 'visual_encoder_m.blocks.0.mlp.fc2.weight', 'visual_encoder_m.blocks.0.mlp.fc2.bias', 'visual_encoder_m.blocks.1.norm1.weight', 'visual_encoder_m.blocks.1.norm1.bias', 'visual_encoder_m.blocks.1.attn.qkv.weight', 'visual_encoder_m.blocks.1.attn.qkv.bias', 'visual_encoder_m.blocks.1.attn.proj.weight', 'visual_encoder_m.blocks.1.attn.proj.bias', 'visual_encoder_m.blocks.1.norm2.weight', 'visual_encoder_m.blocks.1.norm2.bias', 'visual_encoder_m.blocks.1.mlp.fc1.weight', 'visual_encoder_m.blocks.1.mlp.fc1.bias', 'visual_encoder_m.blocks.1.mlp.fc2.weight', 'visual_encoder_m.blocks.1.mlp.fc2.bias', 'visual_encoder_m.blocks.2.norm1.weight', 'visual_encoder_m.blocks.2.norm1.bias', 'visual_encoder_m.blocks.2.attn.qkv.weight', 'visual_encoder_m.blocks.2.attn.qkv.bias', 'visual_encoder_m.blocks.2.attn.proj.weight', 'visual_encoder_m.blocks.2.attn.proj.bias', 'visual_encoder_m.blocks.2.norm2.weight', 'visual_encoder_m.blocks.2.norm2.bias', 'visual_encoder_m.blocks.2.mlp.fc1.weight', 'visual_encoder_m.blocks.2.mlp.fc1.bias', 'visual_encoder_m.blocks.2.mlp.fc2.weight', 'visual_encoder_m.blocks.2.mlp.fc2.bias', 'visual_encoder_m.blocks.3.norm1.weight', 'visual_encoder_m.blocks.3.norm1.bias', 'visual_encoder_m.blocks.3.attn.qkv.weight', 'visual_encoder_m.blocks.3.attn.qkv.bias', 'visual_encoder_m.blocks.3.attn.proj.weight', 'visual_encoder_m.blocks.3.attn.proj.bias', 'visual_encoder_m.blocks.3.norm2.weight', 'visual_encoder_m.blocks.3.norm2.bias', 'visual_encoder_m.blocks.3.mlp.fc1.weight', 'visual_encoder_m.blocks.3.mlp.fc1.bias', 'visual_encoder_m.blocks.3.mlp.fc2.weight', 'visual_encoder_m.blocks.3.mlp.fc2.bias', 'visual_encoder_m.blocks.4.norm1.weight', 'visual_encoder_m.blocks.4.norm1.bias', 'visual_encoder_m.blocks.4.attn.qkv.weight', 'visual_encoder_m.blocks.4.attn.qkv.bias', 'visual_encoder_m.blocks.4.attn.proj.weight', 'visual_encoder_m.blocks.4.attn.proj.bias', 'visual_encoder_m.blocks.4.norm2.weight', 'visual_encoder_m.blocks.4.norm2.bias', 'visual_encoder_m.blocks.4.mlp.fc1.weight', 'visual_encoder_m.blocks.4.mlp.fc1.bias', 'visual_encoder_m.blocks.4.mlp.fc2.weight', 'visual_encoder_m.blocks.4.mlp.fc2.bias', 'visual_encoder_m.blocks.5.norm1.weight', 'visual_encoder_m.blocks.5.norm1.bias', 'visual_encoder_m.blocks.5.attn.qkv.weight', 'visual_encoder_m.blocks.5.attn.qkv.bias', 'visual_encoder_m.blocks.5.attn.proj.weight', 'visual_encoder_m.blocks.5.attn.proj.bias', 'visual_encoder_m.blocks.5.norm2.weight', 'visual_encoder_m.blocks.5.norm2.bias', 'visual_encoder_m.blocks.5.mlp.fc1.weight', 'visual_encoder_m.blocks.5.mlp.fc1.bias', 'visual_encoder_m.blocks.5.mlp.fc2.weight', 'visual_encoder_m.blocks.5.mlp.fc2.bias', 'visual_encoder_m.blocks.6.norm1.weight', 'visual_encoder_m.blocks.6.norm1.bias', 'visual_encoder_m.blocks.6.attn.qkv.weight', 'visual_encoder_m.blocks.6.attn.qkv.bias', 'visual_encoder_m.blocks.6.attn.proj.weight', 'visual_encoder_m.blocks.6.attn.proj.bias', 'visual_encoder_m.blocks.6.norm2.weight', 'visual_encoder_m.blocks.6.norm2.bias', 'visual_encoder_m.blocks.6.mlp.fc1.weight', 'visual_encoder_m.blocks.6.mlp.fc1.bias', 'visual_encoder_m.blocks.6.mlp.fc2.weight', 'visual_encoder_m.blocks.6.mlp.fc2.bias', 'visual_encoder_m.blocks.7.norm1.weight', 'visual_encoder_m.blocks.7.norm1.bias', 'visual_encoder_m.blocks.7.attn.qkv.weight', 'visual_encoder_m.blocks.7.attn.qkv.bias', 'visual_encoder_m.blocks.7.attn.proj.weight', 'visual_encoder_m.blocks.7.attn.proj.bias', 'visual_encoder_m.blocks.7.norm2.weight', 'visual_encoder_m.blocks.7.norm2.bias', 'visual_encoder_m.blocks.7.mlp.fc1.weight', 'visual_encoder_m.blocks.7.mlp.fc1.bias', 'visual_encoder_m.blocks.7.mlp.fc2.weight', 'visual_encoder_m.blocks.7.mlp.fc2.bias', 'visual_encoder_m.blocks.8.norm1.weight', 'visual_encoder_m.blocks.8.norm1.bias', 'visual_encoder_m.blocks.8.attn.qkv.weight', 'visual_encoder_m.blocks.8.attn.qkv.bias', 'visual_encoder_m.blocks.8.attn.proj.weight', 'visual_encoder_m.blocks.8.attn.proj.bias', 'visual_encoder_m.blocks.8.norm2.weight', 'visual_encoder_m.blocks.8.norm2.bias', 'visual_encoder_m.blocks.8.mlp.fc1.weight', 'visual_encoder_m.blocks.8.mlp.fc1.bias', 'visual_encoder_m.blocks.8.mlp.fc2.weight', 'visual_encoder_m.blocks.8.mlp.fc2.bias', 'visual_encoder_m.blocks.9.norm1.weight', 'visual_encoder_m.blocks.9.norm1.bias', 'visual_encoder_m.blocks.9.attn.qkv.weight', 'visual_encoder_m.blocks.9.attn.qkv.bias', 'visual_encoder_m.blocks.9.attn.proj.weight', 'visual_encoder_m.blocks.9.attn.proj.bias', 'visual_encoder_m.blocks.9.norm2.weight', 'visual_encoder_m.blocks.9.norm2.bias', 'visual_encoder_m.blocks.9.mlp.fc1.weight', 'visual_encoder_m.blocks.9.mlp.fc1.bias', 'visual_encoder_m.blocks.9.mlp.fc2.weight', 'visual_encoder_m.blocks.9.mlp.fc2.bias', 'visual_encoder_m.blocks.10.norm1.weight', 'visual_encoder_m.blocks.10.norm1.bias', 'visual_encoder_m.blocks.10.attn.qkv.weight', 'visual_encoder_m.blocks.10.attn.qkv.bias', 'visual_encoder_m.blocks.10.attn.proj.weight', 'visual_encoder_m.blocks.10.attn.proj.bias', 'visual_encoder_m.blocks.10.norm2.weight', 'visual_encoder_m.blocks.10.norm2.bias', 'visual_encoder_m.blocks.10.mlp.fc1.weight', 'visual_encoder_m.blocks.10.mlp.fc1.bias', 'visual_encoder_m.blocks.10.mlp.fc2.weight', 'visual_encoder_m.blocks.10.mlp.fc2.bias', 'visual_encoder_m.blocks.11.norm1.weight', 'visual_encoder_m.blocks.11.norm1.bias', 'visual_encoder_m.blocks.11.attn.qkv.weight', 'visual_encoder_m.blocks.11.attn.qkv.bias', 'visual_encoder_m.blocks.11.attn.proj.weight', 'visual_encoder_m.blocks.11.attn.proj.bias', 'visual_encoder_m.blocks.11.norm2.weight', 'visual_encoder_m.blocks.11.norm2.bias', 'visual_encoder_m.blocks.11.mlp.fc1.weight', 'visual_encoder_m.blocks.11.mlp.fc1.bias', 'visual_encoder_m.blocks.11.mlp.fc2.weight', 'visual_encoder_m.blocks.11.mlp.fc2.bias', 'visual_encoder_m.norm.weight', 'visual_encoder_m.norm.bias', 'vision_proj_m.weight', 'vision_proj_m.bias', 'text_encoder_m.cls.predictions.bias', 'text_encoder_m.cls.predictions.transform.dense.weight', 'text_encoder_m.cls.predictions.transform.dense.bias', 'text_encoder_m.cls.predictions.transform.LayerNorm.weight', 'text_encoder_m.cls.predictions.transform.LayerNorm.bias', 'text_encoder_m.cls.predictions.decoder.weight', 'text_encoder_m.cls.predictions.decoder.bias', 'text_proj_m.weight', 'text_proj_m.bias', 'text_encoder_m.embeddings.position_ids', 'text_encoder_m.embeddings.word_embeddings.weight', 'text_encoder_m.embeddings.position_embeddings.weight', 'text_encoder_m.embeddings.token_type_embeddings.weight', 'text_encoder_m.embeddings.LayerNorm.weight', 'text_encoder_m.embeddings.LayerNorm.bias', 'text_encoder_m.encoder.layer.0.attention.self.query.weight', 'text_encoder_m.encoder.layer.0.attention.self.query.bias', 'text_encoder_m.encoder.layer.0.attention.self.key.weight', 'text_encoder_m.encoder.layer.0.attention.self.key.bias', 'text_encoder_m.encoder.layer.0.attention.self.value.weight', 'text_encoder_m.encoder.layer.0.attention.self.value.bias', 'text_encoder_m.encoder.layer.0.attention.output.dense.weight', 'text_encoder_m.encoder.layer.0.attention.output.dense.bias', 'text_encoder_m.encoder.layer.0.attention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.0.attention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.0.intermediate.dense.weight', 'text_encoder_m.encoder.layer.0.intermediate.dense.bias', 'text_encoder_m.encoder.layer.0.output.dense.weight', 'text_encoder_m.encoder.layer.0.output.dense.bias', 'text_encoder_m.encoder.layer.0.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.0.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.1.attention.self.query.weight', 'text_encoder_m.encoder.layer.1.attention.self.query.bias', 'text_encoder_m.encoder.layer.1.attention.self.key.weight', 'text_encoder_m.encoder.layer.1.attention.self.key.bias', 'text_encoder_m.encoder.layer.1.attention.self.value.weight', 'text_encoder_m.encoder.layer.1.attention.self.value.bias', 'text_encoder_m.encoder.layer.1.attention.output.dense.weight', 'text_encoder_m.encoder.layer.1.attention.output.dense.bias', 'text_encoder_m.encoder.layer.1.attention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.1.attention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.1.intermediate.dense.weight', 'text_encoder_m.encoder.layer.1.intermediate.dense.bias', 'text_encoder_m.encoder.layer.1.output.dense.weight', 'text_encoder_m.encoder.layer.1.output.dense.bias', 'text_encoder_m.encoder.layer.1.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.1.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.2.attention.self.query.weight', 'text_encoder_m.encoder.layer.2.attention.self.query.bias', 'text_encoder_m.encoder.layer.2.attention.self.key.weight', 'text_encoder_m.encoder.layer.2.attention.self.key.bias', 'text_encoder_m.encoder.layer.2.attention.self.value.weight', 'text_encoder_m.encoder.layer.2.attention.self.value.bias', 'text_encoder_m.encoder.layer.2.attention.output.dense.weight', 'text_encoder_m.encoder.layer.2.attention.output.dense.bias', 'text_encoder_m.encoder.layer.2.attention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.2.attention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.2.intermediate.dense.weight', 'text_encoder_m.encoder.layer.2.intermediate.dense.bias', 'text_encoder_m.encoder.layer.2.output.dense.weight', 'text_encoder_m.encoder.layer.2.output.dense.bias', 'text_encoder_m.encoder.layer.2.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.2.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.3.attention.self.query.weight', 'text_encoder_m.encoder.layer.3.attention.self.query.bias', 'text_encoder_m.encoder.layer.3.attention.self.key.weight', 'text_encoder_m.encoder.layer.3.attention.self.key.bias', 'text_encoder_m.encoder.layer.3.attention.self.value.weight', 'text_encoder_m.encoder.layer.3.attention.self.value.bias', 'text_encoder_m.encoder.layer.3.attention.output.dense.weight', 'text_encoder_m.encoder.layer.3.attention.output.dense.bias', 'text_encoder_m.encoder.layer.3.attention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.3.attention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.3.intermediate.dense.weight', 'text_encoder_m.encoder.layer.3.intermediate.dense.bias', 'text_encoder_m.encoder.layer.3.output.dense.weight', 'text_encoder_m.encoder.layer.3.output.dense.bias', 'text_encoder_m.encoder.layer.3.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.3.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.4.attention.self.query.weight', 'text_encoder_m.encoder.layer.4.attention.self.query.bias', 'text_encoder_m.encoder.layer.4.attention.self.key.weight', 'text_encoder_m.encoder.layer.4.attention.self.key.bias', 'text_encoder_m.encoder.layer.4.attention.self.value.weight', 'text_encoder_m.encoder.layer.4.attention.self.value.bias', 'text_encoder_m.encoder.layer.4.attention.output.dense.weight', 'text_encoder_m.encoder.layer.4.attention.output.dense.bias', 'text_encoder_m.encoder.layer.4.attention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.4.attention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.4.intermediate.dense.weight', 'text_encoder_m.encoder.layer.4.intermediate.dense.bias', 'text_encoder_m.encoder.layer.4.output.dense.weight', 'text_encoder_m.encoder.layer.4.output.dense.bias', 'text_encoder_m.encoder.layer.4.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.4.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.5.attention.self.query.weight', 'text_encoder_m.encoder.layer.5.attention.self.query.bias', 'text_encoder_m.encoder.layer.5.attention.self.key.weight', 'text_encoder_m.encoder.layer.5.attention.self.key.bias', 'text_encoder_m.encoder.layer.5.attention.self.value.weight', 'text_encoder_m.encoder.layer.5.attention.self.value.bias', 'text_encoder_m.encoder.layer.5.attention.output.dense.weight', 'text_encoder_m.encoder.layer.5.attention.output.dense.bias', 'text_encoder_m.encoder.layer.5.attention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.5.attention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.5.intermediate.dense.weight', 'text_encoder_m.encoder.layer.5.intermediate.dense.bias', 'text_encoder_m.encoder.layer.5.output.dense.weight', 'text_encoder_m.encoder.layer.5.output.dense.bias', 'text_encoder_m.encoder.layer.5.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.5.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.6.attention.self.query.weight', 'text_encoder_m.encoder.layer.6.attention.self.query.bias', 'text_encoder_m.encoder.layer.6.attention.self.key.weight', 'text_encoder_m.encoder.layer.6.attention.self.key.bias', 'text_encoder_m.encoder.layer.6.attention.self.value.weight', 'text_encoder_m.encoder.layer.6.attention.self.value.bias', 'text_encoder_m.encoder.layer.6.attention.output.dense.weight', 'text_encoder_m.encoder.layer.6.attention.output.dense.bias', 'text_encoder_m.encoder.layer.6.attention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.6.attention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.6.crossattention.self.query.weight', 'text_encoder_m.encoder.layer.6.crossattention.self.query.bias', 'text_encoder_m.encoder.layer.6.crossattention.self.key.weight', 'text_encoder_m.encoder.layer.6.crossattention.self.key.bias', 'text_encoder_m.encoder.layer.6.crossattention.self.value.weight', 'text_encoder_m.encoder.layer.6.crossattention.self.value.bias', 'text_encoder_m.encoder.layer.6.crossattention.output.dense.weight', 'text_encoder_m.encoder.layer.6.crossattention.output.dense.bias', 'text_encoder_m.encoder.layer.6.crossattention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.6.crossattention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.6.intermediate.dense.weight', 'text_encoder_m.encoder.layer.6.intermediate.dense.bias', 'text_encoder_m.encoder.layer.6.output.dense.weight', 'text_encoder_m.encoder.layer.6.output.dense.bias', 'text_encoder_m.encoder.layer.6.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.6.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.7.attention.self.query.weight', 'text_encoder_m.encoder.layer.7.attention.self.query.bias', 'text_encoder_m.encoder.layer.7.attention.self.key.weight', 'text_encoder_m.encoder.layer.7.attention.self.key.bias', 'text_encoder_m.encoder.layer.7.attention.self.value.weight', 'text_encoder_m.encoder.layer.7.attention.self.value.bias', 'text_encoder_m.encoder.layer.7.attention.output.dense.weight', 'text_encoder_m.encoder.layer.7.attention.output.dense.bias', 'text_encoder_m.encoder.layer.7.attention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.7.attention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.7.crossattention.self.query.weight', 'text_encoder_m.encoder.layer.7.crossattention.self.query.bias', 'text_encoder_m.encoder.layer.7.crossattention.self.key.weight', 'text_encoder_m.encoder.layer.7.crossattention.self.key.bias', 'text_encoder_m.encoder.layer.7.crossattention.self.value.weight', 'text_encoder_m.encoder.layer.7.crossattention.self.value.bias', 'text_encoder_m.encoder.layer.7.crossattention.output.dense.weight', 'text_encoder_m.encoder.layer.7.crossattention.output.dense.bias', 'text_encoder_m.encoder.layer.7.crossattention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.7.crossattention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.7.intermediate.dense.weight', 'text_encoder_m.encoder.layer.7.intermediate.dense.bias', 'text_encoder_m.encoder.layer.7.output.dense.weight', 'text_encoder_m.encoder.layer.7.output.dense.bias', 'text_encoder_m.encoder.layer.7.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.7.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.8.attention.self.query.weight', 'text_encoder_m.encoder.layer.8.attention.self.query.bias', 'text_encoder_m.encoder.layer.8.attention.self.key.weight', 'text_encoder_m.encoder.layer.8.attention.self.key.bias', 'text_encoder_m.encoder.layer.8.attention.self.value.weight', 'text_encoder_m.encoder.layer.8.attention.self.value.bias', 'text_encoder_m.encoder.layer.8.attention.output.dense.weight', 'text_encoder_m.encoder.layer.8.attention.output.dense.bias', 'text_encoder_m.encoder.layer.8.attention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.8.attention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.8.crossattention.self.query.weight', 'text_encoder_m.encoder.layer.8.crossattention.self.query.bias', 'text_encoder_m.encoder.layer.8.crossattention.self.key.weight', 'text_encoder_m.encoder.layer.8.crossattention.self.key.bias', 'text_encoder_m.encoder.layer.8.crossattention.self.value.weight', 'text_encoder_m.encoder.layer.8.crossattention.self.value.bias', 'text_encoder_m.encoder.layer.8.crossattention.output.dense.weight', 'text_encoder_m.encoder.layer.8.crossattention.output.dense.bias', 'text_encoder_m.encoder.layer.8.crossattention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.8.crossattention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.8.intermediate.dense.weight', 'text_encoder_m.encoder.layer.8.intermediate.dense.bias', 'text_encoder_m.encoder.layer.8.output.dense.weight', 'text_encoder_m.encoder.layer.8.output.dense.bias', 'text_encoder_m.encoder.layer.8.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.8.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.9.attention.self.query.weight', 'text_encoder_m.encoder.layer.9.attention.self.query.bias', 'text_encoder_m.encoder.layer.9.attention.self.key.weight', 'text_encoder_m.encoder.layer.9.attention.self.key.bias', 'text_encoder_m.encoder.layer.9.attention.self.value.weight', 'text_encoder_m.encoder.layer.9.attention.self.value.bias', 'text_encoder_m.encoder.layer.9.attention.output.dense.weight', 'text_encoder_m.encoder.layer.9.attention.output.dense.bias', 'text_encoder_m.encoder.layer.9.attention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.9.attention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.9.crossattention.self.query.weight', 'text_encoder_m.encoder.layer.9.crossattention.self.query.bias', 'text_encoder_m.encoder.layer.9.crossattention.self.key.weight', 'text_encoder_m.encoder.layer.9.crossattention.self.key.bias', 'text_encoder_m.encoder.layer.9.crossattention.self.value.weight', 'text_encoder_m.encoder.layer.9.crossattention.self.value.bias', 'text_encoder_m.encoder.layer.9.crossattention.output.dense.weight', 'text_encoder_m.encoder.layer.9.crossattention.output.dense.bias', 'text_encoder_m.encoder.layer.9.crossattention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.9.crossattention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.9.intermediate.dense.weight', 'text_encoder_m.encoder.layer.9.intermediate.dense.bias', 'text_encoder_m.encoder.layer.9.output.dense.weight', 'text_encoder_m.encoder.layer.9.output.dense.bias', 'text_encoder_m.encoder.layer.9.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.9.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.10.attention.self.query.weight', 'text_encoder_m.encoder.layer.10.attention.self.query.bias', 'text_encoder_m.encoder.layer.10.attention.self.key.weight', 'text_encoder_m.encoder.layer.10.attention.self.key.bias', 'text_encoder_m.encoder.layer.10.attention.self.value.weight', 'text_encoder_m.encoder.layer.10.attention.self.value.bias', 'text_encoder_m.encoder.layer.10.attention.output.dense.weight', 'text_encoder_m.encoder.layer.10.attention.output.dense.bias', 'text_encoder_m.encoder.layer.10.attention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.10.attention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.10.crossattention.self.query.weight', 'text_encoder_m.encoder.layer.10.crossattention.self.query.bias', 'text_encoder_m.encoder.layer.10.crossattention.self.key.weight', 'text_encoder_m.encoder.layer.10.crossattention.self.key.bias', 'text_encoder_m.encoder.layer.10.crossattention.self.value.weight', 'text_encoder_m.encoder.layer.10.crossattention.self.value.bias', 'text_encoder_m.encoder.layer.10.crossattention.output.dense.weight', 'text_encoder_m.encoder.layer.10.crossattention.output.dense.bias', 'text_encoder_m.encoder.layer.10.crossattention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.10.crossattention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.10.intermediate.dense.weight', 'text_encoder_m.encoder.layer.10.intermediate.dense.bias', 'text_encoder_m.encoder.layer.10.output.dense.weight', 'text_encoder_m.encoder.layer.10.output.dense.bias', 'text_encoder_m.encoder.layer.10.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.10.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.11.attention.self.query.weight', 'text_encoder_m.encoder.layer.11.attention.self.query.bias', 'text_encoder_m.encoder.layer.11.attention.self.key.weight', 'text_encoder_m.encoder.layer.11.attention.self.key.bias', 'text_encoder_m.encoder.layer.11.attention.self.value.weight', 'text_encoder_m.encoder.layer.11.attention.self.value.bias', 'text_encoder_m.encoder.layer.11.attention.output.dense.weight', 'text_encoder_m.encoder.layer.11.attention.output.dense.bias', 'text_encoder_m.encoder.layer.11.attention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.11.attention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.11.crossattention.self.query.weight', 'text_encoder_m.encoder.layer.11.crossattention.self.query.bias', 'text_encoder_m.encoder.layer.11.crossattention.self.key.weight', 'text_encoder_m.encoder.layer.11.crossattention.self.key.bias', 'text_encoder_m.encoder.layer.11.crossattention.self.value.weight', 'text_encoder_m.encoder.layer.11.crossattention.self.value.bias', 'text_encoder_m.encoder.layer.11.crossattention.output.dense.weight', 'text_encoder_m.encoder.layer.11.crossattention.output.dense.bias', 'text_encoder_m.encoder.layer.11.crossattention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.11.crossattention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.11.intermediate.dense.weight', 'text_encoder_m.encoder.layer.11.intermediate.dense.bias', 'text_encoder_m.encoder.layer.11.output.dense.weight', 'text_encoder_m.encoder.layer.11.output.dense.bias', 'text_encoder_m.encoder.layer.11.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.11.output.LayerNorm.bias', 'text_encoder.cls.predictions.bias', 'text_encoder.cls.predictions.transform.dense.weight', 'text_encoder.cls.predictions.transform.dense.bias', 'text_encoder.cls.predictions.transform.LayerNorm.weight', 'text_encoder.cls.predictions.transform.LayerNorm.bias', 'text_encoder.cls.predictions.decoder.weight', 'text_encoder.cls.predictions.decoder.bias'])
Start training
Train Epoch: [0]  [  0/219]  eta: 0:14:43  lr: 0.000001  loss_itm: 0.5348  time: 4.0336  data: 0.0836  max mem: 16290
Train Epoch: [0]  [ 50/219]  eta: 0:04:23  lr: 0.000001  loss_itm: 0.2253  time: 1.5065  data: 0.0756  max mem: 18674
Train Epoch: [0]  [100/219]  eta: 0:03:02  lr: 0.000001  loss_itm: 0.1538  time: 1.5050  data: 0.0748  max mem: 18674
Train Epoch: [0]  [150/219]  eta: 0:01:45  lr: 0.000001  loss_itm: 0.1642  time: 1.5085  data: 0.0756  max mem: 18674
Train Epoch: [0]  [200/219]  eta: 0:00:28  lr: 0.000001  loss_itm: 0.1813  time: 1.5089  data: 0.0744  max mem: 18674
Train Epoch: [0]  [218/219]  eta: 0:00:01  lr: 0.000002  loss_itm: 0.1600  time: 1.5091  data: 0.0755  max mem: 18674
Train Epoch: [0] Total time: 0:05:32 (1.5199 s / it)
Averaged stats: lr: 0.0000  loss_itm: 0.2034
Val :   [ 0/50]  eta: 0:00:30    time: 0.6028  data: 0.1630  max mem: 18674
Val :   [49/50]  eta: 0:00:00    time: 0.2443  data: 0.1476  max mem: 18674
Val :  Total time: 0:00:12 (0.2583 s / it)
F1-score: 0.5379185676574707
Accuracy: 0.5874048471450806
Specificity: 0.5874048471450806
recall: 0.5874048471450806
Precision: 0.6529064178466797
Evaluation time 0:00:12
Train Epoch: [1]  [  0/219]  eta: 0:06:01  lr: 0.000010  loss_itm: 0.2611  time: 1.6507  data: 0.0806  max mem: 18674
Train Epoch: [1]  [ 50/219]  eta: 0:04:15  lr: 0.000010  loss_itm: 0.4173  time: 1.5089  data: 0.0747  max mem: 18675
Train Epoch: [1]  [100/219]  eta: 0:02:59  lr: 0.000010  loss_itm: 0.0932  time: 1.5091  data: 0.0725  max mem: 18675
Train Epoch: [1]  [150/219]  eta: 0:01:44  lr: 0.000010  loss_itm: 0.1803  time: 1.5104  data: 0.0747  max mem: 18677
Train Epoch: [1]  [200/219]  eta: 0:00:28  lr: 0.000010  loss_itm: 0.3150  time: 1.5096  data: 0.0749  max mem: 18677
Train Epoch: [1]  [218/219]  eta: 0:00:01  lr: 0.000010  loss_itm: 0.5058  time: 1.5054  data: 0.0751  max mem: 18677
Train Epoch: [1] Total time: 0:05:30 (1.5091 s / it)
Averaged stats: lr: 0.0000  loss_itm: 0.2059
Val :   [ 0/50]  eta: 0:00:12    time: 0.2545  data: 0.1559  max mem: 18677
Val :   [49/50]  eta: 0:00:00    time: 0.2443  data: 0.1481  max mem: 18677
Val :  Total time: 0:00:12 (0.2515 s / it)
F1-score: 0.48808178305625916
Accuracy: 0.5623413920402527
Specificity: 0.5623413920402527
recall: 0.5623413920402527
Precision: 0.6485186815261841
Evaluation time 0:00:12
Train Epoch: [2]  [  0/219]  eta: 0:06:00  lr: 0.000010  loss_itm: 0.1129  time: 1.6477  data: 0.0785  max mem: 18677
Train Epoch: [2]  [ 50/219]  eta: 0:04:15  lr: 0.000010  loss_itm: 0.0520  time: 1.5073  data: 0.0749  max mem: 18678
Train Epoch: [2]  [100/219]  eta: 0:02:59  lr: 0.000010  loss_itm: 0.1704  time: 1.5097  data: 0.0739  max mem: 18678
Train Epoch: [2]  [150/219]  eta: 0:01:44  lr: 0.000010  loss_itm: 0.1577  time: 1.5105  data: 0.0757  max mem: 18678
Train Epoch: [2]  [200/219]  eta: 0:00:28  lr: 0.000010  loss_itm: 0.2797  time: 1.5088  data: 0.0739  max mem: 18678
Train Epoch: [2]  [218/219]  eta: 0:00:01  lr: 0.000010  loss_itm: 0.4111  time: 1.5087  data: 0.0756  max mem: 18678
Train Epoch: [2] Total time: 0:05:30 (1.5092 s / it)
Averaged stats: lr: 0.0000  loss_itm: 0.1877
Val :   [ 0/50]  eta: 0:00:12    time: 0.2573  data: 0.1559  max mem: 18678
Val :   [49/50]  eta: 0:00:00    time: 0.2434  data: 0.1475  max mem: 18678
Val :  Total time: 0:00:12 (0.2510 s / it)
F1-score: 0.5078584551811218
Accuracy: 0.5750316977500916
Specificity: 0.5750317573547363
recall: 0.5750317573547363
Precision: 0.665256142616272
Evaluation time 0:00:12
Train Epoch: [3]  [  0/219]  eta: 0:06:00  lr: 0.000010  loss_itm: 0.0695  time: 1.6483  data: 0.0739  max mem: 18678
Train Epoch: [3]  [ 50/219]  eta: 0:04:15  lr: 0.000010  loss_itm: 0.0916  time: 1.5069  data: 0.0753  max mem: 18678
Train Epoch: [3]  [100/219]  eta: 0:02:59  lr: 0.000010  loss_itm: 0.1681  time: 1.5050  data: 0.0748  max mem: 18679
Train Epoch: [3]  [150/219]  eta: 0:01:44  lr: 0.000010  loss_itm: 0.2451  time: 1.5067  data: 0.0745  max mem: 18679
Train Epoch: [3]  [200/219]  eta: 0:00:28  lr: 0.000010  loss_itm: 0.2198  time: 1.5070  data: 0.0737  max mem: 18679
Train Epoch: [3]  [218/219]  eta: 0:00:01  lr: 0.000010  loss_itm: 0.3005  time: 1.5075  data: 0.0746  max mem: 18679
Train Epoch: [3] Total time: 0:05:30 (1.5072 s / it)
Averaged stats: lr: 0.0000  loss_itm: 0.1791
Val :   [ 0/50]  eta: 0:00:12    time: 0.2552  data: 0.1565  max mem: 18679
Val :   [49/50]  eta: 0:00:00    time: 0.2436  data: 0.1469  max mem: 18679
Val :  Total time: 0:00:12 (0.2509 s / it)
F1-score: 0.5093607306480408
Accuracy: 0.5728108882904053
Specificity: 0.5728108882904053
recall: 0.5728108882904053
Precision: 0.6508365273475647
Evaluation time 0:00:12
Train Epoch: [4]  [  0/219]  eta: 0:05:58  lr: 0.000010  loss_itm: 0.1502  time: 1.6392  data: 0.0765  max mem: 18679
Train Epoch: [4]  [ 50/219]  eta: 0:04:15  lr: 0.000010  loss_itm: 0.1530  time: 1.5148  data: 0.0741  max mem: 18679
Train Epoch: [4]  [100/219]  eta: 0:02:59  lr: 0.000010  loss_itm: 0.0256  time: 1.5051  data: 0.0740  max mem: 18679
Train Epoch: [4]  [150/219]  eta: 0:01:44  lr: 0.000010  loss_itm: 0.1336  time: 1.5115  data: 0.0746  max mem: 18679
Train Epoch: [4]  [200/219]  eta: 0:00:28  lr: 0.000010  loss_itm: 0.0868  time: 1.5200  data: 0.0740  max mem: 18679
Train Epoch: [4]  [218/219]  eta: 0:00:01  lr: 0.000010  loss_itm: 0.2350  time: 1.5133  data: 0.0745  max mem: 18679
Train Epoch: [4] Total time: 0:05:30 (1.5108 s / it)
Averaged stats: lr: 0.0000  loss_itm: 0.1672
Val :   [ 0/50]  eta: 0:00:12    time: 0.2544  data: 0.1556  max mem: 18679
Val :   [49/50]  eta: 0:00:00    time: 0.2443  data: 0.1483  max mem: 18679
Val :  Total time: 0:00:12 (0.2512 s / it)
F1-score: 0.48023301362991333
Accuracy: 0.5613895654678345
Specificity: 0.5613895654678345
recall: 0.5613895654678345
Precision: 0.663514256477356
Evaluation time 0:00:12
Train Epoch: [5]  [  0/219]  eta: 0:05:59  lr: 0.000010  loss_itm: 0.0378  time: 1.6417  data: 0.0754  max mem: 18679
Train Epoch: [5]  [ 50/219]  eta: 0:04:14  lr: 0.000010  loss_itm: 0.0328  time: 1.5074  data: 0.0748  max mem: 18679
Train Epoch: [5]  [100/219]  eta: 0:02:59  lr: 0.000010  loss_itm: 0.1887  time: 1.5076  data: 0.0740  max mem: 18679
Train Epoch: [5]  [150/219]  eta: 0:01:44  lr: 0.000010  loss_itm: 0.1487  time: 1.5087  data: 0.0750  max mem: 18679
Train Epoch: [5]  [200/219]  eta: 0:00:28  lr: 0.000010  loss_itm: 0.2265  time: 1.5071  data: 0.0748  max mem: 18679
Train Epoch: [5]  [218/219]  eta: 0:00:01  lr: 0.000010  loss_itm: 0.0707  time: 1.5070  data: 0.0743  max mem: 18679
Train Epoch: [5] Total time: 0:05:30 (1.5080 s / it)
Averaged stats: lr: 0.0000  loss_itm: 0.1771
Val :   [ 0/50]  eta: 0:00:12    time: 0.2586  data: 0.1571  max mem: 18679
Val :   [49/50]  eta: 0:00:00    time: 0.2433  data: 0.1473  max mem: 18679
Val :  Total time: 0:00:12 (0.2510 s / it)
F1-score: 0.47593244910240173
Accuracy: 0.5615482330322266
Specificity: 0.5615482330322266
recall: 0.5615482330322266
Precision: 0.6776135563850403
Evaluation time 0:00:12
Train Epoch: [6]  [  0/219]  eta: 0:05:59  lr: 0.000010  loss_itm: 0.2571  time: 1.6406  data: 0.0748  max mem: 18679
Train Epoch: [6]  [ 50/219]  eta: 0:04:15  lr: 0.000010  loss_itm: 0.3003  time: 1.5094  data: 0.0740  max mem: 18679
Train Epoch: [6]  [100/219]  eta: 0:02:59  lr: 0.000010  loss_itm: 0.1257  time: 1.5032  data: 0.0744  max mem: 18679
Train Epoch: [6]  [150/219]  eta: 0:01:44  lr: 0.000010  loss_itm: 0.1005  time: 1.5077  data: 0.0747  max mem: 18679
Train Epoch: [6]  [200/219]  eta: 0:00:28  lr: 0.000010  loss_itm: 0.0641  time: 1.5124  data: 0.0746  max mem: 18679
Train Epoch: [6]  [218/219]  eta: 0:00:01  lr: 0.000010  loss_itm: 0.2930  time: 1.5107  data: 0.0745  max mem: 18679
Train Epoch: [6] Total time: 0:05:30 (1.5093 s / it)
Averaged stats: lr: 0.0000  loss_itm: 0.1640
Val :   [ 0/50]  eta: 0:00:12    time: 0.2563  data: 0.1569  max mem: 18679
Val :   [49/50]  eta: 0:00:00    time: 0.2444  data: 0.1486  max mem: 18679
Val :  Total time: 0:00:12 (0.2513 s / it)
F1-score: 0.484967440366745
Accuracy: 0.5655139684677124
Specificity: 0.5655139684677124
recall: 0.5655139684677124
Precision: 0.6749672889709473
Evaluation time 0:00:12
Train Epoch: [7]  [  0/219]  eta: 0:06:04  lr: 0.000009  loss_itm: 0.2312  time: 1.6652  data: 0.0773  max mem: 18679
Train Epoch: [7]  [ 50/219]  eta: 0:04:14  lr: 0.000009  loss_itm: 0.0214  time: 1.5068  data: 0.0739  max mem: 18679
Train Epoch: [7]  [100/219]  eta: 0:02:59  lr: 0.000009  loss_itm: 0.2909  time: 1.5061  data: 0.0743  max mem: 18679
Train Epoch: [7]  [150/219]  eta: 0:01:44  lr: 0.000009  loss_itm: 0.1473  time: 1.5096  data: 0.0738  max mem: 18679
Train Epoch: [7]  [200/219]  eta: 0:00:28  lr: 0.000009  loss_itm: 0.1211  time: 1.5068  data: 0.0753  max mem: 18679
Train Epoch: [7]  [218/219]  eta: 0:00:01  lr: 0.000009  loss_itm: 0.0516  time: 1.5065  data: 0.0735  max mem: 18679
Train Epoch: [7] Total time: 0:05:30 (1.5074 s / it)
Averaged stats: lr: 0.0000  loss_itm: 0.1620
Val :   [ 0/50]  eta: 0:00:12    time: 0.2589  data: 0.1562  max mem: 18679
Val :   [49/50]  eta: 0:00:00    time: 0.2434  data: 0.1474  max mem: 18679
Val :  Total time: 0:00:12 (0.2511 s / it)
F1-score: 0.491683691740036
Accuracy: 0.5682106614112854
Specificity: 0.5682106614112854
recall: 0.5682106614112854
Precision: 0.6714695692062378
Evaluation time 0:00:12
Train Epoch: [8]  [  0/219]  eta: 0:06:05  lr: 0.000009  loss_itm: 0.1073  time: 1.6694  data: 0.0759  max mem: 18679
Train Epoch: [8]  [ 50/219]  eta: 0:04:15  lr: 0.000009  loss_itm: 0.1077  time: 1.5113  data: 0.0750  max mem: 18679
Train Epoch: [8]  [100/219]  eta: 0:02:59  lr: 0.000009  loss_itm: 0.1886  time: 1.5085  data: 0.0767  max mem: 18679
Train Epoch: [8]  [150/219]  eta: 0:01:44  lr: 0.000009  loss_itm: 0.0308  time: 1.5093  data: 0.0752  max mem: 18679
Train Epoch: [8]  [200/219]  eta: 0:00:28  lr: 0.000009  loss_itm: 0.2271  time: 1.5093  data: 0.0746  max mem: 18679
Train Epoch: [8]  [218/219]  eta: 0:00:01  lr: 0.000009  loss_itm: 0.2825  time: 1.5076  data: 0.0754  max mem: 18679
Train Epoch: [8] Total time: 0:05:30 (1.5088 s / it)
Averaged stats: lr: 0.0000  loss_itm: 0.1538
Val :   [ 0/50]  eta: 0:00:12    time: 0.2587  data: 0.1588  max mem: 18679
Val :   [49/50]  eta: 0:00:00    time: 0.2437  data: 0.1474  max mem: 18679
Val :  Total time: 0:00:12 (0.2516 s / it)
F1-score: 0.4803437888622284
Accuracy: 0.5615482330322266
Specificity: 0.5615482330322266
recall: 0.5615482330322266
Precision: 0.6641560792922974
Evaluation time 0:00:12
Train Epoch: [9]  [  0/219]  eta: 0:06:06  lr: 0.000009  loss_itm: 0.1782  time: 1.6724  data: 0.0734  max mem: 18679
Train Epoch: [9]  [ 50/219]  eta: 0:04:15  lr: 0.000009  loss_itm: 0.0287  time: 1.5054  data: 0.0747  max mem: 18679
Train Epoch: [9]  [100/219]  eta: 0:02:59  lr: 0.000009  loss_itm: 0.0863  time: 1.5063  data: 0.0745  max mem: 18679
Train Epoch: [9]  [150/219]  eta: 0:01:44  lr: 0.000009  loss_itm: 0.1987  time: 1.5041  data: 0.0745  max mem: 18679
Train Epoch: [9]  [200/219]  eta: 0:00:28  lr: 0.000009  loss_itm: 0.2213  time: 1.5109  data: 0.0748  max mem: 18679
Train Epoch: [9]  [218/219]  eta: 0:00:01  lr: 0.000009  loss_itm: 0.1271  time: 1.5043  data: 0.0757  max mem: 18679
Train Epoch: [9] Total time: 0:05:30 (1.5077 s / it)
Averaged stats: lr: 0.0000  loss_itm: 0.1505
Val :   [ 0/50]  eta: 0:00:12    time: 0.2584  data: 0.1569  max mem: 18679
Val :   [49/50]  eta: 0:00:00    time: 0.2440  data: 0.1475  max mem: 18679
Val :  Total time: 0:00:12 (0.2513 s / it)
F1-score: 0.4731164574623108
Accuracy: 0.5593274235725403
Specificity: 0.5593274235725403
recall: 0.5593274235725403
Precision: 0.6717133522033691
Evaluation time 0:00:12
Train Epoch: [10]  [  0/219]  eta: 0:05:57  lr: 0.000009  loss_itm: 0.2574  time: 1.6340  data: 0.0732  max mem: 18679
Train Epoch: [10]  [ 50/219]  eta: 0:04:15  lr: 0.000009  loss_itm: 0.0810  time: 1.5128  data: 0.0753  max mem: 18679
Train Epoch: [10]  [100/219]  eta: 0:02:59  lr: 0.000009  loss_itm: 0.2558  time: 1.5131  data: 0.0761  max mem: 18679
Train Epoch: [10]  [150/219]  eta: 0:01:44  lr: 0.000009  loss_itm: 0.4287  time: 1.5116  data: 0.0748  max mem: 18679
Train Epoch: [10]  [200/219]  eta: 0:00:28  lr: 0.000009  loss_itm: 0.0730  time: 1.5077  data: 0.0750  max mem: 18679
Train Epoch: [10]  [218/219]  eta: 0:00:01  lr: 0.000009  loss_itm: 0.0641  time: 1.5112  data: 0.0738  max mem: 18679
Train Epoch: [10] Total time: 0:05:31 (1.5116 s / it)
Averaged stats: lr: 0.0000  loss_itm: 0.1503
Val :   [ 0/50]  eta: 0:00:12    time: 0.2550  data: 0.1558  max mem: 18679
Val :   [49/50]  eta: 0:00:00    time: 0.2450  data: 0.1485  max mem: 18679
Val :  Total time: 0:00:12 (0.2515 s / it)
F1-score: 0.4903026223182678
Accuracy: 0.5671002268791199
Specificity: 0.5671002268791199
recall: 0.5671002268791199
Precision: 0.6688873171806335
Evaluation time 0:00:12
Train Epoch: [11]  [  0/219]  eta: 0:06:01  lr: 0.000008  loss_itm: 0.1252  time: 1.6506  data: 0.0797  max mem: 18679
