/opt/conda/envs/layoutlmv3/lib/python3.7/site-packages/torch/distributed/launch.py:188: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  FutureWarning,
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Namespace(batch_size=12, datasize=None, device='cuda', dist_url='env://', input_file='/nlp_group/wuxing/yuhuimu/AB-layoutlmv3/data/data_processed_ori/', learning_rate=1e-05, max_epochs=20, model_name='microsoft/layoutlmv3-base-chinese', model_params=None, output_model_dir='/nlp_group/wuxing/yuhuimu/AB-layoutlmv3/output/pretrain_mim_lr1e-5_bs_12/', pretrained='/nlp_group/wuxing/yuhuimu/AB-layoutlmv3/pytorch_model.bin', ratio_train=0.9, seed=42, tokenizer_vocab_dir='/nlp_group/wuxing/yuhuimu/AB-layoutlmv3/data/tokenizer/', world_size=1)Namespace(batch_size=12, datasize=None, device='cuda', dist_url='env://', input_file='/nlp_group/wuxing/yuhuimu/AB-layoutlmv3/data/data_processed_ori/', learning_rate=1e-05, max_epochs=20, model_name='microsoft/layoutlmv3-base-chinese', model_params=None, output_model_dir='/nlp_group/wuxing/yuhuimu/AB-layoutlmv3/output/pretrain_mim_lr1e-5_bs_12/', pretrained='/nlp_group/wuxing/yuhuimu/AB-layoutlmv3/pytorch_model.bin', ratio_train=0.9, seed=42, tokenizer_vocab_dir='/nlp_group/wuxing/yuhuimu/AB-layoutlmv3/data/tokenizer/', world_size=1)Namespace(batch_size=12, datasize=None, device='cuda', dist_url='env://', input_file='/nlp_group/wuxing/yuhuimu/AB-layoutlmv3/data/data_processed_ori/', learning_rate=1e-05, max_epochs=20, model_name='microsoft/layoutlmv3-base-chinese', model_params=None, output_model_dir='/nlp_group/wuxing/yuhuimu/AB-layoutlmv3/output/pretrain_mim_lr1e-5_bs_12/', pretrained='/nlp_group/wuxing/yuhuimu/AB-layoutlmv3/pytorch_model.bin', ratio_train=0.9, seed=42, tokenizer_vocab_dir='/nlp_group/wuxing/yuhuimu/AB-layoutlmv3/data/tokenizer/', world_size=1)
Namespace(batch_size=12, datasize=None, device='cuda', dist_url='env://', input_file='/nlp_group/wuxing/yuhuimu/AB-layoutlmv3/data/data_processed_ori/', learning_rate=1e-05, max_epochs=20, model_name='microsoft/layoutlmv3-base-chinese', model_params=None, output_model_dir='/nlp_group/wuxing/yuhuimu/AB-layoutlmv3/output/pretrain_mim_lr1e-5_bs_12/', pretrained='/nlp_group/wuxing/yuhuimu/AB-layoutlmv3/pytorch_model.bin', ratio_train=0.9, seed=42, tokenizer_vocab_dir='/nlp_group/wuxing/yuhuimu/AB-layoutlmv3/data/tokenizer/', world_size=1)


| distributed init (rank 1, word 4): env://
| distributed init (rank 0, word 4): env://
| distributed init (rank 2, word 4): env://
| distributed init (rank 3, word 4): env://
ai-platform-wlf1-ge10-1:33506:33506 [0] NCCL INFO Bootstrap : Using eth01:10.80.205.179<0>
ai-platform-wlf1-ge10-1:33506:33506 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
ai-platform-wlf1-ge10-1:33506:33506 [0] NCCL INFO cudaDriverVersion 11040
NCCL version 2.14.3+cuda11.7
ai-platform-wlf1-ge10-1:33508:33508 [2] NCCL INFO cudaDriverVersion 11040
ai-platform-wlf1-ge10-1:33510:33510 [3] NCCL INFO cudaDriverVersion 11040
ai-platform-wlf1-ge10-1:33506:33543 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
ai-platform-wlf1-ge10-1:33506:33543 [0] NCCL INFO Failed to open libibverbs.so[.1]
ai-platform-wlf1-ge10-1:33508:33508 [2] NCCL INFO Bootstrap : Using eth01:10.80.205.179<0>
ai-platform-wlf1-ge10-1:33510:33510 [3] NCCL INFO Bootstrap : Using eth01:10.80.205.179<0>
ai-platform-wlf1-ge10-1:33508:33508 [2] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
ai-platform-wlf1-ge10-1:33510:33510 [3] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
ai-platform-wlf1-ge10-1:33506:33543 [0] NCCL INFO NET/Socket : Using [0]eth01:10.80.205.179<0>
ai-platform-wlf1-ge10-1:33506:33543 [0] NCCL INFO Using network Socket
ai-platform-wlf1-ge10-1:33508:33544 [2] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
ai-platform-wlf1-ge10-1:33510:33545 [3] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
ai-platform-wlf1-ge10-1:33508:33544 [2] NCCL INFO Failed to open libibverbs.so[.1]
ai-platform-wlf1-ge10-1:33510:33545 [3] NCCL INFO Failed to open libibverbs.so[.1]
ai-platform-wlf1-ge10-1:33508:33544 [2] NCCL INFO NET/Socket : Using [0]eth01:10.80.205.179<0>
ai-platform-wlf1-ge10-1:33510:33545 [3] NCCL INFO NET/Socket : Using [0]eth01:10.80.205.179<0>
ai-platform-wlf1-ge10-1:33508:33544 [2] NCCL INFO Using network Socket
ai-platform-wlf1-ge10-1:33510:33545 [3] NCCL INFO Using network Socket
ai-platform-wlf1-ge10-1:33507:33507 [1] NCCL INFO cudaDriverVersion 11040
ai-platform-wlf1-ge10-1:33507:33507 [1] NCCL INFO Bootstrap : Using eth01:10.80.205.179<0>
ai-platform-wlf1-ge10-1:33507:33507 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
ai-platform-wlf1-ge10-1:33507:33546 [1] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
ai-platform-wlf1-ge10-1:33507:33546 [1] NCCL INFO Failed to open libibverbs.so[.1]
ai-platform-wlf1-ge10-1:33507:33546 [1] NCCL INFO NET/Socket : Using [0]eth01:10.80.205.179<0>
ai-platform-wlf1-ge10-1:33507:33546 [1] NCCL INFO Using network Socket
ai-platform-wlf1-ge10-1:33510:33545 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:33510:33545 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:33510:33545 [3] NCCL INFO Could not enable P2P between dev 1(=24000) and dev 0(=1000)
ai-platform-wlf1-ge10-1:33510:33545 [3] NCCL INFO Could not enable P2P between dev 2(=81000) and dev 0(=1000)
ai-platform-wlf1-ge10-1:33510:33545 [3] NCCL INFO Could not enable P2P between dev 3(=e1000) and dev 0(=1000)
ai-platform-wlf1-ge10-1:33510:33545 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:33510:33545 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:33510:33545 [3] NCCL INFO Could not enable P2P between dev 0(=1000) and dev 1(=24000)
ai-platform-wlf1-ge10-1:33510:33545 [3] NCCL INFO Could not enable P2P between dev 2(=81000) and dev 1(=24000)
ai-platform-wlf1-ge10-1:33510:33545 [3] NCCL INFO Could not enable P2P between dev 3(=e1000) and dev 1(=24000)
ai-platform-wlf1-ge10-1:33510:33545 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:33510:33545 [3] NCCL INFO Could not enable P2P between dev 0(=1000) and dev 2(=81000)
ai-platform-wlf1-ge10-1:33510:33545 [3] NCCL INFO Could not enable P2P between dev 1(=24000) and dev 2(=81000)
ai-platform-wlf1-ge10-1:33510:33545 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:33510:33545 [3] NCCL INFO Could not enable P2P between dev 3(=e1000) and dev 2(=81000)
ai-platform-wlf1-ge10-1:33510:33545 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:33510:33545 [3] NCCL INFO Could not enable P2P between dev 0(=1000) and dev 3(=e1000)
ai-platform-wlf1-ge10-1:33510:33545 [3] NCCL INFO Could not enable P2P between dev 1(=24000) and dev 3(=e1000)
ai-platform-wlf1-ge10-1:33510:33545 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:33510:33545 [3] NCCL INFO Could not enable P2P between dev 2(=81000) and dev 3(=e1000)
ai-platform-wlf1-ge10-1:33510:33545 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:33510:33545 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:33510:33545 [3] NCCL INFO Could not enable P2P between dev 1(=24000) and dev 0(=1000)
ai-platform-wlf1-ge10-1:33510:33545 [3] NCCL INFO Could not enable P2P between dev 2(=81000) and dev 0(=1000)
ai-platform-wlf1-ge10-1:33510:33545 [3] NCCL INFO Could not enable P2P between dev 3(=e1000) and dev 0(=1000)
ai-platform-wlf1-ge10-1:33510:33545 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:33510:33545 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:33510:33545 [3] NCCL INFO Could not enable P2P between dev 0(=1000) and dev 1(=24000)
ai-platform-wlf1-ge10-1:33510:33545 [3] NCCL INFO Could not enable P2P between dev 2(=81000) and dev 1(=24000)
ai-platform-wlf1-ge10-1:33510:33545 [3] NCCL INFO Could not enable P2P between dev 3(=e1000) and dev 1(=24000)
ai-platform-wlf1-ge10-1:33510:33545 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:33510:33545 [3] NCCL INFO Could not enable P2P between dev 0(=1000) and dev 2(=81000)
ai-platform-wlf1-ge10-1:33510:33545 [3] NCCL INFO Could not enable P2P between dev 1(=24000) and dev 2(=81000)
ai-platform-wlf1-ge10-1:33510:33545 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:33510:33545 [3] NCCL INFO Could not enable P2P between dev 3(=e1000) and dev 2(=81000)
ai-platform-wlf1-ge10-1:33510:33545 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:33510:33545 [3] NCCL INFO Could not enable P2P between dev 0(=1000) and dev 3(=e1000)
ai-platform-wlf1-ge10-1:33510:33545 [3] NCCL INFO Could not enable P2P between dev 1(=24000) and dev 3(=e1000)
ai-platform-wlf1-ge10-1:33510:33545 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:33510:33545 [3] NCCL INFO Could not enable P2P between dev 2(=81000) and dev 3(=e1000)
ai-platform-wlf1-ge10-1:33510:33545 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff,00000000,ffffffff,00000000
ai-platform-wlf1-ge10-1:33506:33543 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:33506:33543 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:33506:33543 [0] NCCL INFO Could not enable P2P between dev 1(=24000) and dev 0(=1000)
ai-platform-wlf1-ge10-1:33506:33543 [0] NCCL INFO Could not enable P2P between dev 2(=81000) and dev 0(=1000)
ai-platform-wlf1-ge10-1:33506:33543 [0] NCCL INFO Could not enable P2P between dev 3(=e1000) and dev 0(=1000)
ai-platform-wlf1-ge10-1:33506:33543 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:33506:33543 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:33506:33543 [0] NCCL INFO Could not enable P2P between dev 0(=1000) and dev 1(=24000)
ai-platform-wlf1-ge10-1:33506:33543 [0] NCCL INFO Could not enable P2P between dev 2(=81000) and dev 1(=24000)
ai-platform-wlf1-ge10-1:33506:33543 [0] NCCL INFO Could not enable P2P between dev 3(=e1000) and dev 1(=24000)
ai-platform-wlf1-ge10-1:33506:33543 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:33506:33543 [0] NCCL INFO Could not enable P2P between dev 0(=1000) and dev 2(=81000)
ai-platform-wlf1-ge10-1:33506:33543 [0] NCCL INFO Could not enable P2P between dev 1(=24000) and dev 2(=81000)
ai-platform-wlf1-ge10-1:33506:33543 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:33506:33543 [0] NCCL INFO Could not enable P2P between dev 3(=e1000) and dev 2(=81000)
ai-platform-wlf1-ge10-1:33506:33543 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:33506:33543 [0] NCCL INFO Could not enable P2P between dev 0(=1000) and dev 3(=e1000)
ai-platform-wlf1-ge10-1:33506:33543 [0] NCCL INFO Could not enable P2P between dev 1(=24000) and dev 3(=e1000)
ai-platform-wlf1-ge10-1:33506:33543 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:33506:33543 [0] NCCL INFO Could not enable P2P between dev 2(=81000) and dev 3(=e1000)
ai-platform-wlf1-ge10-1:33506:33543 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:33506:33543 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:33506:33543 [0] NCCL INFO Could not enable P2P between dev 1(=24000) and dev 0(=1000)
ai-platform-wlf1-ge10-1:33506:33543 [0] NCCL INFO Could not enable P2P between dev 2(=81000) and dev 0(=1000)
ai-platform-wlf1-ge10-1:33506:33543 [0] NCCL INFO Could not enable P2P between dev 3(=e1000) and dev 0(=1000)
ai-platform-wlf1-ge10-1:33506:33543 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:33506:33543 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:33506:33543 [0] NCCL INFO Could not enable P2P between dev 0(=1000) and dev 1(=24000)
ai-platform-wlf1-ge10-1:33506:33543 [0] NCCL INFO Could not enable P2P between dev 2(=81000) and dev 1(=24000)
ai-platform-wlf1-ge10-1:33506:33543 [0] NCCL INFO Could not enable P2P between dev 3(=e1000) and dev 1(=24000)
ai-platform-wlf1-ge10-1:33506:33543 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:33506:33543 [0] NCCL INFO Could not enable P2P between dev 0(=1000) and dev 2(=81000)
ai-platform-wlf1-ge10-1:33506:33543 [0] NCCL INFO Could not enable P2P between dev 1(=24000) and dev 2(=81000)
ai-platform-wlf1-ge10-1:33506:33543 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:33506:33543 [0] NCCL INFO Could not enable P2P between dev 3(=e1000) and dev 2(=81000)
ai-platform-wlf1-ge10-1:33506:33543 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:33506:33543 [0] NCCL INFO Could not enable P2P between dev 0(=1000) and dev 3(=e1000)
ai-platform-wlf1-ge10-1:33506:33543 [0] NCCL INFO Could not enable P2P between dev 1(=24000) and dev 3(=e1000)
ai-platform-wlf1-ge10-1:33508:33544 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:33506:33543 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:33506:33543 [0] NCCL INFO Could not enable P2P between dev 2(=81000) and dev 3(=e1000)
ai-platform-wlf1-ge10-1:33508:33544 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:33508:33544 [2] NCCL INFO Could not enable P2P between dev 1(=24000) and dev 0(=1000)
ai-platform-wlf1-ge10-1:33506:33543 [0] NCCL INFO Setting affinity for GPU 0 to ffffffff,00000000,ffffffff
ai-platform-wlf1-ge10-1:33508:33544 [2] NCCL INFO Could not enable P2P between dev 2(=81000) and dev 0(=1000)
ai-platform-wlf1-ge10-1:33508:33544 [2] NCCL INFO Could not enable P2P between dev 3(=e1000) and dev 0(=1000)
ai-platform-wlf1-ge10-1:33508:33544 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:33508:33544 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:33508:33544 [2] NCCL INFO Could not enable P2P between dev 0(=1000) and dev 1(=24000)
ai-platform-wlf1-ge10-1:33508:33544 [2] NCCL INFO Could not enable P2P between dev 2(=81000) and dev 1(=24000)
ai-platform-wlf1-ge10-1:33508:33544 [2] NCCL INFO Could not enable P2P between dev 3(=e1000) and dev 1(=24000)
ai-platform-wlf1-ge10-1:33508:33544 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:33508:33544 [2] NCCL INFO Could not enable P2P between dev 0(=1000) and dev 2(=81000)
ai-platform-wlf1-ge10-1:33508:33544 [2] NCCL INFO Could not enable P2P between dev 1(=24000) and dev 2(=81000)
ai-platform-wlf1-ge10-1:33508:33544 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:33508:33544 [2] NCCL INFO Could not enable P2P between dev 3(=e1000) and dev 2(=81000)
ai-platform-wlf1-ge10-1:33508:33544 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:33508:33544 [2] NCCL INFO Could not enable P2P between dev 0(=1000) and dev 3(=e1000)
ai-platform-wlf1-ge10-1:33508:33544 [2] NCCL INFO Could not enable P2P between dev 1(=24000) and dev 3(=e1000)
ai-platform-wlf1-ge10-1:33508:33544 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:33508:33544 [2] NCCL INFO Could not enable P2P between dev 2(=81000) and dev 3(=e1000)
ai-platform-wlf1-ge10-1:33508:33544 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:33508:33544 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:33508:33544 [2] NCCL INFO Could not enable P2P between dev 1(=24000) and dev 0(=1000)
ai-platform-wlf1-ge10-1:33508:33544 [2] NCCL INFO Could not enable P2P between dev 2(=81000) and dev 0(=1000)
ai-platform-wlf1-ge10-1:33508:33544 [2] NCCL INFO Could not enable P2P between dev 3(=e1000) and dev 0(=1000)
ai-platform-wlf1-ge10-1:33508:33544 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:33508:33544 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:33508:33544 [2] NCCL INFO Could not enable P2P between dev 0(=1000) and dev 1(=24000)
ai-platform-wlf1-ge10-1:33508:33544 [2] NCCL INFO Could not enable P2P between dev 2(=81000) and dev 1(=24000)
ai-platform-wlf1-ge10-1:33508:33544 [2] NCCL INFO Could not enable P2P between dev 3(=e1000) and dev 1(=24000)
ai-platform-wlf1-ge10-1:33508:33544 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:33508:33544 [2] NCCL INFO Could not enable P2P between dev 0(=1000) and dev 2(=81000)
ai-platform-wlf1-ge10-1:33508:33544 [2] NCCL INFO Could not enable P2P between dev 1(=24000) and dev 2(=81000)
ai-platform-wlf1-ge10-1:33508:33544 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:33508:33544 [2] NCCL INFO Could not enable P2P between dev 3(=e1000) and dev 2(=81000)
ai-platform-wlf1-ge10-1:33508:33544 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:33508:33544 [2] NCCL INFO Could not enable P2P between dev 0(=1000) and dev 3(=e1000)
ai-platform-wlf1-ge10-1:33508:33544 [2] NCCL INFO Could not enable P2P between dev 1(=24000) and dev 3(=e1000)
ai-platform-wlf1-ge10-1:33508:33544 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:33508:33544 [2] NCCL INFO Could not enable P2P between dev 2(=81000) and dev 3(=e1000)
ai-platform-wlf1-ge10-1:33508:33544 [2] NCCL INFO Setting affinity for GPU 2 to ffffffff,00000000,ffffffff,00000000
ai-platform-wlf1-ge10-1:33507:33546 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:33507:33546 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:33507:33546 [1] NCCL INFO Could not enable P2P between dev 1(=24000) and dev 0(=1000)
ai-platform-wlf1-ge10-1:33507:33546 [1] NCCL INFO Could not enable P2P between dev 2(=81000) and dev 0(=1000)
ai-platform-wlf1-ge10-1:33507:33546 [1] NCCL INFO Could not enable P2P between dev 3(=e1000) and dev 0(=1000)
ai-platform-wlf1-ge10-1:33507:33546 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:33507:33546 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:33507:33546 [1] NCCL INFO Could not enable P2P between dev 0(=1000) and dev 1(=24000)
ai-platform-wlf1-ge10-1:33507:33546 [1] NCCL INFO Could not enable P2P between dev 2(=81000) and dev 1(=24000)
ai-platform-wlf1-ge10-1:33507:33546 [1] NCCL INFO Could not enable P2P between dev 3(=e1000) and dev 1(=24000)
ai-platform-wlf1-ge10-1:33507:33546 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:33507:33546 [1] NCCL INFO Could not enable P2P between dev 0(=1000) and dev 2(=81000)
ai-platform-wlf1-ge10-1:33507:33546 [1] NCCL INFO Could not enable P2P between dev 1(=24000) and dev 2(=81000)
ai-platform-wlf1-ge10-1:33507:33546 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:33507:33546 [1] NCCL INFO Could not enable P2P between dev 3(=e1000) and dev 2(=81000)
ai-platform-wlf1-ge10-1:33507:33546 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:33507:33546 [1] NCCL INFO Could not enable P2P between dev 0(=1000) and dev 3(=e1000)
ai-platform-wlf1-ge10-1:33507:33546 [1] NCCL INFO Could not enable P2P between dev 1(=24000) and dev 3(=e1000)
ai-platform-wlf1-ge10-1:33507:33546 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:33507:33546 [1] NCCL INFO Could not enable P2P between dev 2(=81000) and dev 3(=e1000)
ai-platform-wlf1-ge10-1:33507:33546 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:33507:33546 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:33507:33546 [1] NCCL INFO Could not enable P2P between dev 1(=24000) and dev 0(=1000)
ai-platform-wlf1-ge10-1:33507:33546 [1] NCCL INFO Could not enable P2P between dev 2(=81000) and dev 0(=1000)
ai-platform-wlf1-ge10-1:33507:33546 [1] NCCL INFO Could not enable P2P between dev 3(=e1000) and dev 0(=1000)
ai-platform-wlf1-ge10-1:33507:33546 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:33507:33546 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:33507:33546 [1] NCCL INFO Could not enable P2P between dev 0(=1000) and dev 1(=24000)
ai-platform-wlf1-ge10-1:33507:33546 [1] NCCL INFO Could not enable P2P between dev 2(=81000) and dev 1(=24000)
ai-platform-wlf1-ge10-1:33507:33546 [1] NCCL INFO Could not enable P2P between dev 3(=e1000) and dev 1(=24000)
ai-platform-wlf1-ge10-1:33507:33546 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:33507:33546 [1] NCCL INFO Could not enable P2P between dev 0(=1000) and dev 2(=81000)
ai-platform-wlf1-ge10-1:33507:33546 [1] NCCL INFO Could not enable P2P between dev 1(=24000) and dev 2(=81000)
ai-platform-wlf1-ge10-1:33507:33546 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:33507:33546 [1] NCCL INFO Could not enable P2P between dev 3(=e1000) and dev 2(=81000)
ai-platform-wlf1-ge10-1:33507:33546 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:33507:33546 [1] NCCL INFO Could not enable P2P between dev 0(=1000) and dev 3(=e1000)
ai-platform-wlf1-ge10-1:33507:33546 [1] NCCL INFO Could not enable P2P between dev 1(=24000) and dev 3(=e1000)
ai-platform-wlf1-ge10-1:33507:33546 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:33507:33546 [1] NCCL INFO Could not enable P2P between dev 2(=81000) and dev 3(=e1000)
ai-platform-wlf1-ge10-1:33507:33546 [1] NCCL INFO Setting affinity for GPU 1 to ffffffff,00000000,ffffffff
ai-platform-wlf1-ge10-1:33510:33545 [3] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2 [2] -1/-1/-1->3->2 [3] -1/-1/-1->3->2
ai-platform-wlf1-ge10-1:33506:33543 [0] NCCL INFO Channel 00/04 :    0   1   2   3
ai-platform-wlf1-ge10-1:33508:33544 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1 [2] 3/-1/-1->2->1 [3] 3/-1/-1->2->1
ai-platform-wlf1-ge10-1:33507:33546 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0 [2] 2/-1/-1->1->0 [3] 2/-1/-1->1->0
ai-platform-wlf1-ge10-1:33506:33543 [0] NCCL INFO Channel 01/04 :    0   1   2   3
ai-platform-wlf1-ge10-1:33506:33543 [0] NCCL INFO Channel 02/04 :    0   1   2   3
ai-platform-wlf1-ge10-1:33506:33543 [0] NCCL INFO Channel 03/04 :    0   1   2   3
ai-platform-wlf1-ge10-1:33506:33543 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1 [2] 1/-1/-1->0->-1 [3] 1/-1/-1->0->-1
ai-platform-wlf1-ge10-1:33510:33545 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:33510:33545 [3] NCCL INFO Could not enable P2P between dev 3(=e1000) and dev 2(=81000)
ai-platform-wlf1-ge10-1:33508:33544 [2] NCCL INFO Could not enable P2P between dev 2(=81000) and dev 1(=24000)
ai-platform-wlf1-ge10-1:33507:33546 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:33507:33546 [1] NCCL INFO Could not enable P2P between dev 1(=24000) and dev 0(=1000)
ai-platform-wlf1-ge10-1:33506:33543 [0] NCCL INFO Could not enable P2P between dev 0(=1000) and dev 3(=e1000)
ai-platform-wlf1-ge10-1:33510:33545 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:33510:33545 [3] NCCL INFO Could not enable P2P between dev 3(=e1000) and dev 2(=81000)
ai-platform-wlf1-ge10-1:33508:33544 [2] NCCL INFO Could not enable P2P between dev 2(=81000) and dev 1(=24000)
ai-platform-wlf1-ge10-1:33507:33546 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:33507:33546 [1] NCCL INFO Could not enable P2P between dev 1(=24000) and dev 0(=1000)
ai-platform-wlf1-ge10-1:33506:33543 [0] NCCL INFO Could not enable P2P between dev 0(=1000) and dev 3(=e1000)
ai-platform-wlf1-ge10-1:33510:33545 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:33510:33545 [3] NCCL INFO Could not enable P2P between dev 3(=e1000) and dev 2(=81000)
ai-platform-wlf1-ge10-1:33508:33544 [2] NCCL INFO Could not enable P2P between dev 2(=81000) and dev 1(=24000)
ai-platform-wlf1-ge10-1:33507:33546 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:33507:33546 [1] NCCL INFO Could not enable P2P between dev 1(=24000) and dev 0(=1000)
ai-platform-wlf1-ge10-1:33506:33543 [0] NCCL INFO Could not enable P2P between dev 0(=1000) and dev 3(=e1000)
ai-platform-wlf1-ge10-1:33510:33545 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:33510:33545 [3] NCCL INFO Could not enable P2P between dev 3(=e1000) and dev 2(=81000)
ai-platform-wlf1-ge10-1:33508:33544 [2] NCCL INFO Could not enable P2P between dev 2(=81000) and dev 1(=24000)
ai-platform-wlf1-ge10-1:33507:33546 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:33507:33546 [1] NCCL INFO Could not enable P2P between dev 1(=24000) and dev 0(=1000)
ai-platform-wlf1-ge10-1:33506:33543 [0] NCCL INFO Could not enable P2P between dev 0(=1000) and dev 3(=e1000)
ai-platform-wlf1-ge10-1:33510:33545 [3] NCCL INFO Could not enable P2P between dev 3(=e1000) and dev 0(=1000)
ai-platform-wlf1-ge10-1:33508:33544 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:33508:33544 [2] NCCL INFO Could not enable P2P between dev 2(=81000) and dev 3(=e1000)
ai-platform-wlf1-ge10-1:33510:33545 [3] NCCL INFO Channel 00 : 3[e1000] -> 0[1000] via SHM/direct/direct
ai-platform-wlf1-ge10-1:33510:33545 [3] NCCL INFO Could not enable P2P between dev 3(=e1000) and dev 0(=1000)
ai-platform-wlf1-ge10-1:33508:33544 [2] NCCL INFO Channel 00 : 2[81000] -> 3[e1000] via SHM/direct/direct
ai-platform-wlf1-ge10-1:33508:33544 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:33508:33544 [2] NCCL INFO Could not enable P2P between dev 2(=81000) and dev 3(=e1000)
ai-platform-wlf1-ge10-1:33510:33545 [3] NCCL INFO Channel 01 : 3[e1000] -> 0[1000] via SHM/direct/direct
ai-platform-wlf1-ge10-1:33510:33545 [3] NCCL INFO Could not enable P2P between dev 3(=e1000) and dev 0(=1000)
ai-platform-wlf1-ge10-1:33508:33544 [2] NCCL INFO Channel 01 : 2[81000] -> 3[e1000] via SHM/direct/direct
ai-platform-wlf1-ge10-1:33508:33544 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:33508:33544 [2] NCCL INFO Could not enable P2P between dev 2(=81000) and dev 3(=e1000)
ai-platform-wlf1-ge10-1:33510:33545 [3] NCCL INFO Channel 02 : 3[e1000] -> 0[1000] via SHM/direct/direct
ai-platform-wlf1-ge10-1:33510:33545 [3] NCCL INFO Could not enable P2P between dev 3(=e1000) and dev 0(=1000)
ai-platform-wlf1-ge10-1:33508:33544 [2] NCCL INFO Channel 02 : 2[81000] -> 3[e1000] via SHM/direct/direct
ai-platform-wlf1-ge10-1:33508:33544 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:33508:33544 [2] NCCL INFO Could not enable P2P between dev 2(=81000) and dev 3(=e1000)
ai-platform-wlf1-ge10-1:33507:33546 [1] NCCL INFO Could not enable P2P between dev 1(=24000) and dev 2(=81000)
ai-platform-wlf1-ge10-1:33510:33545 [3] NCCL INFO Channel 03 : 3[e1000] -> 0[1000] via SHM/direct/direct
ai-platform-wlf1-ge10-1:33506:33543 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:33506:33543 [0] NCCL INFO Could not enable P2P between dev 0(=1000) and dev 1(=24000)
ai-platform-wlf1-ge10-1:33508:33544 [2] NCCL INFO Channel 03 : 2[81000] -> 3[e1000] via SHM/direct/direct
ai-platform-wlf1-ge10-1:33507:33546 [1] NCCL INFO Channel 00 : 1[24000] -> 2[81000] via SHM/direct/direct
ai-platform-wlf1-ge10-1:33507:33546 [1] NCCL INFO Could not enable P2P between dev 1(=24000) and dev 2(=81000)
ai-platform-wlf1-ge10-1:33506:33543 [0] NCCL INFO Channel 00 : 0[1000] -> 1[24000] via SHM/direct/direct
ai-platform-wlf1-ge10-1:33506:33543 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:33506:33543 [0] NCCL INFO Could not enable P2P between dev 0(=1000) and dev 1(=24000)
ai-platform-wlf1-ge10-1:33507:33546 [1] NCCL INFO Channel 01 : 1[24000] -> 2[81000] via SHM/direct/direct
ai-platform-wlf1-ge10-1:33507:33546 [1] NCCL INFO Could not enable P2P between dev 1(=24000) and dev 2(=81000)
ai-platform-wlf1-ge10-1:33506:33543 [0] NCCL INFO Channel 01 : 0[1000] -> 1[24000] via SHM/direct/direct
ai-platform-wlf1-ge10-1:33506:33543 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:33506:33543 [0] NCCL INFO Could not enable P2P between dev 0(=1000) and dev 1(=24000)
ai-platform-wlf1-ge10-1:33507:33546 [1] NCCL INFO Channel 02 : 1[24000] -> 2[81000] via SHM/direct/direct
ai-platform-wlf1-ge10-1:33507:33546 [1] NCCL INFO Could not enable P2P between dev 1(=24000) and dev 2(=81000)
ai-platform-wlf1-ge10-1:33506:33543 [0] NCCL INFO Channel 02 : 0[1000] -> 1[24000] via SHM/direct/direct
ai-platform-wlf1-ge10-1:33506:33543 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:33506:33543 [0] NCCL INFO Could not enable P2P between dev 0(=1000) and dev 1(=24000)
ai-platform-wlf1-ge10-1:33507:33546 [1] NCCL INFO Channel 03 : 1[24000] -> 2[81000] via SHM/direct/direct
ai-platform-wlf1-ge10-1:33506:33543 [0] NCCL INFO Channel 03 : 0[1000] -> 1[24000] via SHM/direct/direct
ai-platform-wlf1-ge10-1:33508:33544 [2] NCCL INFO Connected all rings
ai-platform-wlf1-ge10-1:33508:33544 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:33508:33544 [2] NCCL INFO Could not enable P2P between dev 2(=81000) and dev 3(=e1000)
ai-platform-wlf1-ge10-1:33506:33543 [0] NCCL INFO Connected all rings
ai-platform-wlf1-ge10-1:33506:33543 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:33506:33543 [0] NCCL INFO Could not enable P2P between dev 0(=1000) and dev 1(=24000)
ai-platform-wlf1-ge10-1:33510:33545 [3] NCCL INFO Connected all rings
ai-platform-wlf1-ge10-1:33510:33545 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:33510:33545 [3] NCCL INFO Could not enable P2P between dev 3(=e1000) and dev 2(=81000)
ai-platform-wlf1-ge10-1:33507:33546 [1] NCCL INFO Connected all rings
ai-platform-wlf1-ge10-1:33507:33546 [1] NCCL INFO Could not enable P2P between dev 1(=24000) and dev 2(=81000)
ai-platform-wlf1-ge10-1:33510:33545 [3] NCCL INFO Channel 00 : 3[e1000] -> 2[81000] via SHM/direct/direct
ai-platform-wlf1-ge10-1:33510:33545 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:33510:33545 [3] NCCL INFO Could not enable P2P between dev 3(=e1000) and dev 2(=81000)
ai-platform-wlf1-ge10-1:33510:33545 [3] NCCL INFO Channel 01 : 3[e1000] -> 2[81000] via SHM/direct/direct
ai-platform-wlf1-ge10-1:33510:33545 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:33510:33545 [3] NCCL INFO Could not enable P2P between dev 3(=e1000) and dev 2(=81000)
ai-platform-wlf1-ge10-1:33510:33545 [3] NCCL INFO Channel 02 : 3[e1000] -> 2[81000] via SHM/direct/direct
ai-platform-wlf1-ge10-1:33510:33545 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:33510:33545 [3] NCCL INFO Could not enable P2P between dev 3(=e1000) and dev 2(=81000)
ai-platform-wlf1-ge10-1:33510:33545 [3] NCCL INFO Channel 03 : 3[e1000] -> 2[81000] via SHM/direct/direct
ai-platform-wlf1-ge10-1:33508:33544 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:33508:33544 [2] NCCL INFO Could not enable P2P between dev 2(=81000) and dev 3(=e1000)
ai-platform-wlf1-ge10-1:33506:33543 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:33506:33543 [0] NCCL INFO Could not enable P2P between dev 0(=1000) and dev 1(=24000)
ai-platform-wlf1-ge10-1:33507:33546 [1] NCCL INFO Could not enable P2P between dev 1(=24000) and dev 2(=81000)
ai-platform-wlf1-ge10-1:33508:33544 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:33508:33544 [2] NCCL INFO Could not enable P2P between dev 2(=81000) and dev 3(=e1000)
ai-platform-wlf1-ge10-1:33506:33543 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:33506:33543 [0] NCCL INFO Could not enable P2P between dev 0(=1000) and dev 1(=24000)
ai-platform-wlf1-ge10-1:33507:33546 [1] NCCL INFO Could not enable P2P between dev 1(=24000) and dev 2(=81000)
ai-platform-wlf1-ge10-1:33508:33544 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:33508:33544 [2] NCCL INFO Could not enable P2P between dev 2(=81000) and dev 3(=e1000)
ai-platform-wlf1-ge10-1:33506:33543 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:33506:33543 [0] NCCL INFO Could not enable P2P between dev 0(=1000) and dev 1(=24000)
ai-platform-wlf1-ge10-1:33507:33546 [1] NCCL INFO Could not enable P2P between dev 1(=24000) and dev 2(=81000)
ai-platform-wlf1-ge10-1:33508:33544 [2] NCCL INFO Could not enable P2P between dev 2(=81000) and dev 1(=24000)
ai-platform-wlf1-ge10-1:33508:33544 [2] NCCL INFO Channel 00 : 2[81000] -> 1[24000] via SHM/direct/direct
ai-platform-wlf1-ge10-1:33508:33544 [2] NCCL INFO Could not enable P2P between dev 2(=81000) and dev 1(=24000)
ai-platform-wlf1-ge10-1:33508:33544 [2] NCCL INFO Channel 01 : 2[81000] -> 1[24000] via SHM/direct/direct
ai-platform-wlf1-ge10-1:33508:33544 [2] NCCL INFO Could not enable P2P between dev 2(=81000) and dev 1(=24000)
ai-platform-wlf1-ge10-1:33508:33544 [2] NCCL INFO Channel 02 : 2[81000] -> 1[24000] via SHM/direct/direct
ai-platform-wlf1-ge10-1:33508:33544 [2] NCCL INFO Could not enable P2P between dev 2(=81000) and dev 1(=24000)
ai-platform-wlf1-ge10-1:33508:33544 [2] NCCL INFO Channel 03 : 2[81000] -> 1[24000] via SHM/direct/direct
ai-platform-wlf1-ge10-1:33507:33546 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:33507:33546 [1] NCCL INFO Could not enable P2P between dev 1(=24000) and dev 0(=1000)
ai-platform-wlf1-ge10-1:33507:33546 [1] NCCL INFO Channel 00 : 1[24000] -> 0[1000] via SHM/direct/direct
ai-platform-wlf1-ge10-1:33507:33546 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:33507:33546 [1] NCCL INFO Could not enable P2P between dev 1(=24000) and dev 0(=1000)
ai-platform-wlf1-ge10-1:33507:33546 [1] NCCL INFO Channel 01 : 1[24000] -> 0[1000] via SHM/direct/direct
ai-platform-wlf1-ge10-1:33507:33546 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:33507:33546 [1] NCCL INFO Could not enable P2P between dev 1(=24000) and dev 0(=1000)
ai-platform-wlf1-ge10-1:33507:33546 [1] NCCL INFO Channel 02 : 1[24000] -> 0[1000] via SHM/direct/direct
ai-platform-wlf1-ge10-1:33507:33546 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.
ai-platform-wlf1-ge10-1:33507:33546 [1] NCCL INFO Could not enable P2P between dev 1(=24000) and dev 0(=1000)
ai-platform-wlf1-ge10-1:33507:33546 [1] NCCL INFO Channel 03 : 1[24000] -> 0[1000] via SHM/direct/direct
ai-platform-wlf1-ge10-1:33506:33543 [0] NCCL INFO Connected all trees
ai-platform-wlf1-ge10-1:33506:33543 [0] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
ai-platform-wlf1-ge10-1:33506:33543 [0] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer
ai-platform-wlf1-ge10-1:33510:33545 [3] NCCL INFO Connected all trees
ai-platform-wlf1-ge10-1:33510:33545 [3] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
ai-platform-wlf1-ge10-1:33510:33545 [3] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer
ai-platform-wlf1-ge10-1:33507:33546 [1] NCCL INFO Connected all trees
ai-platform-wlf1-ge10-1:33507:33546 [1] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
ai-platform-wlf1-ge10-1:33507:33546 [1] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer
ai-platform-wlf1-ge10-1:33508:33544 [2] NCCL INFO Connected all trees
ai-platform-wlf1-ge10-1:33508:33544 [2] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
ai-platform-wlf1-ge10-1:33508:33544 [2] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer
ai-platform-wlf1-ge10-1:33510:33545 [3] NCCL INFO comm 0x42a82a10 rank 3 nranks 4 cudaDev 3 busId e1000 - Init COMPLETE
ai-platform-wlf1-ge10-1:33508:33544 [2] NCCL INFO comm 0x427895e0 rank 2 nranks 4 cudaDev 2 busId 81000 - Init COMPLETE
ai-platform-wlf1-ge10-1:33506:33543 [0] NCCL INFO comm 0x42b44e70 rank 0 nranks 4 cudaDev 0 busId 1000 - Init COMPLETE
ai-platform-wlf1-ge10-1:33507:33546 [1] NCCL INFO comm 0x44e6cfd0 rank 1 nranks 4 cudaDev 1 busId 24000 - Init COMPLETE
The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. 
The tokenizer class you load from this checkpoint is 'XLMRobertaTokenizer'. 
The class this function is called from is 'LayoutLMv3Tokenizer_cn'.
The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. 
The tokenizer class you load from this checkpoint is 'XLMRobertaTokenizer'. 
The class this function is called from is 'LayoutLMv3Tokenizer_cn'.
The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. 
The tokenizer class you load from this checkpoint is 'XLMRobertaTokenizer'. 
The class this function is called from is 'LayoutLMv3Tokenizer_cn'.
The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. 
The tokenizer class you load from this checkpoint is 'XLMRobertaTokenizer'. 
The class this function is called from is 'LayoutLMv3Tokenizer_cn'.
Missing keys:  ['model.mask_token', 'HeadForMIM.bias', 'HeadForMIM.dense.weight', 'HeadForMIM.dense.bias', 'HeadForMIM.LayerNorm.weight', 'HeadForMIM.LayerNorm.bias', 'HeadForMIM.decoder.weight', 'HeadForMIM.decoder.bias']
Unexpected keys:  []
Create Dataset
Create Sampler
Create Dataloader
iter:  0
Start training
/opt/conda/envs/layoutlmv3/lib/python3.7/site-packages/transformers/modeling_utils.py:831: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.
  "The `device` argument is deprecated and will be removed in v5 of Transformers.", FutureWarning
/opt/conda/envs/layoutlmv3/lib/python3.7/site-packages/transformers/modeling_utils.py:831: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.
  "The `device` argument is deprecated and will be removed in v5 of Transformers.", FutureWarning
[W reducer.cpp:1298] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1298] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
/opt/conda/envs/layoutlmv3/lib/python3.7/site-packages/transformers/modeling_utils.py:831: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.
  "The `device` argument is deprecated and will be removed in v5 of Transformers.", FutureWarning
[W reducer.cpp:1298] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
/opt/conda/envs/layoutlmv3/lib/python3.7/site-packages/transformers/modeling_utils.py:831: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.
  "The `device` argument is deprecated and will be removed in v5 of Transformers.", FutureWarning
[W reducer.cpp:1298] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
Train Epoch: [0]  [   0/2023]  eta: 3:36:59  lr: 0.000000  mi_loss: 9.0582  train_loss: 9.0582  time: 6.4359  data: 0.9930  max mem: 17313
Train Epoch: [0]  [  50/2023]  eta: 1:01:52  lr: 0.000000  mi_loss: 9.0332  train_loss: 9.0332  time: 1.7870  data: 0.0001  max mem: 20622
Train Epoch: [0]  [ 100/2023]  eta: 0:58:50  lr: 0.000001  mi_loss: 9.0293  train_loss: 9.0293  time: 1.7879  data: 0.0001  max mem: 20622
Train Epoch: [0]  [ 150/2023]  eta: 0:56:48  lr: 0.000001  mi_loss: 8.9874  train_loss: 8.9874  time: 1.7876  data: 0.0001  max mem: 20622
Train Epoch: [0]  [ 200/2023]  eta: 0:55:02  lr: 0.000001  mi_loss: 8.9247  train_loss: 8.9247  time: 1.7860  data: 0.0001  max mem: 20622
Train Epoch: [0]  [ 250/2023]  eta: 0:53:23  lr: 0.000001  mi_loss: 8.8381  train_loss: 8.8381  time: 1.7877  data: 0.0001  max mem: 20622
Train Epoch: [0]  [ 300/2023]  eta: 0:51:48  lr: 0.000002  mi_loss: 8.7817  train_loss: 8.7817  time: 1.7885  data: 0.0001  max mem: 20622
Train Epoch: [0]  [ 350/2023]  eta: 0:50:14  lr: 0.000002  mi_loss: 8.6985  train_loss: 8.6985  time: 1.7894  data: 0.0001  max mem: 20622
Train Epoch: [0]  [ 400/2023]  eta: 0:48:41  lr: 0.000002  mi_loss: 8.5858  train_loss: 8.5858  time: 1.7873  data: 0.0001  max mem: 20622
Train Epoch: [0]  [ 450/2023]  eta: 0:47:09  lr: 0.000002  mi_loss: 8.3204  train_loss: 8.3204  time: 1.7912  data: 0.0001  max mem: 20622
Train Epoch: [0]  [ 500/2023]  eta: 0:45:37  lr: 0.000003  mi_loss: 8.3760  train_loss: 8.3760  time: 1.7886  data: 0.0001  max mem: 20622
Train Epoch: [0]  [ 550/2023]  eta: 0:44:06  lr: 0.000003  mi_loss: 8.1349  train_loss: 8.1349  time: 1.7891  data: 0.0001  max mem: 20622
Train Epoch: [0]  [ 600/2023]  eta: 0:42:35  lr: 0.000003  mi_loss: 8.1387  train_loss: 8.1387  time: 1.7846  data: 0.0001  max mem: 20622
Train Epoch: [0]  [ 650/2023]  eta: 0:41:04  lr: 0.000003  mi_loss: 8.0742  train_loss: 8.0742  time: 1.7882  data: 0.0001  max mem: 20622
Train Epoch: [0]  [ 700/2023]  eta: 0:39:33  lr: 0.000004  mi_loss: 7.7822  train_loss: 7.7822  time: 1.7868  data: 0.0001  max mem: 20622
Train Epoch: [0]  [ 750/2023]  eta: 0:38:03  lr: 0.000004  mi_loss: 7.5534  train_loss: 7.5534  time: 1.7863  data: 0.0001  max mem: 20622
Train Epoch: [0]  [ 800/2023]  eta: 0:36:33  lr: 0.000004  mi_loss: 7.6868  train_loss: 7.6868  time: 1.7867  data: 0.0001  max mem: 20622
Train Epoch: [0]  [ 850/2023]  eta: 0:35:03  lr: 0.000004  mi_loss: 7.6749  train_loss: 7.6749  time: 1.7867  data: 0.0001  max mem: 20622
Train Epoch: [0]  [ 900/2023]  eta: 0:33:33  lr: 0.000005  mi_loss: 7.1665  train_loss: 7.1665  time: 1.7872  data: 0.0001  max mem: 20622
Train Epoch: [0]  [ 950/2023]  eta: 0:32:03  lr: 0.000005  mi_loss: 7.3597  train_loss: 7.3597  time: 1.7843  data: 0.0001  max mem: 20622
Train Epoch: [0]  [1000/2023]  eta: 0:30:33  lr: 0.000005  mi_loss: 7.2224  train_loss: 7.2224  time: 1.7833  data: 0.0001  max mem: 20622
Train Epoch: [0]  [1050/2023]  eta: 0:29:03  lr: 0.000005  mi_loss: 7.1991  train_loss: 7.1991  time: 1.7867  data: 0.0001  max mem: 20622
Train Epoch: [0]  [1100/2023]  eta: 0:27:33  lr: 0.000006  mi_loss: 7.1319  train_loss: 7.1319  time: 1.7872  data: 0.0001  max mem: 20622
Train Epoch: [0]  [1150/2023]  eta: 0:26:03  lr: 0.000006  mi_loss: 6.9769  train_loss: 6.9769  time: 1.7868  data: 0.0001  max mem: 20622
Train Epoch: [0]  [1200/2023]  eta: 0:24:34  lr: 0.000006  mi_loss: 6.7358  train_loss: 6.7358  time: 1.7866  data: 0.0001  max mem: 20622
Train Epoch: [0]  [1250/2023]  eta: 0:23:04  lr: 0.000006  mi_loss: 6.7863  train_loss: 6.7863  time: 1.7835  data: 0.0001  max mem: 20622
Train Epoch: [0]  [1300/2023]  eta: 0:21:34  lr: 0.000007  mi_loss: 6.7595  train_loss: 6.7595  time: 1.7875  data: 0.0001  max mem: 20622
Train Epoch: [0]  [1350/2023]  eta: 0:20:05  lr: 0.000007  mi_loss: 6.4812  train_loss: 6.4812  time: 1.7900  data: 0.0001  max mem: 20622
Train Epoch: [0]  [1400/2023]  eta: 0:18:35  lr: 0.000007  mi_loss: 6.4665  train_loss: 6.4665  time: 1.7811  data: 0.0001  max mem: 20622
Train Epoch: [0]  [1450/2023]  eta: 0:17:05  lr: 0.000007  mi_loss: 5.8909  train_loss: 5.8909  time: 1.7864  data: 0.0001  max mem: 20622
Train Epoch: [0]  [1500/2023]  eta: 0:15:36  lr: 0.000008  mi_loss: 6.1668  train_loss: 6.1668  time: 1.7855  data: 0.0001  max mem: 20622
Train Epoch: [0]  [1550/2023]  eta: 0:14:06  lr: 0.000008  mi_loss: 6.2645  train_loss: 6.2645  time: 1.7844  data: 0.0001  max mem: 20622
Train Epoch: [0]  [1600/2023]  eta: 0:12:37  lr: 0.000008  mi_loss: 6.2370  train_loss: 6.2370  time: 1.7846  data: 0.0001  max mem: 20622
Train Epoch: [0]  [1650/2023]  eta: 0:11:07  lr: 0.000008  mi_loss: 6.1419  train_loss: 6.1419  time: 1.7854  data: 0.0001  max mem: 20622
Train Epoch: [0]  [1700/2023]  eta: 0:09:37  lr: 0.000009  mi_loss: 5.9131  train_loss: 5.9131  time: 1.7782  data: 0.0001  max mem: 20622
Train Epoch: [0]  [1750/2023]  eta: 0:08:08  lr: 0.000009  mi_loss: 5.5914  train_loss: 5.5914  time: 1.7816  data: 0.0001  max mem: 20622
Train Epoch: [0]  [1800/2023]  eta: 0:06:38  lr: 0.000009  mi_loss: 6.0209  train_loss: 6.0209  time: 1.7826  data: 0.0001  max mem: 20622
Train Epoch: [0]  [1850/2023]  eta: 0:05:09  lr: 0.000010  mi_loss: 5.9168  train_loss: 5.9168  time: 1.7788  data: 0.0002  max mem: 20622
Train Epoch: [0]  [1900/2023]  eta: 0:03:39  lr: 0.000010  mi_loss: 5.9927  train_loss: 5.9927  time: 1.7783  data: 0.0002  max mem: 20622
Train Epoch: [0]  [1950/2023]  eta: 0:02:10  lr: 0.000010  mi_loss: 5.5227  train_loss: 5.5227  time: 1.7770  data: 0.0002  max mem: 20622
Train Epoch: [0]  [2000/2023]  eta: 0:00:41  lr: 0.000010  mi_loss: 5.2201  train_loss: 5.2201  time: 1.7573  data: 0.0001  max mem: 20622
Train Epoch: [0]  [2022/2023]  eta: 0:00:01  lr: 0.000010  mi_loss: 5.8227  train_loss: 5.8227  time: 1.7868  data: 0.0008  max mem: 20622
Train Epoch: [0] Total time: 1:00:16 (1.7878 s / it)
Val Epoch: [0]  [  0/225]  eta: 0:05:36  mi_loss: 5.5256  val_loss: 5.5256  accMI: 0.0682  time: 1.4954  data: 0.9988  max mem: 20622
Val Epoch: [0]  [ 50/225]  eta: 0:00:45  mi_loss: 5.5271  val_loss: 5.5271  accMI: 0.0604  time: 0.2339  data: 0.0001  max mem: 20622
Val Epoch: [0]  [100/225]  eta: 0:00:30  mi_loss: 5.7434  val_loss: 5.7434  accMI: 0.0592  time: 0.2342  data: 0.0001  max mem: 20622
Val Epoch: [0]  [150/225]  eta: 0:00:18  mi_loss: 5.6551  val_loss: 5.6551  accMI: 0.0760  time: 0.2347  data: 0.0001  max mem: 20622
Val Epoch: [0]  [200/225]  eta: 0:00:06  mi_loss: 5.8115  val_loss: 5.8115  accMI: 0.0525  time: 0.2351  data: 0.0001  max mem: 20622
Val Epoch: [0]  [224/225]  eta: 0:00:00  mi_loss: 5.7449  val_loss: 5.7449  accMI: 0.0727  time: 0.2356  data: 0.0002  max mem: 20622
Val Epoch: [0] Total time: 0:00:55 (0.2448 s / it)
epoch:0, iter:2022, 2022,  train_loss: 5.8226542472839355, valid_loss: 5.575528352525499, idiv_loss:5.575528352525499, acc:0.08000786835948626
Averaged stats: lr: 0.0000  mi_loss: 7.2725  train_loss: 7.2725
epoch 0 5.8226542472839355
Train Epoch: [1]  [   0/2023]  eta: 1:33:12  lr: 0.000010  mi_loss: 5.6742  train_loss: 5.6742  time: 2.7644  data: 0.9446  max mem: 20622
Train Epoch: [1]  [  50/2023]  eta: 0:59:37  lr: 0.000010  mi_loss: 4.9897  train_loss: 4.9897  time: 1.7881  data: 0.0001  max mem: 20622
Train Epoch: [1]  [ 100/2023]  eta: 0:57:40  lr: 0.000010  mi_loss: 5.4698  train_loss: 5.4698  time: 1.7883  data: 0.0001  max mem: 20622
Train Epoch: [1]  [ 150/2023]  eta: 0:56:02  lr: 0.000010  mi_loss: 5.3639  train_loss: 5.3639  time: 1.7875  data: 0.0001  max mem: 20622
Train Epoch: [1]  [ 200/2023]  eta: 0:54:27  lr: 0.000010  mi_loss: 5.4283  train_loss: 5.4283  time: 1.7850  data: 0.0001  max mem: 20622
Train Epoch: [1]  [ 250/2023]  eta: 0:52:55  lr: 0.000010  mi_loss: 5.2372  train_loss: 5.2372  time: 1.7851  data: 0.0001  max mem: 20622
Train Epoch: [1]  [ 300/2023]  eta: 0:51:24  lr: 0.000010  mi_loss: 5.3936  train_loss: 5.3936  time: 1.7860  data: 0.0001  max mem: 20622
Train Epoch: [1]  [ 350/2023]  eta: 0:49:52  lr: 0.000010  mi_loss: 5.4672  train_loss: 5.4672  time: 1.7833  data: 0.0001  max mem: 20622
Train Epoch: [1]  [ 400/2023]  eta: 0:48:22  lr: 0.000010  mi_loss: 5.6221  train_loss: 5.6221  time: 1.7838  data: 0.0001  max mem: 20622
Train Epoch: [1]  [ 450/2023]  eta: 0:46:52  lr: 0.000010  mi_loss: 4.8385  train_loss: 4.8385  time: 1.7780  data: 0.0001  max mem: 20622
Train Epoch: [1]  [ 500/2023]  eta: 0:45:21  lr: 0.000010  mi_loss: 5.5794  train_loss: 5.5794  time: 1.7846  data: 0.0001  max mem: 20622
Train Epoch: [1]  [ 550/2023]  eta: 0:43:51  lr: 0.000010  mi_loss: 5.2947  train_loss: 5.2947  time: 1.7839  data: 0.0001  max mem: 20622
Train Epoch: [1]  [ 600/2023]  eta: 0:42:22  lr: 0.000010  mi_loss: 4.8676  train_loss: 4.8676  time: 1.7845  data: 0.0001  max mem: 20622
Train Epoch: [1]  [ 650/2023]  eta: 0:40:52  lr: 0.000010  mi_loss: 5.0863  train_loss: 5.0863  time: 1.7821  data: 0.0001  max mem: 20622
Train Epoch: [1]  [ 700/2023]  eta: 0:39:22  lr: 0.000010  mi_loss: 5.2029  train_loss: 5.2029  time: 1.7826  data: 0.0002  max mem: 20622
Train Epoch: [1]  [ 750/2023]  eta: 0:37:52  lr: 0.000010  mi_loss: 5.2726  train_loss: 5.2726  time: 1.7774  data: 0.0002  max mem: 20622
Train Epoch: [1]  [ 800/2023]  eta: 0:36:23  lr: 0.000010  mi_loss: 5.0590  train_loss: 5.0590  time: 1.7775  data: 0.0002  max mem: 20622
Train Epoch: [1]  [ 850/2023]  eta: 0:34:53  lr: 0.000010  mi_loss: 4.9210  train_loss: 4.9210  time: 1.7829  data: 0.0002  max mem: 20622
Train Epoch: [1]  [ 900/2023]  eta: 0:33:24  lr: 0.000010  mi_loss: 4.6251  train_loss: 4.6251  time: 1.7863  data: 0.0001  max mem: 20622
Train Epoch: [1]  [ 950/2023]  eta: 0:31:55  lr: 0.000010  mi_loss: 5.1863  train_loss: 5.1863  time: 1.7834  data: 0.0001  max mem: 20622
Train Epoch: [1]  [1000/2023]  eta: 0:30:26  lr: 0.000010  mi_loss: 5.2235  train_loss: 5.2235  time: 1.7879  data: 0.0001  max mem: 20622
Train Epoch: [1]  [1050/2023]  eta: 0:28:56  lr: 0.000010  mi_loss: 5.2598  train_loss: 5.2598  time: 1.7839  data: 0.0001  max mem: 20622
Train Epoch: [1]  [1100/2023]  eta: 0:27:27  lr: 0.000010  mi_loss: 4.9750  train_loss: 4.9750  time: 1.7784  data: 0.0001  max mem: 20622
Train Epoch: [1]  [1150/2023]  eta: 0:25:58  lr: 0.000010  mi_loss: 5.0200  train_loss: 5.0200  time: 1.7822  data: 0.0001  max mem: 20622
Train Epoch: [1]  [1200/2023]  eta: 0:24:28  lr: 0.000010  mi_loss: 5.0233  train_loss: 5.0233  time: 1.7807  data: 0.0001  max mem: 20622
Train Epoch: [1]  [1250/2023]  eta: 0:22:59  lr: 0.000010  mi_loss: 4.8945  train_loss: 4.8945  time: 1.7817  data: 0.0001  max mem: 20622
Train Epoch: [1]  [1300/2023]  eta: 0:21:30  lr: 0.000010  mi_loss: 5.5273  train_loss: 5.5273  time: 1.7774  data: 0.0001  max mem: 20622
Train Epoch: [1]  [1350/2023]  eta: 0:20:00  lr: 0.000010  mi_loss: 5.0711  train_loss: 5.0711  time: 1.7828  data: 0.0001  max mem: 20622
Train Epoch: [1]  [1400/2023]  eta: 0:18:31  lr: 0.000010  mi_loss: 4.8106  train_loss: 4.8106  time: 1.7828  data: 0.0001  max mem: 20622
Train Epoch: [1]  [1450/2023]  eta: 0:17:02  lr: 0.000010  mi_loss: 5.2679  train_loss: 5.2679  time: 1.7835  data: 0.0001  max mem: 20622
Train Epoch: [1]  [1500/2023]  eta: 0:15:33  lr: 0.000010  mi_loss: 4.8284  train_loss: 4.8284  time: 1.7793  data: 0.0001  max mem: 20622
Train Epoch: [1]  [1550/2023]  eta: 0:14:03  lr: 0.000010  mi_loss: 4.9397  train_loss: 4.9397  time: 1.7814  data: 0.0001  max mem: 20622
Train Epoch: [1]  [1600/2023]  eta: 0:12:34  lr: 0.000010  mi_loss: 4.9357  train_loss: 4.9357  time: 1.7826  data: 0.0001  max mem: 20622
Train Epoch: [1]  [1650/2023]  eta: 0:11:05  lr: 0.000010  mi_loss: 5.0658  train_loss: 5.0658  time: 1.7808  data: 0.0001  max mem: 20622
Train Epoch: [1]  [1700/2023]  eta: 0:09:36  lr: 0.000010  mi_loss: 4.9232  train_loss: 4.9232  time: 1.7800  data: 0.0001  max mem: 20622
Train Epoch: [1]  [1750/2023]  eta: 0:08:06  lr: 0.000010  mi_loss: 4.5065  train_loss: 4.5065  time: 1.7799  data: 0.0001  max mem: 20622
Train Epoch: [1]  [1800/2023]  eta: 0:06:37  lr: 0.000010  mi_loss: 4.7803  train_loss: 4.7803  time: 1.7798  data: 0.0001  max mem: 20622
Train Epoch: [1]  [1850/2023]  eta: 0:05:08  lr: 0.000010  mi_loss: 4.6744  train_loss: 4.6744  time: 1.7819  data: 0.0001  max mem: 20622
Train Epoch: [1]  [1900/2023]  eta: 0:03:39  lr: 0.000010  mi_loss: 4.7135  train_loss: 4.7135  time: 1.7843  data: 0.0001  max mem: 20622
Train Epoch: [1]  [1950/2023]  eta: 0:02:10  lr: 0.000010  mi_loss: 5.1693  train_loss: 5.1693  time: 1.7830  data: 0.0001  max mem: 20622
Train Epoch: [1]  [2000/2023]  eta: 0:00:41  lr: 0.000010  mi_loss: 4.8446  train_loss: 4.8446  time: 1.7772  data: 0.0001  max mem: 20622
Train Epoch: [1]  [2022/2023]  eta: 0:00:01  lr: 0.000010  mi_loss: 4.1889  train_loss: 4.1889  time: 1.7863  data: 0.0008  max mem: 20622
Train Epoch: [1] Total time: 1:00:09 (1.7842 s / it)
Val Epoch: [1]  [  0/225]  eta: 0:04:11  mi_loss: 4.6554  val_loss: 4.6554  accMI: 0.1107  time: 1.1175  data: 0.8812  max mem: 20622
Val Epoch: [1]  [ 50/225]  eta: 0:00:43  mi_loss: 4.7518  val_loss: 4.7518  accMI: 0.0951  time: 0.2343  data: 0.0002  max mem: 20622
Val Epoch: [1]  [100/225]  eta: 0:00:30  mi_loss: 4.9246  val_loss: 4.9246  accMI: 0.0883  time: 0.2347  data: 0.0002  max mem: 20622
Val Epoch: [1]  [150/225]  eta: 0:00:18  mi_loss: 4.7211  val_loss: 4.7211  accMI: 0.1017  time: 0.2350  data: 0.0002  max mem: 20622
Val Epoch: [1]  [200/225]  eta: 0:00:05  mi_loss: 4.9606  val_loss: 4.9606  accMI: 0.0792  time: 0.2353  data: 0.0003  max mem: 20622
Val Epoch: [1]  [224/225]  eta: 0:00:00  mi_loss: 4.9565  val_loss: 4.9565  accMI: 0.1157  time: 0.2336  data: 0.0003  max mem: 20622
Val Epoch: [1] Total time: 0:00:54 (0.2427 s / it)
epoch:1, iter:4045, 2022,  train_loss: 4.188869476318359, valid_loss: 4.762646454705132, idiv_loss:4.762646454705132, acc:0.11376447891195615
Averaged stats: lr: 0.0000  mi_loss: 5.0762  train_loss: 5.0762
epoch 1 4.188869476318359
Train Epoch: [2]  [   0/2023]  eta: 1:32:40  lr: 0.000010  mi_loss: 4.7070  train_loss: 4.7070  time: 2.7487  data: 0.9800  max mem: 20622
Train Epoch: [2]  [  50/2023]  eta: 0:59:28  lr: 0.000010  mi_loss: 5.0126  train_loss: 5.0126  time: 1.7822  data: 0.0003  max mem: 20623
Train Epoch: [2]  [ 100/2023]  eta: 0:57:34  lr: 0.000010  mi_loss: 5.1897  train_loss: 5.1897  time: 1.7846  data: 0.0001  max mem: 20623
Train Epoch: [2]  [ 150/2023]  eta: 0:55:57  lr: 0.000010  mi_loss: 4.8653  train_loss: 4.8653  time: 1.7831  data: 0.0002  max mem: 20623
Train Epoch: [2]  [ 200/2023]  eta: 0:54:23  lr: 0.000010  mi_loss: 4.8094  train_loss: 4.8094  time: 1.7829  data: 0.0002  max mem: 20623
Train Epoch: [2]  [ 250/2023]  eta: 0:52:50  lr: 0.000010  mi_loss: 4.7838  train_loss: 4.7838  time: 1.7805  data: 0.0002  max mem: 20623
Train Epoch: [2]  [ 300/2023]  eta: 0:51:18  lr: 0.000010  mi_loss: 4.6040  train_loss: 4.6040  time: 1.7831  data: 0.0002  max mem: 20623
Train Epoch: [2]  [ 350/2023]  eta: 0:49:48  lr: 0.000010  mi_loss: 4.7419  train_loss: 4.7419  time: 1.7885  data: 0.0004  max mem: 20623
Train Epoch: [2]  [ 400/2023]  eta: 0:48:19  lr: 0.000010  mi_loss: 4.8521  train_loss: 4.8521  time: 1.7869  data: 0.0001  max mem: 20623
Train Epoch: [2]  [ 450/2023]  eta: 0:46:50  lr: 0.000010  mi_loss: 4.7952  train_loss: 4.7952  time: 1.7854  data: 0.0003  max mem: 20623
Train Epoch: [2]  [ 500/2023]  eta: 0:45:20  lr: 0.000010  mi_loss: 4.8352  train_loss: 4.8352  time: 1.7859  data: 0.0003  max mem: 20623
Train Epoch: [2]  [ 550/2023]  eta: 0:43:51  lr: 0.000010  mi_loss: 4.6504  train_loss: 4.6504  time: 1.7881  data: 0.0003  max mem: 20623
Train Epoch: [2]  [ 600/2023]  eta: 0:42:21  lr: 0.000010  mi_loss: 4.8431  train_loss: 4.8431  time: 1.7854  data: 0.0001  max mem: 20623
Train Epoch: [2]  [ 650/2023]  eta: 0:40:52  lr: 0.000010  mi_loss: 4.6000  train_loss: 4.6000  time: 1.7862  data: 0.0002  max mem: 20623
Train Epoch: [2]  [ 700/2023]  eta: 0:39:23  lr: 0.000010  mi_loss: 4.7875  train_loss: 4.7875  time: 1.7827  data: 0.0004  max mem: 20623
Train Epoch: [2]  [ 750/2023]  eta: 0:37:53  lr: 0.000010  mi_loss: 4.9033  train_loss: 4.9033  time: 1.7875  data: 0.0001  max mem: 20623
Train Epoch: [2]  [ 800/2023]  eta: 0:36:24  lr: 0.000010  mi_loss: 4.6794  train_loss: 4.6794  time: 1.7855  data: 0.0001  max mem: 20623
Train Epoch: [2]  [ 850/2023]  eta: 0:34:55  lr: 0.000010  mi_loss: 4.6590  train_loss: 4.6590  time: 1.7851  data: 0.0001  max mem: 20623
Train Epoch: [2]  [ 900/2023]  eta: 0:33:25  lr: 0.000010  mi_loss: 4.3808  train_loss: 4.3808  time: 1.7881  data: 0.0001  max mem: 20623
Train Epoch: [2]  [ 950/2023]  eta: 0:31:56  lr: 0.000010  mi_loss: 4.7921  train_loss: 4.7921  time: 1.7869  data: 0.0002  max mem: 20623
Train Epoch: [2]  [1000/2023]  eta: 0:30:27  lr: 0.000010  mi_loss: 4.4649  train_loss: 4.4649  time: 1.7855  data: 0.0001  max mem: 20623
Train Epoch: [2]  [1050/2023]  eta: 0:28:58  lr: 0.000010  mi_loss: 4.4808  train_loss: 4.4808  time: 1.7858  data: 0.0002  max mem: 20623
Train Epoch: [2]  [1100/2023]  eta: 0:27:28  lr: 0.000010  mi_loss: 4.5394  train_loss: 4.5394  time: 1.7859  data: 0.0005  max mem: 20623
Train Epoch: [2]  [1150/2023]  eta: 0:25:59  lr: 0.000010  mi_loss: 4.5828  train_loss: 4.5828  time: 1.7846  data: 0.0003  max mem: 20623
Train Epoch: [2]  [1200/2023]  eta: 0:24:30  lr: 0.000010  mi_loss: 4.7172  train_loss: 4.7172  time: 1.7879  data: 0.0001  max mem: 20623
Train Epoch: [2]  [1250/2023]  eta: 0:23:00  lr: 0.000010  mi_loss: 4.3706  train_loss: 4.3706  time: 1.7830  data: 0.0003  max mem: 20623
Train Epoch: [2]  [1300/2023]  eta: 0:21:31  lr: 0.000010  mi_loss: 4.7427  train_loss: 4.7427  time: 1.7867  data: 0.0002  max mem: 20623
Train Epoch: [2]  [1350/2023]  eta: 0:20:02  lr: 0.000010  mi_loss: 4.7890  train_loss: 4.7890  time: 1.7891  data: 0.0003  max mem: 20623
Train Epoch: [2]  [1400/2023]  eta: 0:18:32  lr: 0.000010  mi_loss: 4.6037  train_loss: 4.6037  time: 1.7869  data: 0.0001  max mem: 20623
Train Epoch: [2]  [1450/2023]  eta: 0:17:03  lr: 0.000010  mi_loss: 4.9114  train_loss: 4.9114  time: 1.7858  data: 0.0002  max mem: 20623
Train Epoch: [2]  [1500/2023]  eta: 0:15:34  lr: 0.000010  mi_loss: 4.6345  train_loss: 4.6345  time: 1.7884  data: 0.0002  max mem: 20623
Train Epoch: [2]  [1550/2023]  eta: 0:14:04  lr: 0.000010  mi_loss: 4.4397  train_loss: 4.4397  time: 1.7854  data: 0.0002  max mem: 20623
Train Epoch: [2]  [1600/2023]  eta: 0:12:35  lr: 0.000010  mi_loss: 4.7051  train_loss: 4.7051  time: 1.7821  data: 0.0001  max mem: 20623
Train Epoch: [2]  [1650/2023]  eta: 0:11:06  lr: 0.000010  mi_loss: 4.6227  train_loss: 4.6227  time: 1.7856  data: 0.0002  max mem: 20623
Train Epoch: [2]  [1700/2023]  eta: 0:09:36  lr: 0.000010  mi_loss: 4.6033  train_loss: 4.6033  time: 1.7817  data: 0.0001  max mem: 20623
Train Epoch: [2]  [1750/2023]  eta: 0:08:07  lr: 0.000010  mi_loss: 4.2576  train_loss: 4.2576  time: 1.7835  data: 0.0002  max mem: 20623
Train Epoch: [2]  [1800/2023]  eta: 0:06:38  lr: 0.000010  mi_loss: 4.5723  train_loss: 4.5723  time: 1.7824  data: 0.0002  max mem: 20623
Train Epoch: [2]  [1850/2023]  eta: 0:05:08  lr: 0.000010  mi_loss: 4.4550  train_loss: 4.4550  time: 1.7824  data: 0.0002  max mem: 20623
Train Epoch: [2]  [1900/2023]  eta: 0:03:39  lr: 0.000010  mi_loss: 4.6849  train_loss: 4.6849  time: 1.7830  data: 0.0002  max mem: 20623
Train Epoch: [2]  [1950/2023]  eta: 0:02:10  lr: 0.000010  mi_loss: 4.9674  train_loss: 4.9674  time: 1.7871  data: 0.0001  max mem: 20623
Train Epoch: [2]  [2000/2023]  eta: 0:00:41  lr: 0.000010  mi_loss: 4.5942  train_loss: 4.5942  time: 1.7769  data: 0.0001  max mem: 20623
Train Epoch: [2]  [2022/2023]  eta: 0:00:01  lr: 0.000010  mi_loss: 4.6984  train_loss: 4.6984  time: 1.7810  data: 0.0009  max mem: 20623
Train Epoch: [2] Total time: 1:00:13 (1.7862 s / it)
Val Epoch: [2]  [  0/225]  eta: 0:04:45  mi_loss: 4.4222  val_loss: 4.4222  accMI: 0.1197  time: 1.2678  data: 1.0318  max mem: 20623
Val Epoch: [2]  [ 50/225]  eta: 0:00:44  mi_loss: 4.5155  val_loss: 4.5155  accMI: 0.1063  time: 0.2340  data: 0.0002  max mem: 20623
Val Epoch: [2]  [100/225]  eta: 0:00:30  mi_loss: 4.6772  val_loss: 4.6772  accMI: 0.1006  time: 0.2342  data: 0.0001  max mem: 20623
Val Epoch: [2]  [150/225]  eta: 0:00:18  mi_loss: 4.4689  val_loss: 4.4689  accMI: 0.1184  time: 0.2347  data: 0.0002  max mem: 20623
Val Epoch: [2]  [200/225]  eta: 0:00:05  mi_loss: 4.6973  val_loss: 4.6973  accMI: 0.1071  time: 0.2351  data: 0.0002  max mem: 20623
Val Epoch: [2]  [224/225]  eta: 0:00:00  mi_loss: 4.7168  val_loss: 4.7168  accMI: 0.1157  time: 0.2331  data: 0.0002  max mem: 20623
Val Epoch: [2] Total time: 0:00:55 (0.2446 s / it)
epoch:2, iter:6068, 2022,  train_loss: 4.698362827301025, valid_loss: 4.510834438535902, idiv_loss:4.510834438535902, acc:0.128099690510167
Averaged stats: lr: 0.0000  mi_loss: 4.6369  train_loss: 4.6369
epoch 2 4.698362827301025
Train Epoch: [3]  [   0/2023]  eta: 1:41:11  lr: 0.000010  mi_loss: 4.5815  train_loss: 4.5815  time: 3.0014  data: 1.1703  max mem: 20623
Train Epoch: [3]  [  50/2023]  eta: 0:59:23  lr: 0.000010  mi_loss: 4.2098  train_loss: 4.2098  time: 1.7843  data: 0.0001  max mem: 20623
Train Epoch: [3]  [ 100/2023]  eta: 0:57:30  lr: 0.000010  mi_loss: 4.7134  train_loss: 4.7134  time: 1.7819  data: 0.0002  max mem: 20623
Train Epoch: [3]  [ 150/2023]  eta: 0:55:55  lr: 0.000010  mi_loss: 4.4275  train_loss: 4.4275  time: 1.7872  data: 0.0002  max mem: 20623
Train Epoch: [3]  [ 200/2023]  eta: 0:54:23  lr: 0.000010  mi_loss: 4.0465  train_loss: 4.0465  time: 1.7854  data: 0.0002  max mem: 20623
Train Epoch: [3]  [ 250/2023]  eta: 0:52:52  lr: 0.000010  mi_loss: 4.5001  train_loss: 4.5001  time: 1.7867  data: 0.0002  max mem: 20623
Train Epoch: [3]  [ 300/2023]  eta: 0:51:21  lr: 0.000010  mi_loss: 4.3463  train_loss: 4.3463  time: 1.7832  data: 0.0002  max mem: 20623
Train Epoch: [3]  [ 350/2023]  eta: 0:49:50  lr: 0.000010  mi_loss: 4.5535  train_loss: 4.5535  time: 1.7812  data: 0.0002  max mem: 20623
Train Epoch: [3]  [ 400/2023]  eta: 0:48:20  lr: 0.000010  mi_loss: 4.5624  train_loss: 4.5624  time: 1.7865  data: 0.0002  max mem: 20623
Train Epoch: [3]  [ 450/2023]  eta: 0:46:51  lr: 0.000010  mi_loss: 4.6226  train_loss: 4.6226  time: 1.7862  data: 0.0002  max mem: 20623
Train Epoch: [3]  [ 500/2023]  eta: 0:45:21  lr: 0.000010  mi_loss: 4.3066  train_loss: 4.3066  time: 1.7853  data: 0.0002  max mem: 20623
Train Epoch: [3]  [ 550/2023]  eta: 0:43:51  lr: 0.000010  mi_loss: 4.4348  train_loss: 4.4348  time: 1.7783  data: 0.0002  max mem: 20623
Train Epoch: [3]  [ 600/2023]  eta: 0:42:21  lr: 0.000010  mi_loss: 4.6207  train_loss: 4.6207  time: 1.7839  data: 0.0002  max mem: 20623
Train Epoch: [3]  [ 650/2023]  eta: 0:40:52  lr: 0.000010  mi_loss: 4.4662  train_loss: 4.4662  time: 1.7807  data: 0.0002  max mem: 20623
Train Epoch: [3]  [ 700/2023]  eta: 0:39:22  lr: 0.000010  mi_loss: 4.4807  train_loss: 4.4807  time: 1.7827  data: 0.0002  max mem: 20623
Train Epoch: [3]  [ 750/2023]  eta: 0:37:53  lr: 0.000010  mi_loss: 4.3195  train_loss: 4.3195  time: 1.7848  data: 0.0002  max mem: 20623
Train Epoch: [3]  [ 800/2023]  eta: 0:36:24  lr: 0.000010  mi_loss: 4.5966  train_loss: 4.5966  time: 1.7882  data: 0.0002  max mem: 20623
Train Epoch: [3]  [ 850/2023]  eta: 0:34:54  lr: 0.000010  mi_loss: 4.2897  train_loss: 4.2897  time: 1.7842  data: 0.0002  max mem: 20623
Train Epoch: [3]  [ 900/2023]  eta: 0:33:25  lr: 0.000010  mi_loss: 4.5165  train_loss: 4.5165  time: 1.7828  data: 0.0002  max mem: 20623
Train Epoch: [3]  [ 950/2023]  eta: 0:31:56  lr: 0.000010  mi_loss: 4.2666  train_loss: 4.2666  time: 1.7844  data: 0.0002  max mem: 20623
Train Epoch: [3]  [1000/2023]  eta: 0:30:26  lr: 0.000010  mi_loss: 4.5742  train_loss: 4.5742  time: 1.7825  data: 0.0002  max mem: 20623
Train Epoch: [3]  [1050/2023]  eta: 0:28:57  lr: 0.000010  mi_loss: 4.7206  train_loss: 4.7206  time: 1.7827  data: 0.0001  max mem: 20623
Train Epoch: [3]  [1100/2023]  eta: 0:27:27  lr: 0.000010  mi_loss: 4.3257  train_loss: 4.3257  time: 1.7839  data: 0.0002  max mem: 20623
Train Epoch: [3]  [1150/2023]  eta: 0:25:58  lr: 0.000010  mi_loss: 4.4420  train_loss: 4.4420  time: 1.7866  data: 0.0002  max mem: 20623
Train Epoch: [3]  [1200/2023]  eta: 0:24:29  lr: 0.000010  mi_loss: 4.4421  train_loss: 4.4421  time: 1.7868  data: 0.0002  max mem: 20623
Train Epoch: [3]  [1250/2023]  eta: 0:23:00  lr: 0.000010  mi_loss: 4.1843  train_loss: 4.1843  time: 1.7855  data: 0.0002  max mem: 20623
Train Epoch: [3]  [1300/2023]  eta: 0:21:30  lr: 0.000010  mi_loss: 4.2319  train_loss: 4.2319  time: 1.7850  data: 0.0002  max mem: 20623
Train Epoch: [3]  [1350/2023]  eta: 0:20:01  lr: 0.000010  mi_loss: 4.3481  train_loss: 4.3481  time: 1.7850  data: 0.0001  max mem: 20623
Train Epoch: [3]  [1400/2023]  eta: 0:18:32  lr: 0.000010  mi_loss: 4.3978  train_loss: 4.3978  time: 1.7848  data: 0.0001  max mem: 20623
Train Epoch: [3]  [1450/2023]  eta: 0:17:03  lr: 0.000010  mi_loss: 4.3193  train_loss: 4.3193  time: 1.7844  data: 0.0001  max mem: 20623
Train Epoch: [3]  [1500/2023]  eta: 0:15:33  lr: 0.000010  mi_loss: 4.3445  train_loss: 4.3445  time: 1.7839  data: 0.0001  max mem: 20623
Train Epoch: [3]  [1550/2023]  eta: 0:14:04  lr: 0.000010  mi_loss: 4.7313  train_loss: 4.7313  time: 1.7842  data: 0.0001  max mem: 20623
Train Epoch: [3]  [1600/2023]  eta: 0:12:35  lr: 0.000010  mi_loss: 4.4773  train_loss: 4.4773  time: 1.7858  data: 0.0001  max mem: 20623
Train Epoch: [3]  [1650/2023]  eta: 0:11:05  lr: 0.000010  mi_loss: 4.4342  train_loss: 4.4342  time: 1.7878  data: 0.0001  max mem: 20623
Train Epoch: [3]  [1700/2023]  eta: 0:09:36  lr: 0.000010  mi_loss: 4.4842  train_loss: 4.4842  time: 1.7850  data: 0.0001  max mem: 20623
Train Epoch: [3]  [1750/2023]  eta: 0:08:07  lr: 0.000010  mi_loss: 4.4944  train_loss: 4.4944  time: 1.7854  data: 0.0001  max mem: 20623
Train Epoch: [3]  [1800/2023]  eta: 0:06:38  lr: 0.000010  mi_loss: 4.2913  train_loss: 4.2913  time: 1.7841  data: 0.0001  max mem: 20623
Train Epoch: [3]  [1850/2023]  eta: 0:05:08  lr: 0.000010  mi_loss: 4.4950  train_loss: 4.4950  time: 1.7818  data: 0.0001  max mem: 20623
Train Epoch: [3]  [1900/2023]  eta: 0:03:39  lr: 0.000010  mi_loss: 4.3897  train_loss: 4.3897  time: 1.7846  data: 0.0001  max mem: 20623
Train Epoch: [3]  [1950/2023]  eta: 0:02:10  lr: 0.000010  mi_loss: 4.5402  train_loss: 4.5402  time: 1.7853  data: 0.0001  max mem: 20623
Train Epoch: [3]  [2000/2023]  eta: 0:00:41  lr: 0.000010  mi_loss: 4.6309  train_loss: 4.6309  time: 1.7871  data: 0.0001  max mem: 20623
Train Epoch: [3]  [2022/2023]  eta: 0:00:01  lr: 0.000010  mi_loss: 4.2781  train_loss: 4.2781  time: 1.7869  data: 0.0009  max mem: 20623
Train Epoch: [3] Total time: 1:00:13 (1.7861 s / it)
Val Epoch: [3]  [  0/225]  eta: 0:04:50  mi_loss: 4.2797  val_loss: 4.2797  accMI: 0.1242  time: 1.2891  data: 1.0516  max mem: 20623
Val Epoch: [3]  [ 50/225]  eta: 0:00:44  mi_loss: 4.4239  val_loss: 4.4239  accMI: 0.1029  time: 0.2341  data: 0.0001  max mem: 20623
Val Epoch: [3]  [100/225]  eta: 0:00:30  mi_loss: 4.5393  val_loss: 4.5393  accMI: 0.1050  time: 0.2344  data: 0.0001  max mem: 20623
Val Epoch: [3]  [150/225]  eta: 0:00:18  mi_loss: 4.3029  val_loss: 4.3029  accMI: 0.1263  time: 0.2348  data: 0.0001  max mem: 20623
Val Epoch: [3]  [200/225]  eta: 0:00:05  mi_loss: 4.5765  val_loss: 4.5765  accMI: 0.1138  time: 0.2350  data: 0.0001  max mem: 20623
Val Epoch: [3]  [224/225]  eta: 0:00:00  mi_loss: 4.5450  val_loss: 4.5450  accMI: 0.1265  time: 0.2330  data: 0.0002  max mem: 20623
Val Epoch: [3] Total time: 0:00:54 (0.2432 s / it)
epoch:3, iter:8091, 2022,  train_loss: 4.278051376342773, valid_loss: 4.373061335881551, idiv_loss:4.373061335881551, acc:0.13631352868345048
Averaged stats: lr: 0.0000  mi_loss: 4.4477  train_loss: 4.4477
epoch 3 4.278051376342773
Train Epoch: [4]  [   0/2023]  eta: 1:47:24  lr: 0.000010  mi_loss: 4.5270  train_loss: 4.5270  time: 3.1858  data: 1.0566  max mem: 20623
Train Epoch: [4]  [  50/2023]  eta: 0:59:43  lr: 0.000010  mi_loss: 4.3680  train_loss: 4.3680  time: 1.7836  data: 0.0001  max mem: 20623
Train Epoch: [4]  [ 100/2023]  eta: 0:57:43  lr: 0.000010  mi_loss: 4.8068  train_loss: 4.8068  time: 1.7833  data: 0.0001  max mem: 20623
Train Epoch: [4]  [ 150/2023]  eta: 0:56:02  lr: 0.000010  mi_loss: 4.3767  train_loss: 4.3767  time: 1.7838  data: 0.0001  max mem: 20623
Train Epoch: [4]  [ 200/2023]  eta: 0:54:29  lr: 0.000010  mi_loss: 4.4717  train_loss: 4.4717  time: 1.7878  data: 0.0001  max mem: 20623
Train Epoch: [4]  [ 250/2023]  eta: 0:52:57  lr: 0.000010  mi_loss: 4.3275  train_loss: 4.3275  time: 1.7845  data: 0.0001  max mem: 20623
Train Epoch: [4]  [ 300/2023]  eta: 0:51:25  lr: 0.000010  mi_loss: 4.3757  train_loss: 4.3757  time: 1.7846  data: 0.0003  max mem: 20623
Train Epoch: [4]  [ 350/2023]  eta: 0:49:54  lr: 0.000010  mi_loss: 4.2656  train_loss: 4.2656  time: 1.7845  data: 0.0003  max mem: 20623
Train Epoch: [4]  [ 400/2023]  eta: 0:48:24  lr: 0.000010  mi_loss: 4.5536  train_loss: 4.5536  time: 1.7868  data: 0.0003  max mem: 20623
Train Epoch: [4]  [ 450/2023]  eta: 0:46:54  lr: 0.000010  mi_loss: 4.4774  train_loss: 4.4774  time: 1.7856  data: 0.0003  max mem: 20623
Train Epoch: [4]  [ 500/2023]  eta: 0:45:24  lr: 0.000010  mi_loss: 4.2562  train_loss: 4.2562  time: 1.7891  data: 0.0002  max mem: 20623
Train Epoch: [4]  [ 550/2023]  eta: 0:43:54  lr: 0.000010  mi_loss: 4.0617  train_loss: 4.0617  time: 1.7866  data: 0.0001  max mem: 20623
Train Epoch: [4]  [ 600/2023]  eta: 0:42:24  lr: 0.000010  mi_loss: 4.7782  train_loss: 4.7782  time: 1.7813  data: 0.0001  max mem: 20623
Train Epoch: [4]  [ 650/2023]  eta: 0:40:54  lr: 0.000010  mi_loss: 4.2281  train_loss: 4.2281  time: 1.7882  data: 0.0002  max mem: 20623
Train Epoch: [4]  [ 700/2023]  eta: 0:39:25  lr: 0.000010  mi_loss: 4.3270  train_loss: 4.3270  time: 1.7820  data: 0.0001  max mem: 20623
Train Epoch: [4]  [ 750/2023]  eta: 0:37:55  lr: 0.000010  mi_loss: 4.3956  train_loss: 4.3956  time: 1.7853  data: 0.0001  max mem: 20623
Train Epoch: [4]  [ 800/2023]  eta: 0:36:25  lr: 0.000010  mi_loss: 4.5743  train_loss: 4.5743  time: 1.7814  data: 0.0001  max mem: 20623
Train Epoch: [4]  [ 850/2023]  eta: 0:34:56  lr: 0.000010  mi_loss: 4.1759  train_loss: 4.1759  time: 1.7829  data: 0.0002  max mem: 20623
Train Epoch: [4]  [ 900/2023]  eta: 0:33:26  lr: 0.000010  mi_loss: 4.3033  train_loss: 4.3033  time: 1.7853  data: 0.0001  max mem: 20623
Train Epoch: [4]  [ 950/2023]  eta: 0:31:57  lr: 0.000010  mi_loss: 4.2150  train_loss: 4.2150  time: 1.7861  data: 0.0001  max mem: 20623
Train Epoch: [4]  [1000/2023]  eta: 0:30:27  lr: 0.000010  mi_loss: 4.1060  train_loss: 4.1060  time: 1.7839  data: 0.0001  max mem: 20623
Train Epoch: [4]  [1050/2023]  eta: 0:28:58  lr: 0.000010  mi_loss: 4.1923  train_loss: 4.1923  time: 1.7849  data: 0.0001  max mem: 20623
Train Epoch: [4]  [1100/2023]  eta: 0:27:29  lr: 0.000010  mi_loss: 4.2927  train_loss: 4.2927  time: 1.7844  data: 0.0001  max mem: 20623
Train Epoch: [4]  [1150/2023]  eta: 0:25:59  lr: 0.000010  mi_loss: 4.5411  train_loss: 4.5411  time: 1.7832  data: 0.0001  max mem: 20623
Train Epoch: [4]  [1200/2023]  eta: 0:24:30  lr: 0.000010  mi_loss: 4.3745  train_loss: 4.3745  time: 1.7856  data: 0.0001  max mem: 20623
Train Epoch: [4]  [1250/2023]  eta: 0:23:01  lr: 0.000010  mi_loss: 4.3715  train_loss: 4.3715  time: 1.7863  data: 0.0001  max mem: 20623
Train Epoch: [4]  [1300/2023]  eta: 0:21:31  lr: 0.000010  mi_loss: 4.1974  train_loss: 4.1974  time: 1.7878  data: 0.0001  max mem: 20623
Train Epoch: [4]  [1350/2023]  eta: 0:20:02  lr: 0.000010  mi_loss: 4.1338  train_loss: 4.1338  time: 1.7900  data: 0.0001  max mem: 20623
Train Epoch: [4]  [1400/2023]  eta: 0:18:33  lr: 0.000010  mi_loss: 4.1087  train_loss: 4.1087  time: 1.7855  data: 0.0001  max mem: 20623
Train Epoch: [4]  [1450/2023]  eta: 0:17:03  lr: 0.000010  mi_loss: 4.2785  train_loss: 4.2785  time: 1.7892  data: 0.0001  max mem: 20623
Train Epoch: [4]  [1500/2023]  eta: 0:15:34  lr: 0.000010  mi_loss: 4.3580  train_loss: 4.3580  time: 1.7850  data: 0.0001  max mem: 20623
Train Epoch: [4]  [1550/2023]  eta: 0:14:05  lr: 0.000010  mi_loss: 4.2903  train_loss: 4.2903  time: 1.7854  data: 0.0002  max mem: 20623
Train Epoch: [4]  [1600/2023]  eta: 0:12:35  lr: 0.000010  mi_loss: 3.9262  train_loss: 3.9262  time: 1.7872  data: 0.0002  max mem: 20623
Train Epoch: [4]  [1650/2023]  eta: 0:11:06  lr: 0.000010  mi_loss: 4.3377  train_loss: 4.3377  time: 1.7832  data: 0.0003  max mem: 20623
Train Epoch: [4]  [1700/2023]  eta: 0:09:37  lr: 0.000010  mi_loss: 4.1017  train_loss: 4.1017  time: 1.7863  data: 0.0003  max mem: 20623
Train Epoch: [4]  [1750/2023]  eta: 0:08:07  lr: 0.000010  mi_loss: 4.5053  train_loss: 4.5053  time: 1.7841  data: 0.0003  max mem: 20623
Train Epoch: [4]  [1800/2023]  eta: 0:06:38  lr: 0.000010  mi_loss: 4.2670  train_loss: 4.2670  time: 1.7872  data: 0.0003  max mem: 20623
Train Epoch: [4]  [1850/2023]  eta: 0:05:09  lr: 0.000010  mi_loss: 4.5243  train_loss: 4.5243  time: 1.7855  data: 0.0002  max mem: 20623
Train Epoch: [4]  [1900/2023]  eta: 0:03:39  lr: 0.000010  mi_loss: 4.3862  train_loss: 4.3862  time: 1.7813  data: 0.0001  max mem: 20623
Train Epoch: [4]  [1950/2023]  eta: 0:02:10  lr: 0.000010  mi_loss: 4.2856  train_loss: 4.2856  time: 1.7850  data: 0.0001  max mem: 20623
Train Epoch: [4]  [2000/2023]  eta: 0:00:41  lr: 0.000010  mi_loss: 4.1756  train_loss: 4.1756  time: 1.7874  data: 0.0001  max mem: 20623
Train Epoch: [4]  [2022/2023]  eta: 0:00:01  lr: 0.000010  mi_loss: 3.9374  train_loss: 3.9374  time: 1.7880  data: 0.0008  max mem: 20623
Train Epoch: [4] Total time: 1:00:14 (1.7868 s / it)
Val Epoch: [4]  [  0/225]  eta: 0:04:38  mi_loss: 4.2118  val_loss: 4.2118  accMI: 0.1275  time: 1.2377  data: 0.9997  max mem: 20623
Val Epoch: [4]  [ 50/225]  eta: 0:00:44  mi_loss: 4.3365  val_loss: 4.3365  accMI: 0.1186  time: 0.2341  data: 0.0001  max mem: 20623
Val Epoch: [4]  [100/225]  eta: 0:00:30  mi_loss: 4.4501  val_loss: 4.4501  accMI: 0.1218  time: 0.2344  data: 0.0001  max mem: 20623
Val Epoch: [4]  [150/225]  eta: 0:00:18  mi_loss: 4.2141  val_loss: 4.2141  accMI: 0.1318  time: 0.2347  data: 0.0001  max mem: 20623
Val Epoch: [4]  [200/225]  eta: 0:00:05  mi_loss: 4.4766  val_loss: 4.4766  accMI: 0.1317  time: 0.2349  data: 0.0001  max mem: 20623
Val Epoch: [4]  [224/225]  eta: 0:00:00  mi_loss: 4.4646  val_loss: 4.4646  accMI: 0.1413  time: 0.2329  data: 0.0002  max mem: 20623
Val Epoch: [4] Total time: 0:00:55 (0.2445 s / it)
epoch:4, iter:10114, 2022,  train_loss: 3.937403917312622, valid_loss: 4.282089038425021, idiv_loss:4.282089038425021, acc:0.1424207349949413
Averaged stats: lr: 0.0000  mi_loss: 4.3276  train_loss: 4.3276
epoch 4 3.937403917312622
Train Epoch: [5]  [   0/2023]  eta: 1:42:31  lr: 0.000010  mi_loss: 4.2403  train_loss: 4.2403  time: 3.0407  data: 1.0661  max mem: 20623
Train Epoch: [5]  [  50/2023]  eta: 0:59:32  lr: 0.000010  mi_loss: 4.1380  train_loss: 4.1380  time: 1.7863  data: 0.0001  max mem: 20623
Train Epoch: [5]  [ 100/2023]  eta: 0:57:39  lr: 0.000010  mi_loss: 4.2386  train_loss: 4.2386  time: 1.7862  data: 0.0002  max mem: 20623
Train Epoch: [5]  [ 150/2023]  eta: 0:56:00  lr: 0.000010  mi_loss: 4.2754  train_loss: 4.2754  time: 1.7862  data: 0.0001  max mem: 20623
Train Epoch: [5]  [ 200/2023]  eta: 0:54:26  lr: 0.000010  mi_loss: 3.9102  train_loss: 3.9102  time: 1.7852  data: 0.0002  max mem: 20623
Train Epoch: [5]  [ 250/2023]  eta: 0:52:53  lr: 0.000010  mi_loss: 4.0998  train_loss: 4.0998  time: 1.7807  data: 0.0002  max mem: 20623
Train Epoch: [5]  [ 300/2023]  eta: 0:51:22  lr: 0.000010  mi_loss: 4.1431  train_loss: 4.1431  time: 1.7872  data: 0.0001  max mem: 20623
Train Epoch: [5]  [ 350/2023]  eta: 0:49:52  lr: 0.000010  mi_loss: 3.7942  train_loss: 3.7942  time: 1.7871  data: 0.0001  max mem: 20623
Train Epoch: [5]  [ 400/2023]  eta: 0:48:22  lr: 0.000010  mi_loss: 4.4647  train_loss: 4.4647  time: 1.7853  data: 0.0001  max mem: 20623
Train Epoch: [5]  [ 450/2023]  eta: 0:46:52  lr: 0.000010  mi_loss: 4.4419  train_loss: 4.4419  time: 1.7903  data: 0.0001  max mem: 20623
Train Epoch: [5]  [ 500/2023]  eta: 0:45:22  lr: 0.000010  mi_loss: 4.3145  train_loss: 4.3145  time: 1.7765  data: 0.0001  max mem: 20623
Train Epoch: [5]  [ 550/2023]  eta: 0:43:53  lr: 0.000010  mi_loss: 4.1600  train_loss: 4.1600  time: 1.7861  data: 0.0002  max mem: 20623
Train Epoch: [5]  [ 600/2023]  eta: 0:42:23  lr: 0.000010  mi_loss: 4.1682  train_loss: 4.1682  time: 1.7873  data: 0.0002  max mem: 20623
Train Epoch: [5]  [ 650/2023]  eta: 0:40:54  lr: 0.000010  mi_loss: 4.0949  train_loss: 4.0949  time: 1.7869  data: 0.0002  max mem: 20623
Train Epoch: [5]  [ 700/2023]  eta: 0:39:24  lr: 0.000010  mi_loss: 4.3499  train_loss: 4.3499  time: 1.7846  data: 0.0002  max mem: 20623
Train Epoch: [5]  [ 750/2023]  eta: 0:37:55  lr: 0.000010  mi_loss: 4.3489  train_loss: 4.3489  time: 1.7863  data: 0.0002  max mem: 20623
Train Epoch: [5]  [ 800/2023]  eta: 0:36:25  lr: 0.000010  mi_loss: 3.6327  train_loss: 3.6327  time: 1.7823  data: 0.0002  max mem: 20623
Train Epoch: [5]  [ 850/2023]  eta: 0:34:56  lr: 0.000010  mi_loss: 4.4289  train_loss: 4.4289  time: 1.7836  data: 0.0002  max mem: 20623
Train Epoch: [5]  [ 900/2023]  eta: 0:33:26  lr: 0.000010  mi_loss: 4.1973  train_loss: 4.1973  time: 1.7874  data: 0.0002  max mem: 20623
Train Epoch: [5]  [ 950/2023]  eta: 0:31:57  lr: 0.000010  mi_loss: 3.9381  train_loss: 3.9381  time: 1.7817  data: 0.0002  max mem: 20623
Train Epoch: [5]  [1000/2023]  eta: 0:30:27  lr: 0.000010  mi_loss: 4.8097  train_loss: 4.8097  time: 1.7863  data: 0.0002  max mem: 20623
Train Epoch: [5]  [1050/2023]  eta: 0:28:58  lr: 0.000010  mi_loss: 4.4519  train_loss: 4.4519  time: 1.7845  data: 0.0002  max mem: 20623
Train Epoch: [5]  [1100/2023]  eta: 0:27:28  lr: 0.000010  mi_loss: 3.8908  train_loss: 3.8908  time: 1.7857  data: 0.0002  max mem: 20623
Train Epoch: [5]  [1150/2023]  eta: 0:25:59  lr: 0.000010  mi_loss: 4.4135  train_loss: 4.4135  time: 1.7869  data: 0.0001  max mem: 20623
Train Epoch: [5]  [1200/2023]  eta: 0:24:30  lr: 0.000010  mi_loss: 4.2528  train_loss: 4.2528  time: 1.7856  data: 0.0001  max mem: 20623
Train Epoch: [5]  [1250/2023]  eta: 0:23:00  lr: 0.000010  mi_loss: 3.8472  train_loss: 3.8472  time: 1.7822  data: 0.0001  max mem: 20623
Train Epoch: [5]  [1300/2023]  eta: 0:21:31  lr: 0.000010  mi_loss: 4.4958  train_loss: 4.4958  time: 1.7844  data: 0.0001  max mem: 20623
Train Epoch: [5]  [1350/2023]  eta: 0:20:02  lr: 0.000010  mi_loss: 4.1455  train_loss: 4.1455  time: 1.7825  data: 0.0001  max mem: 20623
Train Epoch: [5]  [1400/2023]  eta: 0:18:32  lr: 0.000010  mi_loss: 3.8413  train_loss: 3.8413  time: 1.7795  data: 0.0001  max mem: 20623
Train Epoch: [5]  [1450/2023]  eta: 0:17:03  lr: 0.000010  mi_loss: 4.0578  train_loss: 4.0578  time: 1.7820  data: 0.0001  max mem: 20623
Train Epoch: [5]  [1500/2023]  eta: 0:15:33  lr: 0.000010  mi_loss: 4.1287  train_loss: 4.1287  time: 1.7854  data: 0.0001  max mem: 20623
Train Epoch: [5]  [1550/2023]  eta: 0:14:04  lr: 0.000010  mi_loss: 4.1839  train_loss: 4.1839  time: 1.7859  data: 0.0001  max mem: 20623
Train Epoch: [5]  [1600/2023]  eta: 0:12:35  lr: 0.000010  mi_loss: 4.1047  train_loss: 4.1047  time: 1.7847  data: 0.0001  max mem: 20623
Train Epoch: [5]  [1650/2023]  eta: 0:11:06  lr: 0.000010  mi_loss: 3.7995  train_loss: 3.7995  time: 1.7843  data: 0.0001  max mem: 20623
Train Epoch: [5]  [1700/2023]  eta: 0:09:36  lr: 0.000010  mi_loss: 3.9979  train_loss: 3.9979  time: 1.7828  data: 0.0001  max mem: 20623
Train Epoch: [5]  [1750/2023]  eta: 0:08:07  lr: 0.000010  mi_loss: 4.4184  train_loss: 4.4184  time: 1.7832  data: 0.0001  max mem: 20623
Train Epoch: [5]  [1800/2023]  eta: 0:06:38  lr: 0.000010  mi_loss: 4.0835  train_loss: 4.0835  time: 1.7827  data: 0.0001  max mem: 20623
Train Epoch: [5]  [1850/2023]  eta: 0:05:08  lr: 0.000010  mi_loss: 4.4530  train_loss: 4.4530  time: 1.7806  data: 0.0001  max mem: 20623
Train Epoch: [5]  [1900/2023]  eta: 0:03:39  lr: 0.000010  mi_loss: 3.9681  train_loss: 3.9681  time: 1.7841  data: 0.0002  max mem: 20623
Train Epoch: [5]  [1950/2023]  eta: 0:02:10  lr: 0.000010  mi_loss: 4.3035  train_loss: 4.3035  time: 1.7842  data: 0.0002  max mem: 20623
Train Epoch: [5]  [2000/2023]  eta: 0:00:41  lr: 0.000010  mi_loss: 4.2786  train_loss: 4.2786  time: 1.7859  data: 0.0002  max mem: 20623
Train Epoch: [5]  [2022/2023]  eta: 0:00:01  lr: 0.000010  mi_loss: 4.3342  train_loss: 4.3342  time: 1.7878  data: 0.0008  max mem: 20623
Train Epoch: [5] Total time: 1:00:13 (1.7860 s / it)
Val Epoch: [5]  [  0/225]  eta: 0:04:36  mi_loss: 4.1444  val_loss: 4.1444  accMI: 0.1409  time: 1.2302  data: 0.9932  max mem: 20623
Val Epoch: [5]  [ 50/225]  eta: 0:00:44  mi_loss: 4.2736  val_loss: 4.2736  accMI: 0.1074  time: 0.2340  data: 0.0001  max mem: 20623
Val Epoch: [5]  [100/225]  eta: 0:00:30  mi_loss: 4.3958  val_loss: 4.3958  accMI: 0.1251  time: 0.2346  data: 0.0001  max mem: 20623
Val Epoch: [5]  [150/225]  eta: 0:00:18  mi_loss: 4.1303  val_loss: 4.1303  accMI: 0.1475  time: 0.2351  data: 0.0001  max mem: 20623
Val Epoch: [5]  [200/225]  eta: 0:00:05  mi_loss: 4.3899  val_loss: 4.3899  accMI: 0.1283  time: 0.2353  data: 0.0002  max mem: 20623
Val Epoch: [5]  [224/225]  eta: 0:00:00  mi_loss: 4.3584  val_loss: 4.3584  accMI: 0.1534  time: 0.2339  data: 0.0005  max mem: 20623
Val Epoch: [5] Total time: 0:00:54 (0.2431 s / it)
epoch:5, iter:12137, 2022,  train_loss: 4.33415412902832, valid_loss: 4.213089864518907, idiv_loss:4.213089864518907, acc:0.14681916495164235
Averaged stats: lr: 0.0000  mi_loss: 4.2423  train_loss: 4.2423
epoch 5 4.33415412902832
Train Epoch: [6]  [   0/2023]  eta: 1:42:22  lr: 0.000010  mi_loss: 4.5550  train_loss: 4.5550  time: 3.0365  data: 1.0843  max mem: 20623
Train Epoch: [6]  [  50/2023]  eta: 0:59:45  lr: 0.000010  mi_loss: 4.0187  train_loss: 4.0187  time: 1.7871  data: 0.0001  max mem: 20623
Train Epoch: [6]  [ 100/2023]  eta: 0:57:44  lr: 0.000010  mi_loss: 4.4760  train_loss: 4.4760  time: 1.7840  data: 0.0001  max mem: 20623
Train Epoch: [6]  [ 150/2023]  eta: 0:56:05  lr: 0.000010  mi_loss: 4.0009  train_loss: 4.0009  time: 1.7860  data: 0.0001  max mem: 20623
Train Epoch: [6]  [ 200/2023]  eta: 0:54:30  lr: 0.000010  mi_loss: 4.1716  train_loss: 4.1716  time: 1.7853  data: 0.0001  max mem: 20623
Train Epoch: [6]  [ 250/2023]  eta: 0:52:58  lr: 0.000010  mi_loss: 4.0654  train_loss: 4.0654  time: 1.7867  data: 0.0001  max mem: 20623
Train Epoch: [6]  [ 300/2023]  eta: 0:51:27  lr: 0.000010  mi_loss: 3.7668  train_loss: 3.7668  time: 1.7896  data: 0.0001  max mem: 20623
Train Epoch: [6]  [ 350/2023]  eta: 0:49:56  lr: 0.000010  mi_loss: 3.9293  train_loss: 3.9293  time: 1.7853  data: 0.0001  max mem: 20623
Train Epoch: [6]  [ 400/2023]  eta: 0:48:26  lr: 0.000010  mi_loss: 4.1594  train_loss: 4.1594  time: 1.7874  data: 0.0001  max mem: 20623
Train Epoch: [6]  [ 450/2023]  eta: 0:46:55  lr: 0.000010  mi_loss: 3.9165  train_loss: 3.9165  time: 1.7864  data: 0.0001  max mem: 20623
Train Epoch: [6]  [ 500/2023]  eta: 0:45:25  lr: 0.000010  mi_loss: 4.3173  train_loss: 4.3173  time: 1.7887  data: 0.0001  max mem: 20623
Train Epoch: [6]  [ 550/2023]  eta: 0:43:55  lr: 0.000010  mi_loss: 3.9143  train_loss: 3.9143  time: 1.7876  data: 0.0001  max mem: 20623
Train Epoch: [6]  [ 600/2023]  eta: 0:42:25  lr: 0.000010  mi_loss: 4.3103  train_loss: 4.3103  time: 1.7880  data: 0.0001  max mem: 20623
Train Epoch: [6]  [ 650/2023]  eta: 0:40:56  lr: 0.000010  mi_loss: 3.8246  train_loss: 3.8246  time: 1.7882  data: 0.0001  max mem: 20623
Train Epoch: [6]  [ 700/2023]  eta: 0:39:26  lr: 0.000010  mi_loss: 4.1400  train_loss: 4.1400  time: 1.7866  data: 0.0001  max mem: 20623
Train Epoch: [6]  [ 750/2023]  eta: 0:37:56  lr: 0.000010  mi_loss: 4.0705  train_loss: 4.0705  time: 1.7832  data: 0.0001  max mem: 20623
Train Epoch: [6]  [ 800/2023]  eta: 0:36:27  lr: 0.000010  mi_loss: 4.0632  train_loss: 4.0632  time: 1.7848  data: 0.0001  max mem: 20623
Train Epoch: [6]  [ 850/2023]  eta: 0:34:57  lr: 0.000010  mi_loss: 4.3092  train_loss: 4.3092  time: 1.7854  data: 0.0001  max mem: 20623
Train Epoch: [6]  [ 900/2023]  eta: 0:33:28  lr: 0.000010  mi_loss: 3.7677  train_loss: 3.7677  time: 1.7862  data: 0.0001  max mem: 20623
Train Epoch: [6]  [ 950/2023]  eta: 0:31:58  lr: 0.000010  mi_loss: 3.9952  train_loss: 3.9952  time: 1.7837  data: 0.0001  max mem: 20623
Train Epoch: [6]  [1000/2023]  eta: 0:30:29  lr: 0.000010  mi_loss: 4.1180  train_loss: 4.1180  time: 1.7875  data: 0.0001  max mem: 20623
Train Epoch: [6]  [1050/2023]  eta: 0:28:59  lr: 0.000010  mi_loss: 4.0477  train_loss: 4.0477  time: 1.7861  data: 0.0001  max mem: 20623
Train Epoch: [6]  [1100/2023]  eta: 0:27:30  lr: 0.000010  mi_loss: 3.6190  train_loss: 3.6190  time: 1.7825  data: 0.0002  max mem: 20623
Train Epoch: [6]  [1150/2023]  eta: 0:26:00  lr: 0.000010  mi_loss: 4.0499  train_loss: 4.0499  time: 1.7819  data: 0.0001  max mem: 20623
Train Epoch: [6]  [1200/2023]  eta: 0:24:31  lr: 0.000010  mi_loss: 4.4560  train_loss: 4.4560  time: 1.7854  data: 0.0001  max mem: 20623
Train Epoch: [6]  [1250/2023]  eta: 0:23:01  lr: 0.000010  mi_loss: 3.5701  train_loss: 3.5701  time: 1.7847  data: 0.0001  max mem: 20623
Train Epoch: [6]  [1300/2023]  eta: 0:21:32  lr: 0.000010  mi_loss: 4.2805  train_loss: 4.2805  time: 1.7832  data: 0.0002  max mem: 20623
Train Epoch: [6]  [1350/2023]  eta: 0:20:02  lr: 0.000010  mi_loss: 4.1192  train_loss: 4.1192  time: 1.7892  data: 0.0003  max mem: 20623
Train Epoch: [6]  [1400/2023]  eta: 0:18:33  lr: 0.000010  mi_loss: 4.3951  train_loss: 4.3951  time: 1.7872  data: 0.0003  max mem: 20623
Train Epoch: [6]  [1450/2023]  eta: 0:17:03  lr: 0.000010  mi_loss: 3.6980  train_loss: 3.6980  time: 1.7840  data: 0.0003  max mem: 20623
Train Epoch: [6]  [1500/2023]  eta: 0:15:34  lr: 0.000010  mi_loss: 4.2653  train_loss: 4.2653  time: 1.7882  data: 0.0003  max mem: 20623
Train Epoch: [6]  [1550/2023]  eta: 0:14:05  lr: 0.000010  mi_loss: 4.4096  train_loss: 4.4096  time: 1.7899  data: 0.0003  max mem: 20623
Train Epoch: [6]  [1600/2023]  eta: 0:12:35  lr: 0.000010  mi_loss: 3.8415  train_loss: 3.8415  time: 1.7847  data: 0.0003  max mem: 20623
Train Epoch: [6]  [1650/2023]  eta: 0:11:06  lr: 0.000010  mi_loss: 4.4869  train_loss: 4.4869  time: 1.7903  data: 0.0003  max mem: 20623
Train Epoch: [6]  [1700/2023]  eta: 0:09:37  lr: 0.000010  mi_loss: 4.0675  train_loss: 4.0675  time: 1.7821  data: 0.0003  max mem: 20623
Train Epoch: [6]  [1750/2023]  eta: 0:08:07  lr: 0.000010  mi_loss: 4.2484  train_loss: 4.2484  time: 1.7831  data: 0.0003  max mem: 20623
Train Epoch: [6]  [1800/2023]  eta: 0:06:38  lr: 0.000010  mi_loss: 3.9358  train_loss: 3.9358  time: 1.7801  data: 0.0003  max mem: 20623
Train Epoch: [6]  [1850/2023]  eta: 0:05:09  lr: 0.000010  mi_loss: 4.1795  train_loss: 4.1795  time: 1.7882  data: 0.0003  max mem: 20623
Train Epoch: [6]  [1900/2023]  eta: 0:03:39  lr: 0.000010  mi_loss: 4.0853  train_loss: 4.0853  time: 1.7817  data: 0.0001  max mem: 20623
Train Epoch: [6]  [1950/2023]  eta: 0:02:10  lr: 0.000010  mi_loss: 4.0415  train_loss: 4.0415  time: 1.7811  data: 0.0003  max mem: 20623
Train Epoch: [6]  [2000/2023]  eta: 0:00:41  lr: 0.000010  mi_loss: 3.9867  train_loss: 3.9867  time: 1.7872  data: 0.0001  max mem: 20623
Train Epoch: [6]  [2022/2023]  eta: 0:00:01  lr: 0.000010  mi_loss: 4.2572  train_loss: 4.2572  time: 1.7867  data: 0.0008  max mem: 20623
Train Epoch: [6] Total time: 1:00:15 (1.7870 s / it)
Val Epoch: [6]  [  0/225]  eta: 0:04:39  mi_loss: 4.0949  val_loss: 4.0949  accMI: 0.1488  time: 1.2427  data: 1.0069  max mem: 20623
Val Epoch: [6]  [ 50/225]  eta: 0:00:44  mi_loss: 4.2488  val_loss: 4.2488  accMI: 0.1119  time: 0.2340  data: 0.0001  max mem: 20623
Val Epoch: [6]  [100/225]  eta: 0:00:30  mi_loss: 4.3686  val_loss: 4.3686  accMI: 0.1263  time: 0.2346  data: 0.0002  max mem: 20623
Val Epoch: [6]  [150/225]  eta: 0:00:18  mi_loss: 4.0810  val_loss: 4.0810  accMI: 0.1441  time: 0.2349  data: 0.0002  max mem: 20623
Val Epoch: [6]  [200/225]  eta: 0:00:05  mi_loss: 4.3205  val_loss: 4.3205  accMI: 0.1283  time: 0.2352  data: 0.0001  max mem: 20623
Val Epoch: [6]  [224/225]  eta: 0:00:00  mi_loss: 4.3749  val_loss: 4.3749  accMI: 0.1467  time: 0.2335  data: 0.0002  max mem: 20623
Val Epoch: [6] Total time: 0:00:55 (0.2447 s / it)
epoch:6, iter:14160, 2022,  train_loss: 4.257211208343506, valid_loss: 4.166961921056112, idiv_loss:4.166961921056112, acc:0.14974649167723125
Averaged stats: lr: 0.0000  mi_loss: 4.1776  train_loss: 4.1776
epoch 6 4.257211208343506
Train Epoch: [7]  [   0/2023]  eta: 1:43:13  lr: 0.000010  mi_loss: 4.1156  train_loss: 4.1156  time: 3.0614  data: 1.0934  max mem: 20623
Train Epoch: [7]  [  50/2023]  eta: 0:59:35  lr: 0.000010  mi_loss: 3.8960  train_loss: 3.8960  time: 1.7873  data: 0.0001  max mem: 20623
Train Epoch: [7]  [ 100/2023]  eta: 0:57:39  lr: 0.000010  mi_loss: 4.2514  train_loss: 4.2514  time: 1.7877  data: 0.0002  max mem: 20623
Train Epoch: [7]  [ 150/2023]  eta: 0:56:01  lr: 0.000010  mi_loss: 3.6773  train_loss: 3.6773  time: 1.7881  data: 0.0002  max mem: 20623
Train Epoch: [7]  [ 200/2023]  eta: 0:54:27  lr: 0.000010  mi_loss: 4.1098  train_loss: 4.1098  time: 1.7864  data: 0.0001  max mem: 20623
Train Epoch: [7]  [ 250/2023]  eta: 0:52:56  lr: 0.000010  mi_loss: 3.8339  train_loss: 3.8339  time: 1.7878  data: 0.0001  max mem: 20623
Train Epoch: [7]  [ 300/2023]  eta: 0:51:24  lr: 0.000010  mi_loss: 4.1722  train_loss: 4.1722  time: 1.7874  data: 0.0002  max mem: 20623
Train Epoch: [7]  [ 350/2023]  eta: 0:49:54  lr: 0.000010  mi_loss: 4.2848  train_loss: 4.2848  time: 1.7877  data: 0.0002  max mem: 20623
Train Epoch: [7]  [ 400/2023]  eta: 0:48:23  lr: 0.000010  mi_loss: 4.2061  train_loss: 4.2061  time: 1.7848  data: 0.0002  max mem: 20623
Train Epoch: [7]  [ 450/2023]  eta: 0:46:53  lr: 0.000010  mi_loss: 4.2017  train_loss: 4.2017  time: 1.7854  data: 0.0002  max mem: 20623
Train Epoch: [7]  [ 500/2023]  eta: 0:45:23  lr: 0.000010  mi_loss: 4.1883  train_loss: 4.1883  time: 1.7867  data: 0.0002  max mem: 20623
Train Epoch: [7]  [ 550/2023]  eta: 0:43:54  lr: 0.000010  mi_loss: 4.2118  train_loss: 4.2118  time: 1.7855  data: 0.0001  max mem: 20623
Train Epoch: [7]  [ 600/2023]  eta: 0:42:24  lr: 0.000010  mi_loss: 3.8095  train_loss: 3.8095  time: 1.7864  data: 0.0001  max mem: 20623
Train Epoch: [7]  [ 650/2023]  eta: 0:40:55  lr: 0.000010  mi_loss: 3.5634  train_loss: 3.5634  time: 1.7861  data: 0.0001  max mem: 20623
Train Epoch: [7]  [ 700/2023]  eta: 0:39:25  lr: 0.000010  mi_loss: 4.0059  train_loss: 4.0059  time: 1.7818  data: 0.0001  max mem: 20623
Train Epoch: [7]  [ 750/2023]  eta: 0:37:55  lr: 0.000010  mi_loss: 4.1400  train_loss: 4.1400  time: 1.7838  data: 0.0001  max mem: 20623
Train Epoch: [7]  [ 800/2023]  eta: 0:36:26  lr: 0.000010  mi_loss: 4.0683  train_loss: 4.0683  time: 1.7842  data: 0.0001  max mem: 20623
Train Epoch: [7]  [ 850/2023]  eta: 0:34:56  lr: 0.000010  mi_loss: 4.0521  train_loss: 4.0521  time: 1.7860  data: 0.0001  max mem: 20623
Train Epoch: [7]  [ 900/2023]  eta: 0:33:27  lr: 0.000010  mi_loss: 3.9894  train_loss: 3.9894  time: 1.7861  data: 0.0003  max mem: 20623
Train Epoch: [7]  [ 950/2023]  eta: 0:31:57  lr: 0.000010  mi_loss: 3.9815  train_loss: 3.9815  time: 1.7848  data: 0.0001  max mem: 20623
Train Epoch: [7]  [1000/2023]  eta: 0:30:28  lr: 0.000010  mi_loss: 3.9623  train_loss: 3.9623  time: 1.7883  data: 0.0001  max mem: 20623
Train Epoch: [7]  [1050/2023]  eta: 0:28:58  lr: 0.000010  mi_loss: 4.0398  train_loss: 4.0398  time: 1.7877  data: 0.0001  max mem: 20623
Train Epoch: [7]  [1100/2023]  eta: 0:27:29  lr: 0.000010  mi_loss: 4.3974  train_loss: 4.3974  time: 1.7835  data: 0.0001  max mem: 20623
Train Epoch: [7]  [1150/2023]  eta: 0:26:00  lr: 0.000010  mi_loss: 4.1685  train_loss: 4.1685  time: 1.7865  data: 0.0001  max mem: 20623
Train Epoch: [7]  [1200/2023]  eta: 0:24:30  lr: 0.000010  mi_loss: 4.2053  train_loss: 4.2053  time: 1.7830  data: 0.0002  max mem: 20623
Train Epoch: [7]  [1250/2023]  eta: 0:23:01  lr: 0.000010  mi_loss: 3.9713  train_loss: 3.9713  time: 1.7829  data: 0.0002  max mem: 20623
Train Epoch: [7]  [1300/2023]  eta: 0:21:31  lr: 0.000010  mi_loss: 4.1094  train_loss: 4.1094  time: 1.7834  data: 0.0002  max mem: 20623
Train Epoch: [7]  [1350/2023]  eta: 0:20:02  lr: 0.000010  mi_loss: 4.0363  train_loss: 4.0363  time: 1.7826  data: 0.0002  max mem: 20623
Train Epoch: [7]  [1400/2023]  eta: 0:18:32  lr: 0.000010  mi_loss: 3.9665  train_loss: 3.9665  time: 1.7856  data: 0.0002  max mem: 20623
Train Epoch: [7]  [1450/2023]  eta: 0:17:03  lr: 0.000010  mi_loss: 4.1037  train_loss: 4.1037  time: 1.7844  data: 0.0002  max mem: 20623
Train Epoch: [7]  [1500/2023]  eta: 0:15:34  lr: 0.000010  mi_loss: 4.2629  train_loss: 4.2629  time: 1.7881  data: 0.0002  max mem: 20623
Train Epoch: [7]  [1550/2023]  eta: 0:14:04  lr: 0.000010  mi_loss: 4.0876  train_loss: 4.0876  time: 1.7831  data: 0.0002  max mem: 20623
Train Epoch: [7]  [1600/2023]  eta: 0:12:35  lr: 0.000010  mi_loss: 4.3756  train_loss: 4.3756  time: 1.7851  data: 0.0002  max mem: 20623
Train Epoch: [7]  [1650/2023]  eta: 0:11:06  lr: 0.000010  mi_loss: 4.3564  train_loss: 4.3564  time: 1.7827  data: 0.0002  max mem: 20623
Train Epoch: [7]  [1700/2023]  eta: 0:09:36  lr: 0.000010  mi_loss: 3.7367  train_loss: 3.7367  time: 1.7869  data: 0.0002  max mem: 20623
Train Epoch: [7]  [1750/2023]  eta: 0:08:07  lr: 0.000010  mi_loss: 4.0624  train_loss: 4.0624  time: 1.7858  data: 0.0002  max mem: 20623
Train Epoch: [7]  [1800/2023]  eta: 0:06:38  lr: 0.000010  mi_loss: 4.3161  train_loss: 4.3161  time: 1.7825  data: 0.0002  max mem: 20623
Train Epoch: [7]  [1850/2023]  eta: 0:05:08  lr: 0.000010  mi_loss: 4.3564  train_loss: 4.3564  time: 1.7865  data: 0.0002  max mem: 20623
Train Epoch: [7]  [1900/2023]  eta: 0:03:39  lr: 0.000010  mi_loss: 4.3712  train_loss: 4.3712  time: 1.7861  data: 0.0002  max mem: 20623
Train Epoch: [7]  [1950/2023]  eta: 0:02:10  lr: 0.000010  mi_loss: 3.8101  train_loss: 3.8101  time: 1.7855  data: 0.0002  max mem: 20623
Train Epoch: [7]  [2000/2023]  eta: 0:00:41  lr: 0.000010  mi_loss: 4.2977  train_loss: 4.2977  time: 1.7873  data: 0.0001  max mem: 20623
Train Epoch: [7]  [2022/2023]  eta: 0:00:01  lr: 0.000010  mi_loss: 4.1541  train_loss: 4.1541  time: 1.7824  data: 0.0007  max mem: 20623
Train Epoch: [7] Total time: 1:00:14 (1.7865 s / it)
Val Epoch: [7]  [  0/225]  eta: 0:04:38  mi_loss: 4.0220  val_loss: 4.0220  accMI: 0.1544  time: 1.2387  data: 1.0014  max mem: 20623
Val Epoch: [7]  [ 50/225]  eta: 0:00:44  mi_loss: 4.2058  val_loss: 4.2058  accMI: 0.1186  time: 0.2340  data: 0.0001  max mem: 20623
Val Epoch: [7]  [100/225]  eta: 0:00:30  mi_loss: 4.3510  val_loss: 4.3510  accMI: 0.1307  time: 0.2344  data: 0.0001  max mem: 20623
Val Epoch: [7]  [150/225]  eta: 0:00:18  mi_loss: 4.0371  val_loss: 4.0371  accMI: 0.1575  time: 0.2345  data: 0.0001  max mem: 20623
Val Epoch: [7]  [200/225]  eta: 0:00:05  mi_loss: 4.2619  val_loss: 4.2619  accMI: 0.1440  time: 0.2349  data: 0.0002  max mem: 20623
Val Epoch: [7]  [224/225]  eta: 0:00:00  mi_loss: 4.3240  val_loss: 4.3240  accMI: 0.1642  time: 0.2331  data: 0.0002  max mem: 20623
Val Epoch: [7] Total time: 0:00:54 (0.2428 s / it)
epoch:7, iter:16183, 2022,  train_loss: 4.154065132141113, valid_loss: 4.129093797471788, idiv_loss:4.129093797471788, acc:0.15299841579463747
Averaged stats: lr: 0.0000  mi_loss: 4.1258  train_loss: 4.1258
epoch 7 4.154065132141113
Train Epoch: [8]  [   0/2023]  eta: 1:43:17  lr: 0.000010  mi_loss: 4.1112  train_loss: 4.1112  time: 3.0636  data: 1.0831  max mem: 20623
Train Epoch: [8]  [  50/2023]  eta: 0:59:41  lr: 0.000010  mi_loss: 4.2589  train_loss: 4.2589  time: 1.7857  data: 0.0001  max mem: 20623
Train Epoch: [8]  [ 100/2023]  eta: 0:57:41  lr: 0.000010  mi_loss: 3.9073  train_loss: 3.9073  time: 1.7810  data: 0.0001  max mem: 20623
Train Epoch: [8]  [ 150/2023]  eta: 0:56:03  lr: 0.000010  mi_loss: 3.8866  train_loss: 3.8866  time: 1.7890  data: 0.0001  max mem: 20623
Train Epoch: [8]  [ 200/2023]  eta: 0:54:28  lr: 0.000010  mi_loss: 4.1497  train_loss: 4.1497  time: 1.7860  data: 0.0001  max mem: 20623
Train Epoch: [8]  [ 250/2023]  eta: 0:52:56  lr: 0.000010  mi_loss: 3.9624  train_loss: 3.9624  time: 1.7832  data: 0.0001  max mem: 20623
Train Epoch: [8]  [ 300/2023]  eta: 0:51:25  lr: 0.000010  mi_loss: 4.1995  train_loss: 4.1995  time: 1.7847  data: 0.0002  max mem: 20623
Train Epoch: [8]  [ 350/2023]  eta: 0:49:55  lr: 0.000010  mi_loss: 3.9483  train_loss: 3.9483  time: 1.7903  data: 0.0002  max mem: 20623
Train Epoch: [8]  [ 400/2023]  eta: 0:48:25  lr: 0.000010  mi_loss: 3.9827  train_loss: 3.9827  time: 1.7892  data: 0.0001  max mem: 20623
Train Epoch: [8]  [ 450/2023]  eta: 0:46:55  lr: 0.000010  mi_loss: 4.2875  train_loss: 4.2875  time: 1.7889  data: 0.0001  max mem: 20623
Train Epoch: [8]  [ 500/2023]  eta: 0:45:25  lr: 0.000010  mi_loss: 4.2224  train_loss: 4.2224  time: 1.7822  data: 0.0002  max mem: 20623
Train Epoch: [8]  [ 550/2023]  eta: 0:43:55  lr: 0.000010  mi_loss: 4.0898  train_loss: 4.0898  time: 1.7888  data: 0.0001  max mem: 20623
Train Epoch: [8]  [ 600/2023]  eta: 0:42:26  lr: 0.000010  mi_loss: 4.0466  train_loss: 4.0466  time: 1.7898  data: 0.0001  max mem: 20623
Train Epoch: [8]  [ 650/2023]  eta: 0:40:56  lr: 0.000010  mi_loss: 4.0841  train_loss: 4.0841  time: 1.7841  data: 0.0001  max mem: 20623
Train Epoch: [8]  [ 700/2023]  eta: 0:39:26  lr: 0.000010  mi_loss: 4.1853  train_loss: 4.1853  time: 1.7852  data: 0.0002  max mem: 20623
Train Epoch: [8]  [ 750/2023]  eta: 0:37:57  lr: 0.000010  mi_loss: 4.0740  train_loss: 4.0740  time: 1.7845  data: 0.0003  max mem: 20623
Train Epoch: [8]  [ 800/2023]  eta: 0:36:27  lr: 0.000010  mi_loss: 4.3348  train_loss: 4.3348  time: 1.7879  data: 0.0003  max mem: 20623
Train Epoch: [8]  [ 850/2023]  eta: 0:34:58  lr: 0.000010  mi_loss: 4.2303  train_loss: 4.2303  time: 1.7846  data: 0.0003  max mem: 20623
Train Epoch: [8]  [ 900/2023]  eta: 0:33:28  lr: 0.000010  mi_loss: 4.3608  train_loss: 4.3608  time: 1.7863  data: 0.0003  max mem: 20623
Train Epoch: [8]  [ 950/2023]  eta: 0:31:58  lr: 0.000010  mi_loss: 4.1808  train_loss: 4.1808  time: 1.7863  data: 0.0003  max mem: 20623
Train Epoch: [8]  [1000/2023]  eta: 0:30:29  lr: 0.000010  mi_loss: 3.9259  train_loss: 3.9259  time: 1.7854  data: 0.0003  max mem: 20623
Train Epoch: [8]  [1050/2023]  eta: 0:28:59  lr: 0.000010  mi_loss: 3.8879  train_loss: 3.8879  time: 1.7902  data: 0.0003  max mem: 20623
Train Epoch: [8]  [1100/2023]  eta: 0:27:30  lr: 0.000010  mi_loss: 3.9472  train_loss: 3.9472  time: 1.7872  data: 0.0003  max mem: 20623
Train Epoch: [8]  [1150/2023]  eta: 0:26:00  lr: 0.000010  mi_loss: 3.9655  train_loss: 3.9655  time: 1.7850  data: 0.0003  max mem: 20623
Train Epoch: [8]  [1200/2023]  eta: 0:24:31  lr: 0.000010  mi_loss: 4.0205  train_loss: 4.0205  time: 1.7861  data: 0.0001  max mem: 20623
Train Epoch: [8]  [1250/2023]  eta: 0:23:02  lr: 0.000010  mi_loss: 4.2614  train_loss: 4.2614  time: 1.7837  data: 0.0001  max mem: 20623
Train Epoch: [8]  [1300/2023]  eta: 0:21:32  lr: 0.000010  mi_loss: 4.2612  train_loss: 4.2612  time: 1.7890  data: 0.0001  max mem: 20623
Train Epoch: [8]  [1350/2023]  eta: 0:20:03  lr: 0.000010  mi_loss: 3.8930  train_loss: 3.8930  time: 1.7883  data: 0.0001  max mem: 20623
Train Epoch: [8]  [1400/2023]  eta: 0:18:33  lr: 0.000010  mi_loss: 3.6091  train_loss: 3.6091  time: 1.7887  data: 0.0002  max mem: 20623
Train Epoch: [8]  [1450/2023]  eta: 0:17:04  lr: 0.000010  mi_loss: 4.1410  train_loss: 4.1410  time: 1.7844  data: 0.0003  max mem: 20623
Train Epoch: [8]  [1500/2023]  eta: 0:15:34  lr: 0.000010  mi_loss: 3.8437  train_loss: 3.8437  time: 1.7842  data: 0.0001  max mem: 20623
Train Epoch: [8]  [1550/2023]  eta: 0:14:05  lr: 0.000010  mi_loss: 3.9093  train_loss: 3.9093  time: 1.7841  data: 0.0001  max mem: 20623
Train Epoch: [8]  [1600/2023]  eta: 0:12:36  lr: 0.000010  mi_loss: 3.8677  train_loss: 3.8677  time: 1.7856  data: 0.0001  max mem: 20623
Train Epoch: [8]  [1650/2023]  eta: 0:11:06  lr: 0.000010  mi_loss: 4.2353  train_loss: 4.2353  time: 1.7839  data: 0.0001  max mem: 20623
Train Epoch: [8]  [1700/2023]  eta: 0:09:37  lr: 0.000010  mi_loss: 4.0621  train_loss: 4.0621  time: 1.7844  data: 0.0001  max mem: 20623
Train Epoch: [8]  [1750/2023]  eta: 0:08:07  lr: 0.000010  mi_loss: 4.1852  train_loss: 4.1852  time: 1.7878  data: 0.0001  max mem: 20623
Train Epoch: [8]  [1800/2023]  eta: 0:06:38  lr: 0.000010  mi_loss: 4.1527  train_loss: 4.1527  time: 1.7868  data: 0.0001  max mem: 20623
Train Epoch: [8]  [1850/2023]  eta: 0:05:09  lr: 0.000010  mi_loss: 4.1033  train_loss: 4.1033  time: 1.7859  data: 0.0001  max mem: 20623
Train Epoch: [8]  [1900/2023]  eta: 0:03:39  lr: 0.000010  mi_loss: 4.1396  train_loss: 4.1396  time: 1.7811  data: 0.0001  max mem: 20623
Train Epoch: [8]  [1950/2023]  eta: 0:02:10  lr: 0.000010  mi_loss: 4.3719  train_loss: 4.3719  time: 1.7892  data: 0.0001  max mem: 20623
Train Epoch: [8]  [2000/2023]  eta: 0:00:41  lr: 0.000010  mi_loss: 4.1830  train_loss: 4.1830  time: 1.7853  data: 0.0001  max mem: 20623
Train Epoch: [8]  [2022/2023]  eta: 0:00:01  lr: 0.000010  mi_loss: 4.1601  train_loss: 4.1601  time: 1.7816  data: 0.0007  max mem: 20623
Train Epoch: [8] Total time: 1:00:16 (1.7879 s / it)
Val Epoch: [8]  [  0/225]  eta: 0:04:40  mi_loss: 4.0137  val_loss: 4.0137  accMI: 0.1532  time: 1.2476  data: 1.0095  max mem: 20623
Val Epoch: [8]  [ 50/225]  eta: 0:00:44  mi_loss: 4.1854  val_loss: 4.1854  accMI: 0.1130  time: 0.2339  data: 0.0001  max mem: 20623
Val Epoch: [8]  [100/225]  eta: 0:00:30  mi_loss: 4.2881  val_loss: 4.2881  accMI: 0.1285  time: 0.2344  data: 0.0001  max mem: 20623
Val Epoch: [8]  [150/225]  eta: 0:00:18  mi_loss: 4.0039  val_loss: 4.0039  accMI: 0.1587  time: 0.2346  data: 0.0001  max mem: 20623
Val Epoch: [8]  [200/225]  eta: 0:00:05  mi_loss: 4.2286  val_loss: 4.2286  accMI: 0.1440  time: 0.2350  data: 0.0001  max mem: 20623
Val Epoch: [8]  [224/225]  eta: 0:00:00  mi_loss: 4.3218  val_loss: 4.3218  accMI: 0.1534  time: 0.2331  data: 0.0001  max mem: 20623
Val Epoch: [8] Total time: 0:00:54 (0.2429 s / it)
epoch:8, iter:18206, 2022,  train_loss: 4.160071849822998, valid_loss: 4.1013419087727865, idiv_loss:4.1013419087727865, acc:0.15487190826071634
Averaged stats: lr: 0.0000  mi_loss: 4.0827  train_loss: 4.0827
epoch 8 4.160071849822998
Train Epoch: [9]  [   0/2023]  eta: 1:43:47  lr: 0.000010  mi_loss: 4.0407  train_loss: 4.0407  time: 3.0785  data: 0.9997  max mem: 20623
Train Epoch: [9]  [  50/2023]  eta: 0:59:36  lr: 0.000010  mi_loss: 4.2452  train_loss: 4.2452  time: 1.7849  data: 0.0001  max mem: 20623
Train Epoch: [9]  [ 100/2023]  eta: 0:57:38  lr: 0.000010  mi_loss: 3.8303  train_loss: 3.8303  time: 1.7838  data: 0.0001  max mem: 20623
Train Epoch: [9]  [ 150/2023]  eta: 0:56:01  lr: 0.000010  mi_loss: 3.8441  train_loss: 3.8441  time: 1.7843  data: 0.0001  max mem: 20623
Train Epoch: [9]  [ 200/2023]  eta: 0:54:25  lr: 0.000010  mi_loss: 4.4858  train_loss: 4.4858  time: 1.7801  data: 0.0001  max mem: 20623
Train Epoch: [9]  [ 250/2023]  eta: 0:52:53  lr: 0.000010  mi_loss: 3.9232  train_loss: 3.9232  time: 1.7854  data: 0.0001  max mem: 20623
Train Epoch: [9]  [ 300/2023]  eta: 0:51:22  lr: 0.000010  mi_loss: 4.1634  train_loss: 4.1634  time: 1.7853  data: 0.0001  max mem: 20623
Train Epoch: [9]  [ 350/2023]  eta: 0:49:52  lr: 0.000010  mi_loss: 3.9163  train_loss: 3.9163  time: 1.7835  data: 0.0001  max mem: 20623
Train Epoch: [9]  [ 400/2023]  eta: 0:48:22  lr: 0.000010  mi_loss: 4.0429  train_loss: 4.0429  time: 1.7901  data: 0.0001  max mem: 20623
Train Epoch: [9]  [ 450/2023]  eta: 0:46:52  lr: 0.000010  mi_loss: 3.9989  train_loss: 3.9989  time: 1.7824  data: 0.0001  max mem: 20623
Train Epoch: [9]  [ 500/2023]  eta: 0:45:22  lr: 0.000010  mi_loss: 4.0609  train_loss: 4.0609  time: 1.7827  data: 0.0001  max mem: 20623
Train Epoch: [9]  [ 550/2023]  eta: 0:43:52  lr: 0.000010  mi_loss: 4.4188  train_loss: 4.4188  time: 1.7848  data: 0.0001  max mem: 20623
Train Epoch: [9]  [ 600/2023]  eta: 0:42:23  lr: 0.000010  mi_loss: 3.8689  train_loss: 3.8689  time: 1.7882  data: 0.0001  max mem: 20623
Train Epoch: [9]  [ 650/2023]  eta: 0:40:53  lr: 0.000010  mi_loss: 4.1913  train_loss: 4.1913  time: 1.7826  data: 0.0001  max mem: 20623
Train Epoch: [9]  [ 700/2023]  eta: 0:39:24  lr: 0.000010  mi_loss: 4.1434  train_loss: 4.1434  time: 1.7854  data: 0.0001  max mem: 20623
Train Epoch: [9]  [ 750/2023]  eta: 0:37:54  lr: 0.000010  mi_loss: 4.1958  train_loss: 4.1958  time: 1.7837  data: 0.0001  max mem: 20623
Train Epoch: [9]  [ 800/2023]  eta: 0:36:24  lr: 0.000010  mi_loss: 4.1376  train_loss: 4.1376  time: 1.7780  data: 0.0001  max mem: 20623
Train Epoch: [9]  [ 850/2023]  eta: 0:34:55  lr: 0.000010  mi_loss: 4.1556  train_loss: 4.1556  time: 1.7838  data: 0.0001  max mem: 20623
Train Epoch: [9]  [ 900/2023]  eta: 0:33:25  lr: 0.000010  mi_loss: 3.8897  train_loss: 3.8897  time: 1.7825  data: 0.0001  max mem: 20623
Train Epoch: [9]  [ 950/2023]  eta: 0:31:56  lr: 0.000010  mi_loss: 4.1647  train_loss: 4.1647  time: 1.7836  data: 0.0001  max mem: 20623
Train Epoch: [9]  [1000/2023]  eta: 0:30:26  lr: 0.000010  mi_loss: 4.1889  train_loss: 4.1889  time: 1.7820  data: 0.0001  max mem: 20623
Train Epoch: [9]  [1050/2023]  eta: 0:28:57  lr: 0.000010  mi_loss: 4.1603  train_loss: 4.1603  time: 1.7831  data: 0.0001  max mem: 20623
Train Epoch: [9]  [1100/2023]  eta: 0:27:28  lr: 0.000010  mi_loss: 3.8487  train_loss: 3.8487  time: 1.7850  data: 0.0001  max mem: 20623
Train Epoch: [9]  [1150/2023]  eta: 0:25:58  lr: 0.000010  mi_loss: 3.9658  train_loss: 3.9658  time: 1.7879  data: 0.0001  max mem: 20623
Train Epoch: [9]  [1200/2023]  eta: 0:24:29  lr: 0.000010  mi_loss: 3.7671  train_loss: 3.7671  time: 1.7829  data: 0.0001  max mem: 20623
Train Epoch: [9]  [1250/2023]  eta: 0:23:00  lr: 0.000010  mi_loss: 4.1645  train_loss: 4.1645  time: 1.7883  data: 0.0002  max mem: 20623
Train Epoch: [9]  [1300/2023]  eta: 0:21:30  lr: 0.000010  mi_loss: 3.8650  train_loss: 3.8650  time: 1.7838  data: 0.0002  max mem: 20623
Train Epoch: [9]  [1350/2023]  eta: 0:20:01  lr: 0.000010  mi_loss: 3.8742  train_loss: 3.8742  time: 1.7873  data: 0.0002  max mem: 20623
Train Epoch: [9]  [1400/2023]  eta: 0:18:32  lr: 0.000010  mi_loss: 3.9766  train_loss: 3.9766  time: 1.7852  data: 0.0001  max mem: 20623
Train Epoch: [9]  [1450/2023]  eta: 0:17:03  lr: 0.000010  mi_loss: 3.9426  train_loss: 3.9426  time: 1.7843  data: 0.0001  max mem: 20623
Train Epoch: [9]  [1500/2023]  eta: 0:15:33  lr: 0.000010  mi_loss: 3.8327  train_loss: 3.8327  time: 1.7850  data: 0.0001  max mem: 20623
Train Epoch: [9]  [1550/2023]  eta: 0:14:04  lr: 0.000010  mi_loss: 4.2703  train_loss: 4.2703  time: 1.7844  data: 0.0001  max mem: 20623
Train Epoch: [9]  [1600/2023]  eta: 0:12:35  lr: 0.000010  mi_loss: 3.9909  train_loss: 3.9909  time: 1.7823  data: 0.0001  max mem: 20623
Train Epoch: [9]  [1650/2023]  eta: 0:11:05  lr: 0.000010  mi_loss: 3.8652  train_loss: 3.8652  time: 1.7864  data: 0.0001  max mem: 20623
Train Epoch: [9]  [1700/2023]  eta: 0:09:36  lr: 0.000010  mi_loss: 3.6265  train_loss: 3.6265  time: 1.7802  data: 0.0001  max mem: 20623
Train Epoch: [9]  [1750/2023]  eta: 0:08:07  lr: 0.000010  mi_loss: 4.0262  train_loss: 4.0262  time: 1.7861  data: 0.0001  max mem: 20623
Train Epoch: [9]  [1800/2023]  eta: 0:06:38  lr: 0.000010  mi_loss: 3.9451  train_loss: 3.9451  time: 1.7799  data: 0.0001  max mem: 20623
Train Epoch: [9]  [1850/2023]  eta: 0:05:08  lr: 0.000010  mi_loss: 4.1068  train_loss: 4.1068  time: 1.7819  data: 0.0001  max mem: 20623
Train Epoch: [9]  [1900/2023]  eta: 0:03:39  lr: 0.000010  mi_loss: 3.8624  train_loss: 3.8624  time: 1.7836  data: 0.0001  max mem: 20623
Train Epoch: [9]  [1950/2023]  eta: 0:02:10  lr: 0.000010  mi_loss: 3.9071  train_loss: 3.9071  time: 1.7846  data: 0.0001  max mem: 20623
Train Epoch: [9]  [2000/2023]  eta: 0:00:41  lr: 0.000010  mi_loss: 4.0143  train_loss: 4.0143  time: 1.7847  data: 0.0001  max mem: 20623
Train Epoch: [9]  [2022/2023]  eta: 0:00:01  lr: 0.000010  mi_loss: 4.0799  train_loss: 4.0799  time: 1.7835  data: 0.0007  max mem: 20623
Train Epoch: [9] Total time: 1:00:12 (1.7859 s / it)
Val Epoch: [9]  [  0/225]  eta: 0:04:16  mi_loss: 3.9844  val_loss: 3.9844  accMI: 0.1566  time: 1.1422  data: 0.9050  max mem: 20623
Val Epoch: [9]  [ 50/225]  eta: 0:00:44  mi_loss: 4.1563  val_loss: 4.1563  accMI: 0.1197  time: 0.2342  data: 0.0002  max mem: 20623
Val Epoch: [9]  [100/225]  eta: 0:00:30  mi_loss: 4.2790  val_loss: 4.2790  accMI: 0.1330  time: 0.2347  data: 0.0002  max mem: 20623
Val Epoch: [9]  [150/225]  eta: 0:00:18  mi_loss: 3.9514  val_loss: 3.9514  accMI: 0.1654  time: 0.2346  data: 0.0002  max mem: 20623
Val Epoch: [9]  [200/225]  eta: 0:00:05  mi_loss: 4.2580  val_loss: 4.2580  accMI: 0.1417  time: 0.2354  data: 0.0002  max mem: 20623
Val Epoch: [9]  [224/225]  eta: 0:00:00  mi_loss: 4.2806  val_loss: 4.2806  accMI: 0.1588  time: 0.2330  data: 0.0001  max mem: 20623
Val Epoch: [9] Total time: 0:00:54 (0.2437 s / it)
epoch:9, iter:20229, 2022,  train_loss: 4.07985258102417, valid_loss: 4.073101050059001, idiv_loss:4.073101050059001, acc:0.15704492800765568
Averaged stats: lr: 0.0000  mi_loss: 4.0449  train_loss: 4.0449
./src/pretrain_mim.py:79: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig = plt.figure()
epoch 9 4.07985258102417
Train Epoch: [10]  [   0/2023]  eta: 1:41:59  lr: 0.000010  mi_loss: 3.8258  train_loss: 3.8258  time: 3.0251  data: 1.0426  max mem: 20623
Train Epoch: [10]  [  50/2023]  eta: 0:59:39  lr: 0.000010  mi_loss: 4.0531  train_loss: 4.0531  time: 1.7838  data: 0.0003  max mem: 20623
Train Epoch: [10]  [ 100/2023]  eta: 0:57:42  lr: 0.000010  mi_loss: 4.0245  train_loss: 4.0245  time: 1.7868  data: 0.0003  max mem: 20623
Train Epoch: [10]  [ 150/2023]  eta: 0:56:03  lr: 0.000010  mi_loss: 3.9815  train_loss: 3.9815  time: 1.7823  data: 0.0003  max mem: 20623
Train Epoch: [10]  [ 200/2023]  eta: 0:54:26  lr: 0.000010  mi_loss: 3.5424  train_loss: 3.5424  time: 1.7866  data: 0.0003  max mem: 20623
Train Epoch: [10]  [ 250/2023]  eta: 0:52:54  lr: 0.000010  mi_loss: 3.9307  train_loss: 3.9307  time: 1.7841  data: 0.0003  max mem: 20623
Train Epoch: [10]  [ 300/2023]  eta: 0:51:24  lr: 0.000010  mi_loss: 3.9525  train_loss: 3.9525  time: 1.7847  data: 0.0001  max mem: 20623
Train Epoch: [10]  [ 350/2023]  eta: 0:49:53  lr: 0.000010  mi_loss: 3.6679  train_loss: 3.6679  time: 1.7887  data: 0.0001  max mem: 20623
Train Epoch: [10]  [ 400/2023]  eta: 0:48:24  lr: 0.000010  mi_loss: 3.9565  train_loss: 3.9565  time: 1.7895  data: 0.0002  max mem: 20623
Train Epoch: [10]  [ 450/2023]  eta: 0:46:54  lr: 0.000010  mi_loss: 3.8310  train_loss: 3.8310  time: 1.7862  data: 0.0001  max mem: 20623
Train Epoch: [10]  [ 500/2023]  eta: 0:45:24  lr: 0.000010  mi_loss: 4.1745  train_loss: 4.1745  time: 1.7870  data: 0.0001  max mem: 20623
Train Epoch: [10]  [ 550/2023]  eta: 0:43:55  lr: 0.000010  mi_loss: 4.0196  train_loss: 4.0196  time: 1.7904  data: 0.0001  max mem: 20623
Train Epoch: [10]  [ 600/2023]  eta: 0:42:25  lr: 0.000010  mi_loss: 3.9428  train_loss: 3.9428  time: 1.7884  data: 0.0001  max mem: 20623
Train Epoch: [10]  [ 650/2023]  eta: 0:40:56  lr: 0.000010  mi_loss: 4.1253  train_loss: 4.1253  time: 1.7879  data: 0.0002  max mem: 20623
Train Epoch: [10]  [ 700/2023]  eta: 0:39:26  lr: 0.000010  mi_loss: 3.9855  train_loss: 3.9855  time: 1.7884  data: 0.0002  max mem: 20623
Train Epoch: [10]  [ 750/2023]  eta: 0:37:57  lr: 0.000010  mi_loss: 3.8801  train_loss: 3.8801  time: 1.7862  data: 0.0001  max mem: 20623
Train Epoch: [10]  [ 800/2023]  eta: 0:36:27  lr: 0.000010  mi_loss: 4.0827  train_loss: 4.0827  time: 1.7887  data: 0.0001  max mem: 20623
Train Epoch: [10]  [ 850/2023]  eta: 0:34:58  lr: 0.000010  mi_loss: 3.8769  train_loss: 3.8769  time: 1.7893  data: 0.0001  max mem: 20623
Train Epoch: [10]  [ 900/2023]  eta: 0:33:28  lr: 0.000010  mi_loss: 4.1690  train_loss: 4.1690  time: 1.7846  data: 0.0001  max mem: 20623
Train Epoch: [10]  [ 950/2023]  eta: 0:31:59  lr: 0.000010  mi_loss: 3.8121  train_loss: 3.8121  time: 1.7880  data: 0.0001  max mem: 20623
Train Epoch: [10]  [1000/2023]  eta: 0:30:29  lr: 0.000010  mi_loss: 4.2851  train_loss: 4.2851  time: 1.7861  data: 0.0001  max mem: 20623
Train Epoch: [10]  [1050/2023]  eta: 0:28:59  lr: 0.000010  mi_loss: 4.4400  train_loss: 4.4400  time: 1.7852  data: 0.0001  max mem: 20623
Train Epoch: [10]  [1100/2023]  eta: 0:27:30  lr: 0.000010  mi_loss: 4.3055  train_loss: 4.3055  time: 1.7861  data: 0.0001  max mem: 20623
Train Epoch: [10]  [1150/2023]  eta: 0:26:00  lr: 0.000010  mi_loss: 4.1625  train_loss: 4.1625  time: 1.7906  data: 0.0002  max mem: 20623
Train Epoch: [10]  [1200/2023]  eta: 0:24:31  lr: 0.000010  mi_loss: 4.0719  train_loss: 4.0719  time: 1.7876  data: 0.0001  max mem: 20623
Train Epoch: [10]  [1250/2023]  eta: 0:23:02  lr: 0.000010  mi_loss: 4.3838  train_loss: 4.3838  time: 1.7881  data: 0.0001  max mem: 20623
Train Epoch: [10]  [1300/2023]  eta: 0:21:32  lr: 0.000010  mi_loss: 3.9209  train_loss: 3.9209  time: 1.7835  data: 0.0001  max mem: 20623
Train Epoch: [10]  [1350/2023]  eta: 0:20:03  lr: 0.000010  mi_loss: 4.0924  train_loss: 4.0924  time: 1.7834  data: 0.0001  max mem: 20623
Train Epoch: [10]  [1400/2023]  eta: 0:18:33  lr: 0.000010  mi_loss: 3.9932  train_loss: 3.9932  time: 1.7896  data: 0.0001  max mem: 20623
Train Epoch: [10]  [1450/2023]  eta: 0:17:04  lr: 0.000010  mi_loss: 4.0153  train_loss: 4.0153  time: 1.7838  data: 0.0001  max mem: 20623
Train Epoch: [10]  [1500/2023]  eta: 0:15:34  lr: 0.000010  mi_loss: 4.1172  train_loss: 4.1172  time: 1.7875  data: 0.0001  max mem: 20623
Train Epoch: [10]  [1550/2023]  eta: 0:14:05  lr: 0.000010  mi_loss: 4.0385  train_loss: 4.0385  time: 1.7807  data: 0.0001  max mem: 20623
Train Epoch: [10]  [1600/2023]  eta: 0:12:36  lr: 0.000010  mi_loss: 4.0631  train_loss: 4.0631  time: 1.7846  data: 0.0001  max mem: 20623
Train Epoch: [10]  [1650/2023]  eta: 0:11:06  lr: 0.000010  mi_loss: 4.0684  train_loss: 4.0684  time: 1.7847  data: 0.0001  max mem: 20623
Train Epoch: [10]  [1700/2023]  eta: 0:09:37  lr: 0.000010  mi_loss: 4.1504  train_loss: 4.1504  time: 1.7862  data: 0.0001  max mem: 20623
Train Epoch: [10]  [1750/2023]  eta: 0:08:07  lr: 0.000010  mi_loss: 4.1573  train_loss: 4.1573  time: 1.7807  data: 0.0001  max mem: 20623
Train Epoch: [10]  [1800/2023]  eta: 0:06:38  lr: 0.000010  mi_loss: 4.1565  train_loss: 4.1565  time: 1.7813  data: 0.0001  max mem: 20623
Train Epoch: [10]  [1850/2023]  eta: 0:05:09  lr: 0.000010  mi_loss: 4.1552  train_loss: 4.1552  time: 1.7865  data: 0.0001  max mem: 20623
Train Epoch: [10]  [1900/2023]  eta: 0:03:39  lr: 0.000010  mi_loss: 3.8116  train_loss: 3.8116  time: 1.7867  data: 0.0001  max mem: 20623
Train Epoch: [10]  [1950/2023]  eta: 0:02:10  lr: 0.000010  mi_loss: 4.2092  train_loss: 4.2092  time: 1.7844  data: 0.0001  max mem: 20623
Train Epoch: [10]  [2000/2023]  eta: 0:00:41  lr: 0.000010  mi_loss: 4.1420  train_loss: 4.1420  time: 1.7846  data: 0.0001  max mem: 20623
Train Epoch: [10]  [2022/2023]  eta: 0:00:01  lr: 0.000010  mi_loss: 4.4110  train_loss: 4.4110  time: 1.7839  data: 0.0007  max mem: 20623
Train Epoch: [10] Total time: 1:00:16 (1.7874 s / it)
Val Epoch: [10]  [  0/225]  eta: 0:04:46  mi_loss: 3.9114  val_loss: 3.9114  accMI: 0.1655  time: 1.2720  data: 1.0340  max mem: 20623
Val Epoch: [10]  [ 50/225]  eta: 0:00:44  mi_loss: 4.1378  val_loss: 4.1378  accMI: 0.1242  time: 0.2341  data: 0.0001  max mem: 20623
Val Epoch: [10]  [100/225]  eta: 0:00:30  mi_loss: 4.2898  val_loss: 4.2898  accMI: 0.1385  time: 0.2343  data: 0.0001  max mem: 20623
Val Epoch: [10]  [150/225]  eta: 0:00:18  mi_loss: 3.9411  val_loss: 3.9411  accMI: 0.1687  time: 0.2345  data: 0.0001  max mem: 20623
Val Epoch: [10]  [200/225]  eta: 0:00:05  mi_loss: 4.2092  val_loss: 4.2092  accMI: 0.1417  time: 0.2348  data: 0.0001  max mem: 20623
Val Epoch: [10]  [224/225]  eta: 0:00:00  mi_loss: 4.2721  val_loss: 4.2721  accMI: 0.1575  time: 0.2333  data: 0.0002  max mem: 20623
Val Epoch: [10] Total time: 0:00:54 (0.2430 s / it)
epoch:10, iter:22252, 2022,  train_loss: 4.4109673500061035, valid_loss: 4.053261897828844, idiv_loss:4.053261897828844, acc:0.15770556569099425
Averaged stats: lr: 0.0000  mi_loss: 4.0117  train_loss: 4.0117
./src/pretrain_mim.py:58: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig = plt.figure()
epoch 10 4.4109673500061035
Train Epoch: [11]  [   0/2023]  eta: 1:40:11  lr: 0.000010  mi_loss: 3.6663  train_loss: 3.6663  time: 2.9717  data: 1.0714  max mem: 20623
Train Epoch: [11]  [  50/2023]  eta: 0:59:26  lr: 0.000010  mi_loss: 3.9715  train_loss: 3.9715  time: 1.7840  data: 0.0001  max mem: 20623
Train Epoch: [11]  [ 100/2023]  eta: 0:57:31  lr: 0.000010  mi_loss: 4.1264  train_loss: 4.1264  time: 1.7810  data: 0.0001  max mem: 20623
Train Epoch: [11]  [ 150/2023]  eta: 0:55:56  lr: 0.000010  mi_loss: 3.9253  train_loss: 3.9253  time: 1.7874  data: 0.0001  max mem: 20623
Train Epoch: [11]  [ 200/2023]  eta: 0:54:23  lr: 0.000010  mi_loss: 3.5642  train_loss: 3.5642  time: 1.7821  data: 0.0002  max mem: 20623
Train Epoch: [11]  [ 250/2023]  eta: 0:52:51  lr: 0.000010  mi_loss: 4.0646  train_loss: 4.0646  time: 1.7848  data: 0.0001  max mem: 20623
Train Epoch: [11]  [ 300/2023]  eta: 0:51:21  lr: 0.000010  mi_loss: 4.0716  train_loss: 4.0716  time: 1.7853  data: 0.0002  max mem: 20623
Train Epoch: [11]  [ 350/2023]  eta: 0:49:51  lr: 0.000010  mi_loss: 3.7665  train_loss: 3.7665  time: 1.7865  data: 0.0001  max mem: 20623
Train Epoch: [11]  [ 400/2023]  eta: 0:48:21  lr: 0.000010  mi_loss: 4.1744  train_loss: 4.1744  time: 1.7856  data: 0.0001  max mem: 20623
Train Epoch: [11]  [ 450/2023]  eta: 0:46:51  lr: 0.000010  mi_loss: 3.8451  train_loss: 3.8451  time: 1.7856  data: 0.0001  max mem: 20623
Train Epoch: [11]  [ 500/2023]  eta: 0:45:21  lr: 0.000010  mi_loss: 4.0771  train_loss: 4.0771  time: 1.7832  data: 0.0002  max mem: 20623
Train Epoch: [11]  [ 550/2023]  eta: 0:43:52  lr: 0.000010  mi_loss: 4.0750  train_loss: 4.0750  time: 1.7880  data: 0.0002  max mem: 20623
Train Epoch: [11]  [ 600/2023]  eta: 0:42:23  lr: 0.000010  mi_loss: 3.8715  train_loss: 3.8715  time: 1.7865  data: 0.0003  max mem: 20623
Train Epoch: [11]  [ 650/2023]  eta: 0:40:53  lr: 0.000010  mi_loss: 3.4355  train_loss: 3.4355  time: 1.7861  data: 0.0002  max mem: 20623
Train Epoch: [11]  [ 700/2023]  eta: 0:39:24  lr: 0.000010  mi_loss: 4.1194  train_loss: 4.1194  time: 1.7855  data: 0.0002  max mem: 20623
Train Epoch: [11]  [ 750/2023]  eta: 0:37:54  lr: 0.000010  mi_loss: 4.0102  train_loss: 4.0102  time: 1.7845  data: 0.0002  max mem: 20623
Train Epoch: [11]  [ 800/2023]  eta: 0:36:25  lr: 0.000010  mi_loss: 3.8587  train_loss: 3.8587  time: 1.7860  data: 0.0002  max mem: 20623
Train Epoch: [11]  [ 850/2023]  eta: 0:34:55  lr: 0.000010  mi_loss: 3.9418  train_loss: 3.9418  time: 1.7872  data: 0.0002  max mem: 20623
Train Epoch: [11]  [ 900/2023]  eta: 0:33:26  lr: 0.000010  mi_loss: 3.6194  train_loss: 3.6194  time: 1.7859  data: 0.0002  max mem: 20623
Train Epoch: [11]  [ 950/2023]  eta: 0:31:57  lr: 0.000010  mi_loss: 4.2498  train_loss: 4.2498  time: 1.7884  data: 0.0002  max mem: 20623
Train Epoch: [11]  [1000/2023]  eta: 0:30:27  lr: 0.000010  mi_loss: 3.5282  train_loss: 3.5282  time: 1.7874  data: 0.0002  max mem: 20623
Train Epoch: [11]  [1050/2023]  eta: 0:28:58  lr: 0.000010  mi_loss: 3.8153  train_loss: 3.8153  time: 1.7832  data: 0.0001  max mem: 20623
Train Epoch: [11]  [1100/2023]  eta: 0:27:28  lr: 0.000010  mi_loss: 3.8150  train_loss: 3.8150  time: 1.7859  data: 0.0001  max mem: 20623
Train Epoch: [11]  [1150/2023]  eta: 0:25:59  lr: 0.000010  mi_loss: 3.9032  train_loss: 3.9032  time: 1.7867  data: 0.0001  max mem: 20623
Train Epoch: [11]  [1200/2023]  eta: 0:24:30  lr: 0.000010  mi_loss: 4.0469  train_loss: 4.0469  time: 1.7871  data: 0.0001  max mem: 20623
Train Epoch: [11]  [1250/2023]  eta: 0:23:00  lr: 0.000010  mi_loss: 4.1888  train_loss: 4.1888  time: 1.7856  data: 0.0003  max mem: 20623
Train Epoch: [11]  [1300/2023]  eta: 0:21:31  lr: 0.000010  mi_loss: 3.6894  train_loss: 3.6894  time: 1.7885  data: 0.0001  max mem: 20623
Train Epoch: [11]  [1350/2023]  eta: 0:20:02  lr: 0.000010  mi_loss: 3.8868  train_loss: 3.8868  time: 1.7819  data: 0.0001  max mem: 20623
Train Epoch: [11]  [1400/2023]  eta: 0:18:32  lr: 0.000010  mi_loss: 4.0178  train_loss: 4.0178  time: 1.7855  data: 0.0001  max mem: 20623
Train Epoch: [11]  [1450/2023]  eta: 0:17:03  lr: 0.000010  mi_loss: 3.5586  train_loss: 3.5586  time: 1.7879  data: 0.0001  max mem: 20623
Train Epoch: [11]  [1500/2023]  eta: 0:15:34  lr: 0.000010  mi_loss: 4.0491  train_loss: 4.0491  time: 1.7852  data: 0.0001  max mem: 20623
Train Epoch: [11]  [1550/2023]  eta: 0:14:04  lr: 0.000010  mi_loss: 3.9129  train_loss: 3.9129  time: 1.7868  data: 0.0001  max mem: 20623
Train Epoch: [11]  [1600/2023]  eta: 0:12:35  lr: 0.000010  mi_loss: 3.7470  train_loss: 3.7470  time: 1.7835  data: 0.0001  max mem: 20623
Train Epoch: [11]  [1650/2023]  eta: 0:11:06  lr: 0.000010  mi_loss: 3.9949  train_loss: 3.9949  time: 1.7865  data: 0.0002  max mem: 20623
Train Epoch: [11]  [1700/2023]  eta: 0:09:36  lr: 0.000010  mi_loss: 3.9667  train_loss: 3.9667  time: 1.7887  data: 0.0002  max mem: 20623
Train Epoch: [11]  [1750/2023]  eta: 0:08:07  lr: 0.000010  mi_loss: 4.0545  train_loss: 4.0545  time: 1.7852  data: 0.0002  max mem: 20623
Train Epoch: [11]  [1800/2023]  eta: 0:06:38  lr: 0.000010  mi_loss: 4.3758  train_loss: 4.3758  time: 1.7842  data: 0.0002  max mem: 20623
Train Epoch: [11]  [1850/2023]  eta: 0:05:09  lr: 0.000010  mi_loss: 3.9379  train_loss: 3.9379  time: 1.7866  data: 0.0002  max mem: 20623
Train Epoch: [11]  [1900/2023]  eta: 0:03:39  lr: 0.000010  mi_loss: 4.1582  train_loss: 4.1582  time: 1.7822  data: 0.0002  max mem: 20623
Train Epoch: [11]  [1950/2023]  eta: 0:02:10  lr: 0.000010  mi_loss: 4.1034  train_loss: 4.1034  time: 1.7856  data: 0.0002  max mem: 20623
Train Epoch: [11]  [2000/2023]  eta: 0:00:41  lr: 0.000010  mi_loss: 3.8918  train_loss: 3.8918  time: 1.7826  data: 0.0001  max mem: 20623
Train Epoch: [11]  [2022/2023]  eta: 0:00:01  lr: 0.000010  mi_loss: 3.7340  train_loss: 3.7340  time: 1.7864  data: 0.0021  max mem: 20623
Train Epoch: [11] Total time: 1:00:14 (1.7868 s / it)
Val Epoch: [11]  [  0/225]  eta: 0:04:22  mi_loss: 3.9033  val_loss: 3.9033  accMI: 0.1622  time: 1.1666  data: 0.9296  max mem: 20623
Val Epoch: [11]  [ 50/225]  eta: 0:00:44  mi_loss: 4.1050  val_loss: 4.1050  accMI: 0.1275  time: 0.2345  data: 0.0002  max mem: 20623
Val Epoch: [11]  [100/225]  eta: 0:00:30  mi_loss: 4.2692  val_loss: 4.2692  accMI: 0.1385  time: 0.2346  data: 0.0002  max mem: 20623
Val Epoch: [11]  [150/225]  eta: 0:00:18  mi_loss: 3.9166  val_loss: 3.9166  accMI: 0.1609  time: 0.2348  data: 0.0002  max mem: 20623
Val Epoch: [11]  [200/225]  eta: 0:00:05  mi_loss: 4.1885  val_loss: 4.1885  accMI: 0.1484  time: 0.2348  data: 0.0002  max mem: 20623
Val Epoch: [11]  [224/225]  eta: 0:00:00  mi_loss: 4.2950  val_loss: 4.2950  accMI: 0.1521  time: 0.2331  data: 0.0002  max mem: 20623
Val Epoch: [11] Total time: 0:00:54 (0.2428 s / it)
epoch:11, iter:24275, 2022,  train_loss: 3.734022855758667, valid_loss: 4.033968643612332, idiv_loss:4.033968643612332, acc:0.15979890131288105
Averaged stats: lr: 0.0000  mi_loss: 3.9817  train_loss: 3.9817
epoch 11 3.734022855758667
Train Epoch: [12]  [   0/2023]  eta: 1:46:49  lr: 0.000010  mi_loss: 4.0310  train_loss: 4.0310  time: 3.1682  data: 1.0744  max mem: 20623
Train Epoch: [12]  [  50/2023]  eta: 0:59:43  lr: 0.000010  mi_loss: 3.7633  train_loss: 3.7633  time: 1.7826  data: 0.0001  max mem: 20623
Train Epoch: [12]  [ 100/2023]  eta: 0:57:43  lr: 0.000010  mi_loss: 3.7284  train_loss: 3.7284  time: 1.7858  data: 0.0001  max mem: 20623
Train Epoch: [12]  [ 150/2023]  eta: 0:56:05  lr: 0.000010  mi_loss: 3.5936  train_loss: 3.5936  time: 1.7871  data: 0.0001  max mem: 20623
Train Epoch: [12]  [ 200/2023]  eta: 0:54:30  lr: 0.000010  mi_loss: 4.0129  train_loss: 4.0129  time: 1.7877  data: 0.0001  max mem: 20623
Train Epoch: [12]  [ 250/2023]  eta: 0:52:58  lr: 0.000010  mi_loss: 3.4776  train_loss: 3.4776  time: 1.7863  data: 0.0001  max mem: 20623
Train Epoch: [12]  [ 300/2023]  eta: 0:51:27  lr: 0.000010  mi_loss: 3.9480  train_loss: 3.9480  time: 1.7889  data: 0.0001  max mem: 20623
Train Epoch: [12]  [ 350/2023]  eta: 0:49:56  lr: 0.000010  mi_loss: 3.9325  train_loss: 3.9325  time: 1.7891  data: 0.0001  max mem: 20623
Train Epoch: [12]  [ 400/2023]  eta: 0:48:26  lr: 0.000010  mi_loss: 4.2140  train_loss: 4.2140  time: 1.7861  data: 0.0001  max mem: 20623
Train Epoch: [12]  [ 450/2023]  eta: 0:46:56  lr: 0.000010  mi_loss: 3.9019  train_loss: 3.9019  time: 1.7902  data: 0.0001  max mem: 20623
Train Epoch: [12]  [ 500/2023]  eta: 0:45:26  lr: 0.000010  mi_loss: 3.9015  train_loss: 3.9015  time: 1.7881  data: 0.0001  max mem: 20623
Train Epoch: [12]  [ 550/2023]  eta: 0:43:56  lr: 0.000010  mi_loss: 4.2685  train_loss: 4.2685  time: 1.7858  data: 0.0001  max mem: 20623
Train Epoch: [12]  [ 600/2023]  eta: 0:42:26  lr: 0.000010  mi_loss: 3.9837  train_loss: 3.9837  time: 1.7891  data: 0.0001  max mem: 20623
Train Epoch: [12]  [ 650/2023]  eta: 0:40:56  lr: 0.000010  mi_loss: 3.8779  train_loss: 3.8779  time: 1.7875  data: 0.0001  max mem: 20623
Train Epoch: [12]  [ 700/2023]  eta: 0:39:27  lr: 0.000010  mi_loss: 4.1019  train_loss: 4.1019  time: 1.7856  data: 0.0001  max mem: 20623
Train Epoch: [12]  [ 750/2023]  eta: 0:37:57  lr: 0.000010  mi_loss: 3.7750  train_loss: 3.7750  time: 1.7846  data: 0.0001  max mem: 20623
Train Epoch: [12]  [ 800/2023]  eta: 0:36:27  lr: 0.000010  mi_loss: 4.0189  train_loss: 4.0189  time: 1.7867  data: 0.0002  max mem: 20623
Train Epoch: [12]  [ 850/2023]  eta: 0:34:58  lr: 0.000010  mi_loss: 3.8055  train_loss: 3.8055  time: 1.7879  data: 0.0001  max mem: 20623
Train Epoch: [12]  [ 900/2023]  eta: 0:33:28  lr: 0.000010  mi_loss: 4.3034  train_loss: 4.3034  time: 1.7846  data: 0.0002  max mem: 20623
Train Epoch: [12]  [ 950/2023]  eta: 0:31:58  lr: 0.000010  mi_loss: 3.8415  train_loss: 3.8415  time: 1.7847  data: 0.0001  max mem: 20623
Train Epoch: [12]  [1000/2023]  eta: 0:30:29  lr: 0.000010  mi_loss: 4.1321  train_loss: 4.1321  time: 1.7850  data: 0.0001  max mem: 20623
Train Epoch: [12]  [1050/2023]  eta: 0:28:59  lr: 0.000010  mi_loss: 3.9068  train_loss: 3.9068  time: 1.7845  data: 0.0001  max mem: 20623
Train Epoch: [12]  [1100/2023]  eta: 0:27:30  lr: 0.000010  mi_loss: 4.1409  train_loss: 4.1409  time: 1.7859  data: 0.0001  max mem: 20623
Train Epoch: [12]  [1150/2023]  eta: 0:26:00  lr: 0.000010  mi_loss: 3.9046  train_loss: 3.9046  time: 1.7872  data: 0.0001  max mem: 20623
Train Epoch: [12]  [1200/2023]  eta: 0:24:31  lr: 0.000010  mi_loss: 3.9567  train_loss: 3.9567  time: 1.7847  data: 0.0001  max mem: 20623
Train Epoch: [12]  [1250/2023]  eta: 0:23:01  lr: 0.000010  mi_loss: 3.9482  train_loss: 3.9482  time: 1.7863  data: 0.0001  max mem: 20623
Train Epoch: [12]  [1300/2023]  eta: 0:21:32  lr: 0.000010  mi_loss: 3.7087  train_loss: 3.7087  time: 1.7795  data: 0.0001  max mem: 20623
Train Epoch: [12]  [1350/2023]  eta: 0:20:02  lr: 0.000010  mi_loss: 4.0929  train_loss: 4.0929  time: 1.7861  data: 0.0001  max mem: 20623
Train Epoch: [12]  [1400/2023]  eta: 0:18:33  lr: 0.000010  mi_loss: 3.7016  train_loss: 3.7016  time: 1.7861  data: 0.0001  max mem: 20623
Train Epoch: [12]  [1450/2023]  eta: 0:17:04  lr: 0.000010  mi_loss: 4.1735  train_loss: 4.1735  time: 1.7898  data: 0.0003  max mem: 20623
Train Epoch: [12]  [1500/2023]  eta: 0:15:34  lr: 0.000010  mi_loss: 4.0554  train_loss: 4.0554  time: 1.7838  data: 0.0003  max mem: 20623
Train Epoch: [12]  [1550/2023]  eta: 0:14:05  lr: 0.000010  mi_loss: 3.9202  train_loss: 3.9202  time: 1.7847  data: 0.0003  max mem: 20623
Train Epoch: [12]  [1600/2023]  eta: 0:12:35  lr: 0.000010  mi_loss: 4.0919  train_loss: 4.0919  time: 1.7838  data: 0.0003  max mem: 20623
Train Epoch: [12]  [1650/2023]  eta: 0:11:06  lr: 0.000010  mi_loss: 4.1285  train_loss: 4.1285  time: 1.7852  data: 0.0003  max mem: 20623
Train Epoch: [12]  [1700/2023]  eta: 0:09:37  lr: 0.000010  mi_loss: 4.1283  train_loss: 4.1283  time: 1.7842  data: 0.0003  max mem: 20623
Train Epoch: [12]  [1750/2023]  eta: 0:08:07  lr: 0.000010  mi_loss: 4.0377  train_loss: 4.0377  time: 1.7836  data: 0.0003  max mem: 20623
Train Epoch: [12]  [1800/2023]  eta: 0:06:38  lr: 0.000010  mi_loss: 4.2285  train_loss: 4.2285  time: 1.7826  data: 0.0002  max mem: 20623
Train Epoch: [12]  [1850/2023]  eta: 0:05:09  lr: 0.000010  mi_loss: 3.8637  train_loss: 3.8637  time: 1.7836  data: 0.0003  max mem: 20623
Train Epoch: [12]  [1900/2023]  eta: 0:03:39  lr: 0.000010  mi_loss: 3.8869  train_loss: 3.8869  time: 1.7839  data: 0.0001  max mem: 20623
Train Epoch: [12]  [1950/2023]  eta: 0:02:10  lr: 0.000010  mi_loss: 4.1358  train_loss: 4.1358  time: 1.7841  data: 0.0001  max mem: 20623
Train Epoch: [12]  [2000/2023]  eta: 0:00:41  lr: 0.000010  mi_loss: 3.8852  train_loss: 3.8852  time: 1.7848  data: 0.0001  max mem: 20623
Train Epoch: [12]  [2022/2023]  eta: 0:00:01  lr: 0.000010  mi_loss: 3.3767  train_loss: 3.3767  time: 1.7865  data: 0.0007  max mem: 20623
Train Epoch: [12] Total time: 1:00:14 (1.7869 s / it)
Val Epoch: [12]  [  0/225]  eta: 0:04:39  mi_loss: 3.8933  val_loss: 3.8933  accMI: 0.1711  time: 1.2424  data: 1.0048  max mem: 20623
Val Epoch: [12]  [ 50/225]  eta: 0:00:44  mi_loss: 4.1249  val_loss: 4.1249  accMI: 0.1253  time: 0.2342  data: 0.0002  max mem: 20623
Val Epoch: [12]  [100/225]  eta: 0:00:30  mi_loss: 4.2674  val_loss: 4.2674  accMI: 0.1453  time: 0.2347  data: 0.0002  max mem: 20623
Val Epoch: [12]  [150/225]  eta: 0:00:18  mi_loss: 3.8995  val_loss: 3.8995  accMI: 0.1598  time: 0.2348  data: 0.0002  max mem: 20623
Val Epoch: [12]  [200/225]  eta: 0:00:05  mi_loss: 4.1739  val_loss: 4.1739  accMI: 0.1440  time: 0.2351  data: 0.0002  max mem: 20623
Val Epoch: [12]  [224/225]  eta: 0:00:00  mi_loss: 4.1978  val_loss: 4.1978  accMI: 0.1548  time: 0.2333  data: 0.0003  max mem: 20623
Val Epoch: [12] Total time: 0:00:55 (0.2448 s / it)
epoch:12, iter:26298, 2022,  train_loss: 3.376671314239502, valid_loss: 4.016635206010607, idiv_loss:4.016635206010607, acc:0.16063745660914316
Averaged stats: lr: 0.0000  mi_loss: 3.9546  train_loss: 3.9546
epoch 12 3.376671314239502
Train Epoch: [13]  [   0/2023]  eta: 1:42:39  lr: 0.000010  mi_loss: 3.8460  train_loss: 3.8460  time: 3.0449  data: 1.0122  max mem: 20623
Train Epoch: [13]  [  50/2023]  eta: 0:59:25  lr: 0.000010  mi_loss: 3.7452  train_loss: 3.7452  time: 1.7817  data: 0.0001  max mem: 20623
Train Epoch: [13]  [ 100/2023]  eta: 0:57:31  lr: 0.000010  mi_loss: 3.9349  train_loss: 3.9349  time: 1.7849  data: 0.0001  max mem: 20623
Train Epoch: [13]  [ 150/2023]  eta: 0:55:55  lr: 0.000010  mi_loss: 4.0305  train_loss: 4.0305  time: 1.7847  data: 0.0001  max mem: 20623
Train Epoch: [13]  [ 200/2023]  eta: 0:54:23  lr: 0.000010  mi_loss: 3.8376  train_loss: 3.8376  time: 1.7846  data: 0.0001  max mem: 20623
Train Epoch: [13]  [ 250/2023]  eta: 0:52:52  lr: 0.000010  mi_loss: 4.0606  train_loss: 4.0606  time: 1.7852  data: 0.0001  max mem: 20623
Train Epoch: [13]  [ 300/2023]  eta: 0:51:21  lr: 0.000010  mi_loss: 4.2473  train_loss: 4.2473  time: 1.7826  data: 0.0001  max mem: 20623
Train Epoch: [13]  [ 350/2023]  eta: 0:49:51  lr: 0.000010  mi_loss: 3.7895  train_loss: 3.7895  time: 1.7858  data: 0.0001  max mem: 20623
Train Epoch: [13]  [ 400/2023]  eta: 0:48:21  lr: 0.000010  mi_loss: 3.8283  train_loss: 3.8283  time: 1.7876  data: 0.0001  max mem: 20623
Train Epoch: [13]  [ 450/2023]  eta: 0:46:51  lr: 0.000010  mi_loss: 4.1989  train_loss: 4.1989  time: 1.7843  data: 0.0002  max mem: 20623
Train Epoch: [13]  [ 500/2023]  eta: 0:45:22  lr: 0.000010  mi_loss: 4.1509  train_loss: 4.1509  time: 1.7822  data: 0.0001  max mem: 20623
Train Epoch: [13]  [ 550/2023]  eta: 0:43:52  lr: 0.000010  mi_loss: 3.8137  train_loss: 3.8137  time: 1.7870  data: 0.0001  max mem: 20623
Train Epoch: [13]  [ 600/2023]  eta: 0:42:23  lr: 0.000010  mi_loss: 4.0702  train_loss: 4.0702  time: 1.7889  data: 0.0001  max mem: 20623
Train Epoch: [13]  [ 650/2023]  eta: 0:40:54  lr: 0.000010  mi_loss: 3.9935  train_loss: 3.9935  time: 1.7880  data: 0.0001  max mem: 20623
Train Epoch: [13]  [ 700/2023]  eta: 0:39:24  lr: 0.000010  mi_loss: 3.9749  train_loss: 3.9749  time: 1.7864  data: 0.0001  max mem: 20623
Train Epoch: [13]  [ 750/2023]  eta: 0:37:55  lr: 0.000010  mi_loss: 3.6408  train_loss: 3.6408  time: 1.7841  data: 0.0001  max mem: 20623
Train Epoch: [13]  [ 800/2023]  eta: 0:36:25  lr: 0.000010  mi_loss: 4.0499  train_loss: 4.0499  time: 1.7834  data: 0.0002  max mem: 20623
Train Epoch: [13]  [ 850/2023]  eta: 0:34:56  lr: 0.000010  mi_loss: 3.9990  train_loss: 3.9990  time: 1.7875  data: 0.0001  max mem: 20623
Train Epoch: [13]  [ 900/2023]  eta: 0:33:26  lr: 0.000010  mi_loss: 4.1472  train_loss: 4.1472  time: 1.7855  data: 0.0001  max mem: 20623
Train Epoch: [13]  [ 950/2023]  eta: 0:31:57  lr: 0.000010  mi_loss: 3.8439  train_loss: 3.8439  time: 1.7882  data: 0.0002  max mem: 20623
Train Epoch: [13]  [1000/2023]  eta: 0:30:27  lr: 0.000010  mi_loss: 3.6834  train_loss: 3.6834  time: 1.7813  data: 0.0001  max mem: 20623
Train Epoch: [13]  [1050/2023]  eta: 0:28:58  lr: 0.000010  mi_loss: 3.8784  train_loss: 3.8784  time: 1.7877  data: 0.0001  max mem: 20623
Train Epoch: [13]  [1100/2023]  eta: 0:27:29  lr: 0.000010  mi_loss: 3.7436  train_loss: 3.7436  time: 1.7844  data: 0.0001  max mem: 20623
Train Epoch: [13]  [1150/2023]  eta: 0:25:59  lr: 0.000010  mi_loss: 4.0465  train_loss: 4.0465  time: 1.7845  data: 0.0001  max mem: 20623
Train Epoch: [13]  [1200/2023]  eta: 0:24:30  lr: 0.000010  mi_loss: 3.4572  train_loss: 3.4572  time: 1.7815  data: 0.0001  max mem: 20623
Train Epoch: [13]  [1250/2023]  eta: 0:23:00  lr: 0.000010  mi_loss: 3.6933  train_loss: 3.6933  time: 1.7850  data: 0.0001  max mem: 20623
Train Epoch: [13]  [1300/2023]  eta: 0:21:31  lr: 0.000010  mi_loss: 3.8277  train_loss: 3.8277  time: 1.7852  data: 0.0001  max mem: 20623
Train Epoch: [13]  [1350/2023]  eta: 0:20:02  lr: 0.000010  mi_loss: 4.0700  train_loss: 4.0700  time: 1.7867  data: 0.0001  max mem: 20623
Train Epoch: [13]  [1400/2023]  eta: 0:18:32  lr: 0.000010  mi_loss: 3.9563  train_loss: 3.9563  time: 1.7843  data: 0.0001  max mem: 20623
Train Epoch: [13]  [1450/2023]  eta: 0:17:03  lr: 0.000010  mi_loss: 4.0023  train_loss: 4.0023  time: 1.7852  data: 0.0001  max mem: 20623
Train Epoch: [13]  [1500/2023]  eta: 0:15:34  lr: 0.000010  mi_loss: 3.7430  train_loss: 3.7430  time: 1.7839  data: 0.0001  max mem: 20623
Train Epoch: [13]  [1550/2023]  eta: 0:14:04  lr: 0.000010  mi_loss: 4.0724  train_loss: 4.0724  time: 1.7873  data: 0.0001  max mem: 20623
Train Epoch: [13]  [1600/2023]  eta: 0:12:35  lr: 0.000010  mi_loss: 4.0626  train_loss: 4.0626  time: 1.7850  data: 0.0001  max mem: 20623
Train Epoch: [13]  [1650/2023]  eta: 0:11:06  lr: 0.000010  mi_loss: 4.3340  train_loss: 4.3340  time: 1.7797  data: 0.0001  max mem: 20623
Train Epoch: [13]  [1700/2023]  eta: 0:09:36  lr: 0.000010  mi_loss: 4.0033  train_loss: 4.0033  time: 1.7842  data: 0.0001  max mem: 20623
Train Epoch: [13]  [1750/2023]  eta: 0:08:07  lr: 0.000010  mi_loss: 3.7026  train_loss: 3.7026  time: 1.7801  data: 0.0001  max mem: 20623
Train Epoch: [13]  [1800/2023]  eta: 0:06:38  lr: 0.000010  mi_loss: 4.1438  train_loss: 4.1438  time: 1.7851  data: 0.0001  max mem: 20623
Train Epoch: [13]  [1850/2023]  eta: 0:05:08  lr: 0.000010  mi_loss: 4.1052  train_loss: 4.1052  time: 1.7810  data: 0.0001  max mem: 20623
Train Epoch: [13]  [1900/2023]  eta: 0:03:39  lr: 0.000010  mi_loss: 3.8920  train_loss: 3.8920  time: 1.7847  data: 0.0001  max mem: 20623
Train Epoch: [13]  [1950/2023]  eta: 0:02:10  lr: 0.000010  mi_loss: 3.6288  train_loss: 3.6288  time: 1.7842  data: 0.0001  max mem: 20623
Train Epoch: [13]  [2000/2023]  eta: 0:00:41  lr: 0.000010  mi_loss: 3.9534  train_loss: 3.9534  time: 1.7868  data: 0.0001  max mem: 20623
Train Epoch: [13]  [2022/2023]  eta: 0:00:01  lr: 0.000010  mi_loss: 3.9207  train_loss: 3.9207  time: 1.7832  data: 0.0006  max mem: 20623
Train Epoch: [13] Total time: 1:00:12 (1.7860 s / it)
Val Epoch: [13]  [  0/225]  eta: 0:04:49  mi_loss: 3.9344  val_loss: 3.9344  accMI: 0.1700  time: 1.2883  data: 1.0517  max mem: 20623
Val Epoch: [13]  [ 50/225]  eta: 0:00:44  mi_loss: 4.0899  val_loss: 4.0899  accMI: 0.1342  time: 0.2340  data: 0.0001  max mem: 20623
Val Epoch: [13]  [100/225]  eta: 0:00:30  mi_loss: 4.2286  val_loss: 4.2286  accMI: 0.1486  time: 0.2344  data: 0.0002  max mem: 20623
Val Epoch: [13]  [150/225]  eta: 0:00:18  mi_loss: 3.8842  val_loss: 3.8842  accMI: 0.1698  time: 0.2347  data: 0.0002  max mem: 20623
Val Epoch: [13]  [200/225]  eta: 0:00:05  mi_loss: 4.1829  val_loss: 4.1829  accMI: 0.1429  time: 0.2351  data: 0.0002  max mem: 20623
Val Epoch: [13]  [224/225]  eta: 0:00:00  mi_loss: 4.2789  val_loss: 4.2789  accMI: 0.1588  time: 0.2332  data: 0.0002  max mem: 20623
Val Epoch: [13] Total time: 0:00:55 (0.2449 s / it)
epoch:13, iter:28321, 2022,  train_loss: 3.920708417892456, valid_loss: 4.005941017998589, idiv_loss:4.005941017998589, acc:0.1621176423297988
Averaged stats: lr: 0.0000  mi_loss: 3.9291  train_loss: 3.9291
epoch 13 3.920708417892456
Train Epoch: [14]  [   0/2023]  eta: 1:44:32  lr: 0.000010  mi_loss: 3.7107  train_loss: 3.7107  time: 3.1008  data: 0.9445  max mem: 20623
Train Epoch: [14]  [  50/2023]  eta: 0:59:42  lr: 0.000010  mi_loss: 3.7030  train_loss: 3.7030  time: 1.7844  data: 0.0001  max mem: 20623
Train Epoch: [14]  [ 100/2023]  eta: 0:57:42  lr: 0.000010  mi_loss: 3.9481  train_loss: 3.9481  time: 1.7873  data: 0.0002  max mem: 20623
Train Epoch: [14]  [ 150/2023]  eta: 0:56:04  lr: 0.000010  mi_loss: 3.8334  train_loss: 3.8334  time: 1.7874  data: 0.0001  max mem: 20623
Train Epoch: [14]  [ 200/2023]  eta: 0:54:31  lr: 0.000010  mi_loss: 3.9973  train_loss: 3.9973  time: 1.7874  data: 0.0001  max mem: 20623
Train Epoch: [14]  [ 250/2023]  eta: 0:52:59  lr: 0.000010  mi_loss: 3.9177  train_loss: 3.9177  time: 1.7882  data: 0.0001  max mem: 20623
Train Epoch: [14]  [ 300/2023]  eta: 0:51:28  lr: 0.000010  mi_loss: 3.8730  train_loss: 3.8730  time: 1.7870  data: 0.0001  max mem: 20623
Train Epoch: [14]  [ 350/2023]  eta: 0:49:57  lr: 0.000010  mi_loss: 3.8164  train_loss: 3.8164  time: 1.7888  data: 0.0001  max mem: 20623
Train Epoch: [14]  [ 400/2023]  eta: 0:48:26  lr: 0.000010  mi_loss: 3.9262  train_loss: 3.9262  time: 1.7860  data: 0.0003  max mem: 20623
Train Epoch: [14]  [ 450/2023]  eta: 0:46:55  lr: 0.000010  mi_loss: 4.0555  train_loss: 4.0555  time: 1.7850  data: 0.0003  max mem: 20623
Train Epoch: [14]  [ 500/2023]  eta: 0:45:26  lr: 0.000010  mi_loss: 3.8802  train_loss: 3.8802  time: 1.7860  data: 0.0002  max mem: 20623
Train Epoch: [14]  [ 550/2023]  eta: 0:43:56  lr: 0.000010  mi_loss: 3.8911  train_loss: 3.8911  time: 1.7858  data: 0.0001  max mem: 20623
Train Epoch: [14]  [ 600/2023]  eta: 0:42:26  lr: 0.000010  mi_loss: 4.0484  train_loss: 4.0484  time: 1.7871  data: 0.0001  max mem: 20623
Train Epoch: [14]  [ 650/2023]  eta: 0:40:56  lr: 0.000010  mi_loss: 4.3064  train_loss: 4.3064  time: 1.7843  data: 0.0001  max mem: 20623
Train Epoch: [14]  [ 700/2023]  eta: 0:39:27  lr: 0.000010  mi_loss: 3.6558  train_loss: 3.6558  time: 1.7903  data: 0.0001  max mem: 20623
Train Epoch: [14]  [ 750/2023]  eta: 0:37:57  lr: 0.000010  mi_loss: 3.9021  train_loss: 3.9021  time: 1.7822  data: 0.0001  max mem: 20623
Train Epoch: [14]  [ 800/2023]  eta: 0:36:27  lr: 0.000010  mi_loss: 4.2552  train_loss: 4.2552  time: 1.7842  data: 0.0001  max mem: 20623
Train Epoch: [14]  [ 850/2023]  eta: 0:34:57  lr: 0.000010  mi_loss: 3.7572  train_loss: 3.7572  time: 1.7842  data: 0.0001  max mem: 20623
Train Epoch: [14]  [ 900/2023]  eta: 0:33:28  lr: 0.000010  mi_loss: 3.9862  train_loss: 3.9862  time: 1.7859  data: 0.0001  max mem: 20623
Train Epoch: [14]  [ 950/2023]  eta: 0:31:58  lr: 0.000010  mi_loss: 4.0625  train_loss: 4.0625  time: 1.7874  data: 0.0001  max mem: 20623
Train Epoch: [14]  [1000/2023]  eta: 0:30:29  lr: 0.000010  mi_loss: 3.8133  train_loss: 3.8133  time: 1.7850  data: 0.0001  max mem: 20623
Train Epoch: [14]  [1050/2023]  eta: 0:28:59  lr: 0.000010  mi_loss: 4.0339  train_loss: 4.0339  time: 1.7819  data: 0.0001  max mem: 20623
Train Epoch: [14]  [1100/2023]  eta: 0:27:29  lr: 0.000010  mi_loss: 3.5648  train_loss: 3.5648  time: 1.7872  data: 0.0001  max mem: 20623
Train Epoch: [14]  [1150/2023]  eta: 0:26:00  lr: 0.000010  mi_loss: 3.7809  train_loss: 3.7809  time: 1.7823  data: 0.0001  max mem: 20623
Train Epoch: [14]  [1200/2023]  eta: 0:24:30  lr: 0.000010  mi_loss: 3.5982  train_loss: 3.5982  time: 1.7844  data: 0.0001  max mem: 20623
Train Epoch: [14]  [1250/2023]  eta: 0:23:01  lr: 0.000010  mi_loss: 4.1245  train_loss: 4.1245  time: 1.7822  data: 0.0001  max mem: 20623
Train Epoch: [14]  [1300/2023]  eta: 0:21:32  lr: 0.000010  mi_loss: 4.2182  train_loss: 4.2182  time: 1.7852  data: 0.0001  max mem: 20623
Train Epoch: [14]  [1350/2023]  eta: 0:20:02  lr: 0.000010  mi_loss: 3.9409  train_loss: 3.9409  time: 1.7819  data: 0.0001  max mem: 20623
Train Epoch: [14]  [1400/2023]  eta: 0:18:33  lr: 0.000010  mi_loss: 4.0806  train_loss: 4.0806  time: 1.7772  data: 0.0001  max mem: 20623
Train Epoch: [14]  [1450/2023]  eta: 0:17:03  lr: 0.000010  mi_loss: 3.8582  train_loss: 3.8582  time: 1.7816  data: 0.0001  max mem: 20623
Train Epoch: [14]  [1500/2023]  eta: 0:15:34  lr: 0.000010  mi_loss: 4.2336  train_loss: 4.2336  time: 1.7805  data: 0.0001  max mem: 20623
Train Epoch: [14]  [1550/2023]  eta: 0:14:04  lr: 0.000010  mi_loss: 3.6089  train_loss: 3.6089  time: 1.7849  data: 0.0001  max mem: 20623
Train Epoch: [14]  [1600/2023]  eta: 0:12:35  lr: 0.000010  mi_loss: 3.7460  train_loss: 3.7460  time: 1.7843  data: 0.0001  max mem: 20623
Train Epoch: [14]  [1650/2023]  eta: 0:11:06  lr: 0.000010  mi_loss: 4.0592  train_loss: 4.0592  time: 1.7841  data: 0.0001  max mem: 20623
Train Epoch: [14]  [1700/2023]  eta: 0:09:36  lr: 0.000010  mi_loss: 3.9614  train_loss: 3.9614  time: 1.7823  data: 0.0003  max mem: 20623
Train Epoch: [14]  [1750/2023]  eta: 0:08:07  lr: 0.000010  mi_loss: 3.6539  train_loss: 3.6539  time: 1.7870  data: 0.0001  max mem: 20623
Train Epoch: [14]  [1800/2023]  eta: 0:06:38  lr: 0.000010  mi_loss: 3.6030  train_loss: 3.6030  time: 1.7836  data: 0.0003  max mem: 20623
Train Epoch: [14]  [1850/2023]  eta: 0:05:08  lr: 0.000010  mi_loss: 3.8011  train_loss: 3.8011  time: 1.7889  data: 0.0001  max mem: 20623
Train Epoch: [14]  [1900/2023]  eta: 0:03:39  lr: 0.000010  mi_loss: 3.5853  train_loss: 3.5853  time: 1.7882  data: 0.0001  max mem: 20623
Train Epoch: [14]  [1950/2023]  eta: 0:02:10  lr: 0.000010  mi_loss: 3.8806  train_loss: 3.8806  time: 1.7863  data: 0.0001  max mem: 20623
Train Epoch: [14]  [2000/2023]  eta: 0:00:41  lr: 0.000010  mi_loss: 3.6356  train_loss: 3.6356  time: 1.7824  data: 0.0001  max mem: 20623
Train Epoch: [14]  [2022/2023]  eta: 0:00:01  lr: 0.000010  mi_loss: 4.1212  train_loss: 4.1212  time: 1.7876  data: 0.0006  max mem: 20623
Train Epoch: [14] Total time: 1:00:14 (1.7865 s / it)
Val Epoch: [14]  [  0/225]  eta: 0:04:43  mi_loss: 3.8878  val_loss: 3.8878  accMI: 0.1644  time: 1.2619  data: 1.0238  max mem: 20623
Val Epoch: [14]  [ 50/225]  eta: 0:00:44  mi_loss: 4.0750  val_loss: 4.0750  accMI: 0.1353  time: 0.2340  data: 0.0001  max mem: 20623
Val Epoch: [14]  [100/225]  eta: 0:00:30  mi_loss: 4.2187  val_loss: 4.2187  accMI: 0.1553  time: 0.2342  data: 0.0001  max mem: 20623
Val Epoch: [14]  [150/225]  eta: 0:00:18  mi_loss: 3.8705  val_loss: 3.8705  accMI: 0.1765  time: 0.2347  data: 0.0001  max mem: 20623
Val Epoch: [14]  [200/225]  eta: 0:00:05  mi_loss: 4.1871  val_loss: 4.1871  accMI: 0.1585  time: 0.2351  data: 0.0001  max mem: 20623
Val Epoch: [14]  [224/225]  eta: 0:00:00  mi_loss: 4.2493  val_loss: 4.2493  accMI: 0.1588  time: 0.2332  data: 0.0002  max mem: 20623
Val Epoch: [14] Total time: 0:00:54 (0.2430 s / it)
epoch:14, iter:30344, 2022,  train_loss: 4.1212077140808105, valid_loss: 3.987672103246053, idiv_loss:3.987672103246053, acc:0.16351991775963043
Averaged stats: lr: 0.0000  mi_loss: 3.9053  train_loss: 3.9053
epoch 14 4.1212077140808105
Train Epoch: [15]  [   0/2023]  eta: 1:44:39  lr: 0.000010  mi_loss: 4.0456  train_loss: 4.0456  time: 3.1039  data: 1.0751  max mem: 20623
Train Epoch: [15]  [  50/2023]  eta: 0:59:30  lr: 0.000010  mi_loss: 3.7153  train_loss: 3.7153  time: 1.7827  data: 0.0001  max mem: 20623
Train Epoch: [15]  [ 100/2023]  eta: 0:57:36  lr: 0.000010  mi_loss: 4.0221  train_loss: 4.0221  time: 1.7843  data: 0.0001  max mem: 20623
Train Epoch: [15]  [ 150/2023]  eta: 0:56:00  lr: 0.000010  mi_loss: 4.3804  train_loss: 4.3804  time: 1.7884  data: 0.0001  max mem: 20623
Train Epoch: [15]  [ 200/2023]  eta: 0:54:26  lr: 0.000010  mi_loss: 4.0060  train_loss: 4.0060  time: 1.7838  data: 0.0001  max mem: 20623
Train Epoch: [15]  [ 250/2023]  eta: 0:52:55  lr: 0.000010  mi_loss: 3.9063  train_loss: 3.9063  time: 1.7839  data: 0.0001  max mem: 20623
Train Epoch: [15]  [ 300/2023]  eta: 0:51:24  lr: 0.000010  mi_loss: 3.5115  train_loss: 3.5115  time: 1.7867  data: 0.0002  max mem: 20623
Train Epoch: [15]  [ 350/2023]  eta: 0:49:53  lr: 0.000010  mi_loss: 3.8466  train_loss: 3.8466  time: 1.7848  data: 0.0001  max mem: 20623
Train Epoch: [15]  [ 400/2023]  eta: 0:48:23  lr: 0.000010  mi_loss: 3.3564  train_loss: 3.3564  time: 1.7877  data: 0.0002  max mem: 20623
Train Epoch: [15]  [ 450/2023]  eta: 0:46:53  lr: 0.000010  mi_loss: 3.3908  train_loss: 3.3908  time: 1.7848  data: 0.0002  max mem: 20623
Train Epoch: [15]  [ 500/2023]  eta: 0:45:24  lr: 0.000010  mi_loss: 3.8028  train_loss: 3.8028  time: 1.7898  data: 0.0001  max mem: 20623
Train Epoch: [15]  [ 550/2023]  eta: 0:43:54  lr: 0.000010  mi_loss: 3.9626  train_loss: 3.9626  time: 1.7873  data: 0.0002  max mem: 20623
Train Epoch: [15]  [ 600/2023]  eta: 0:42:24  lr: 0.000010  mi_loss: 4.2312  train_loss: 4.2312  time: 1.7876  data: 0.0002  max mem: 20623
Train Epoch: [15]  [ 650/2023]  eta: 0:40:54  lr: 0.000010  mi_loss: 3.9189  train_loss: 3.9189  time: 1.7861  data: 0.0002  max mem: 20623
Train Epoch: [15]  [ 700/2023]  eta: 0:39:25  lr: 0.000010  mi_loss: 3.5657  train_loss: 3.5657  time: 1.7852  data: 0.0002  max mem: 20623
Train Epoch: [15]  [ 750/2023]  eta: 0:37:55  lr: 0.000010  mi_loss: 3.6636  train_loss: 3.6636  time: 1.7820  data: 0.0001  max mem: 20623
Train Epoch: [15]  [ 800/2023]  eta: 0:36:25  lr: 0.000010  mi_loss: 3.9004  train_loss: 3.9004  time: 1.7814  data: 0.0002  max mem: 20623
Train Epoch: [15]  [ 850/2023]  eta: 0:34:56  lr: 0.000010  mi_loss: 3.8738  train_loss: 3.8738  time: 1.7886  data: 0.0002  max mem: 20623
Train Epoch: [15]  [ 900/2023]  eta: 0:33:26  lr: 0.000010  mi_loss: 4.1656  train_loss: 4.1656  time: 1.7841  data: 0.0001  max mem: 20623
Train Epoch: [15]  [ 950/2023]  eta: 0:31:57  lr: 0.000010  mi_loss: 4.0768  train_loss: 4.0768  time: 1.7824  data: 0.0001  max mem: 20623
Train Epoch: [15]  [1000/2023]  eta: 0:30:27  lr: 0.000010  mi_loss: 3.9146  train_loss: 3.9146  time: 1.7856  data: 0.0001  max mem: 20623
Train Epoch: [15]  [1050/2023]  eta: 0:28:58  lr: 0.000010  mi_loss: 3.9834  train_loss: 3.9834  time: 1.7862  data: 0.0001  max mem: 20623
Train Epoch: [15]  [1100/2023]  eta: 0:27:29  lr: 0.000010  mi_loss: 3.9873  train_loss: 3.9873  time: 1.7889  data: 0.0001  max mem: 20623
Train Epoch: [15]  [1150/2023]  eta: 0:25:59  lr: 0.000010  mi_loss: 3.8057  train_loss: 3.8057  time: 1.7878  data: 0.0001  max mem: 20623
Train Epoch: [15]  [1200/2023]  eta: 0:24:30  lr: 0.000010  mi_loss: 4.0306  train_loss: 4.0306  time: 1.7879  data: 0.0002  max mem: 20623
Train Epoch: [15]  [1250/2023]  eta: 0:23:00  lr: 0.000010  mi_loss: 3.8283  train_loss: 3.8283  time: 1.7863  data: 0.0001  max mem: 20623
Train Epoch: [15]  [1300/2023]  eta: 0:21:31  lr: 0.000010  mi_loss: 3.7244  train_loss: 3.7244  time: 1.7827  data: 0.0001  max mem: 20623
Train Epoch: [15]  [1350/2023]  eta: 0:20:02  lr: 0.000010  mi_loss: 3.9924  train_loss: 3.9924  time: 1.7839  data: 0.0001  max mem: 20623
Train Epoch: [15]  [1400/2023]  eta: 0:18:32  lr: 0.000010  mi_loss: 3.8788  train_loss: 3.8788  time: 1.7848  data: 0.0001  max mem: 20623
Train Epoch: [15]  [1450/2023]  eta: 0:17:03  lr: 0.000010  mi_loss: 3.9484  train_loss: 3.9484  time: 1.7864  data: 0.0001  max mem: 20623
Train Epoch: [15]  [1500/2023]  eta: 0:15:34  lr: 0.000010  mi_loss: 3.8448  train_loss: 3.8448  time: 1.7842  data: 0.0001  max mem: 20623
Train Epoch: [15]  [1550/2023]  eta: 0:14:04  lr: 0.000010  mi_loss: 3.8769  train_loss: 3.8769  time: 1.7845  data: 0.0001  max mem: 20623
Train Epoch: [15]  [1600/2023]  eta: 0:12:35  lr: 0.000010  mi_loss: 3.4920  train_loss: 3.4920  time: 1.7841  data: 0.0001  max mem: 20623
Train Epoch: [15]  [1650/2023]  eta: 0:11:06  lr: 0.000010  mi_loss: 3.9733  train_loss: 3.9733  time: 1.7844  data: 0.0001  max mem: 20623
Train Epoch: [15]  [1700/2023]  eta: 0:09:36  lr: 0.000010  mi_loss: 3.7662  train_loss: 3.7662  time: 1.7798  data: 0.0001  max mem: 20623
Train Epoch: [15]  [1750/2023]  eta: 0:08:07  lr: 0.000010  mi_loss: 3.9735  train_loss: 3.9735  time: 1.7866  data: 0.0001  max mem: 20623
Train Epoch: [15]  [1800/2023]  eta: 0:06:38  lr: 0.000010  mi_loss: 4.0793  train_loss: 4.0793  time: 1.7874  data: 0.0001  max mem: 20623
Train Epoch: [15]  [1850/2023]  eta: 0:05:08  lr: 0.000010  mi_loss: 3.9018  train_loss: 3.9018  time: 1.7818  data: 0.0003  max mem: 20623
Train Epoch: [15]  [1900/2023]  eta: 0:03:39  lr: 0.000010  mi_loss: 3.9340  train_loss: 3.9340  time: 1.7864  data: 0.0001  max mem: 20623
Train Epoch: [15]  [1950/2023]  eta: 0:02:10  lr: 0.000010  mi_loss: 3.8349  train_loss: 3.8349  time: 1.7797  data: 0.0001  max mem: 20623
Train Epoch: [15]  [2000/2023]  eta: 0:00:41  lr: 0.000010  mi_loss: 3.8800  train_loss: 3.8800  time: 1.7845  data: 0.0001  max mem: 20623
Train Epoch: [15]  [2022/2023]  eta: 0:00:01  lr: 0.000010  mi_loss: 3.9238  train_loss: 3.9238  time: 1.7827  data: 0.0007  max mem: 20623
Train Epoch: [15] Total time: 1:00:13 (1.7863 s / it)
Val Epoch: [15]  [  0/225]  eta: 0:04:49  mi_loss: 3.8474  val_loss: 3.8474  accMI: 0.1611  time: 1.2857  data: 1.0477  max mem: 20623
Val Epoch: [15]  [ 50/225]  eta: 0:00:44  mi_loss: 4.0764  val_loss: 4.0764  accMI: 0.1320  time: 0.2339  data: 0.0001  max mem: 20623
Val Epoch: [15]  [100/225]  eta: 0:00:30  mi_loss: 4.2202  val_loss: 4.2202  accMI: 0.1531  time: 0.2344  data: 0.0001  max mem: 20623
Val Epoch: [15]  [150/225]  eta: 0:00:18  mi_loss: 3.8775  val_loss: 3.8775  accMI: 0.1665  time: 0.2349  data: 0.0002  max mem: 20623
Val Epoch: [15]  [200/225]  eta: 0:00:05  mi_loss: 4.1494  val_loss: 4.1494  accMI: 0.1562  time: 0.2349  data: 0.0001  max mem: 20623
Val Epoch: [15]  [224/225]  eta: 0:00:00  mi_loss: 4.1743  val_loss: 4.1743  accMI: 0.1602  time: 0.2332  data: 0.0001  max mem: 20623
Val Epoch: [15] Total time: 0:00:54 (0.2438 s / it)
epoch:15, iter:32367, 2022,  train_loss: 3.923815965652466, valid_loss: 3.9770414373609753, idiv_loss:3.9770414373609753, acc:0.16520294941133923
Averaged stats: lr: 0.0000  mi_loss: 3.8833  train_loss: 3.8833
epoch 15 3.923815965652466
Train Epoch: [16]  [   0/2023]  eta: 1:43:38  lr: 0.000010  mi_loss: 3.6885  train_loss: 3.6885  time: 3.0738  data: 0.9415  max mem: 20623
Train Epoch: [16]  [  50/2023]  eta: 0:59:39  lr: 0.000010  mi_loss: 3.7094  train_loss: 3.7094  time: 1.7869  data: 0.0001  max mem: 20623
Train Epoch: [16]  [ 100/2023]  eta: 0:57:41  lr: 0.000010  mi_loss: 3.9505  train_loss: 3.9505  time: 1.7837  data: 0.0001  max mem: 20623
Train Epoch: [16]  [ 150/2023]  eta: 0:56:03  lr: 0.000010  mi_loss: 3.6258  train_loss: 3.6258  time: 1.7890  data: 0.0001  max mem: 20623
Train Epoch: [16]  [ 200/2023]  eta: 0:54:28  lr: 0.000010  mi_loss: 4.0538  train_loss: 4.0538  time: 1.7868  data: 0.0002  max mem: 20623
Train Epoch: [16]  [ 250/2023]  eta: 0:52:57  lr: 0.000010  mi_loss: 4.1139  train_loss: 4.1139  time: 1.7904  data: 0.0001  max mem: 20623
Train Epoch: [16]  [ 300/2023]  eta: 0:51:27  lr: 0.000010  mi_loss: 4.2194  train_loss: 4.2194  time: 1.7878  data: 0.0004  max mem: 20623
Train Epoch: [16]  [ 350/2023]  eta: 0:49:56  lr: 0.000010  mi_loss: 4.1937  train_loss: 4.1937  time: 1.7883  data: 0.0003  max mem: 20623
Train Epoch: [16]  [ 400/2023]  eta: 0:48:25  lr: 0.000010  mi_loss: 3.8450  train_loss: 3.8450  time: 1.7846  data: 0.0004  max mem: 20623
Train Epoch: [16]  [ 450/2023]  eta: 0:46:55  lr: 0.000010  mi_loss: 3.6552  train_loss: 3.6552  time: 1.7876  data: 0.0003  max mem: 20623
Train Epoch: [16]  [ 500/2023]  eta: 0:45:25  lr: 0.000010  mi_loss: 3.8126  train_loss: 3.8126  time: 1.7874  data: 0.0002  max mem: 20623
Train Epoch: [16]  [ 550/2023]  eta: 0:43:55  lr: 0.000010  mi_loss: 3.6626  train_loss: 3.6626  time: 1.7897  data: 0.0002  max mem: 20623
Train Epoch: [16]  [ 600/2023]  eta: 0:42:26  lr: 0.000010  mi_loss: 4.0395  train_loss: 4.0395  time: 1.7886  data: 0.0004  max mem: 20623
Train Epoch: [16]  [ 650/2023]  eta: 0:40:56  lr: 0.000010  mi_loss: 4.1230  train_loss: 4.1230  time: 1.7884  data: 0.0001  max mem: 20623
Train Epoch: [16]  [ 700/2023]  eta: 0:39:27  lr: 0.000010  mi_loss: 4.0294  train_loss: 4.0294  time: 1.7851  data: 0.0004  max mem: 20623
Train Epoch: [16]  [ 750/2023]  eta: 0:37:57  lr: 0.000010  mi_loss: 3.9679  train_loss: 3.9679  time: 1.7877  data: 0.0005  max mem: 20623
Train Epoch: [16]  [ 800/2023]  eta: 0:36:27  lr: 0.000010  mi_loss: 3.9014  train_loss: 3.9014  time: 1.7867  data: 0.0005  max mem: 20623
Train Epoch: [16]  [ 850/2023]  eta: 0:34:58  lr: 0.000010  mi_loss: 3.7796  train_loss: 3.7796  time: 1.7881  data: 0.0004  max mem: 20623
Train Epoch: [16]  [ 900/2023]  eta: 0:33:28  lr: 0.000010  mi_loss: 4.1267  train_loss: 4.1267  time: 1.7800  data: 0.0001  max mem: 20623
Train Epoch: [16]  [ 950/2023]  eta: 0:31:58  lr: 0.000010  mi_loss: 3.7159  train_loss: 3.7159  time: 1.7871  data: 0.0001  max mem: 20623
Train Epoch: [16]  [1000/2023]  eta: 0:30:29  lr: 0.000010  mi_loss: 4.0622  train_loss: 4.0622  time: 1.7875  data: 0.0001  max mem: 20623
Train Epoch: [16]  [1050/2023]  eta: 0:28:59  lr: 0.000010  mi_loss: 3.8239  train_loss: 3.8239  time: 1.7870  data: 0.0002  max mem: 20623
Train Epoch: [16]  [1100/2023]  eta: 0:27:30  lr: 0.000010  mi_loss: 3.7911  train_loss: 3.7911  time: 1.7859  data: 0.0003  max mem: 20623
Train Epoch: [16]  [1150/2023]  eta: 0:26:01  lr: 0.000010  mi_loss: 3.7240  train_loss: 3.7240  time: 1.7852  data: 0.0003  max mem: 20623
Train Epoch: [16]  [1200/2023]  eta: 0:24:31  lr: 0.000010  mi_loss: 3.8615  train_loss: 3.8615  time: 1.7854  data: 0.0003  max mem: 20623
Train Epoch: [16]  [1250/2023]  eta: 0:23:02  lr: 0.000010  mi_loss: 3.6511  train_loss: 3.6511  time: 1.7823  data: 0.0003  max mem: 20623
Train Epoch: [16]  [1300/2023]  eta: 0:21:32  lr: 0.000010  mi_loss: 3.8182  train_loss: 3.8182  time: 1.7867  data: 0.0003  max mem: 20623
Train Epoch: [16]  [1350/2023]  eta: 0:20:03  lr: 0.000010  mi_loss: 3.8218  train_loss: 3.8218  time: 1.7866  data: 0.0003  max mem: 20623
Train Epoch: [16]  [1400/2023]  eta: 0:18:33  lr: 0.000010  mi_loss: 3.7762  train_loss: 3.7762  time: 1.7856  data: 0.0003  max mem: 20623
Train Epoch: [16]  [1450/2023]  eta: 0:17:04  lr: 0.000010  mi_loss: 4.2156  train_loss: 4.2156  time: 1.7846  data: 0.0003  max mem: 20623
Train Epoch: [16]  [1500/2023]  eta: 0:15:34  lr: 0.000010  mi_loss: 3.6005  train_loss: 3.6005  time: 1.7812  data: 0.0003  max mem: 20623
Train Epoch: [16]  [1550/2023]  eta: 0:14:05  lr: 0.000010  mi_loss: 3.7134  train_loss: 3.7134  time: 1.7889  data: 0.0003  max mem: 20623
Train Epoch: [16]  [1600/2023]  eta: 0:12:36  lr: 0.000010  mi_loss: 3.9568  train_loss: 3.9568  time: 1.7880  data: 0.0003  max mem: 20623
Train Epoch: [16]  [1650/2023]  eta: 0:11:06  lr: 0.000010  mi_loss: 3.7122  train_loss: 3.7122  time: 1.7829  data: 0.0003  max mem: 20623
Train Epoch: [16]  [1700/2023]  eta: 0:09:37  lr: 0.000010  mi_loss: 3.1658  train_loss: 3.1658  time: 1.7877  data: 0.0003  max mem: 20623
Train Epoch: [16]  [1750/2023]  eta: 0:08:07  lr: 0.000010  mi_loss: 3.6992  train_loss: 3.6992  time: 1.7853  data: 0.0003  max mem: 20623
Train Epoch: [16]  [1800/2023]  eta: 0:06:38  lr: 0.000010  mi_loss: 3.6711  train_loss: 3.6711  time: 1.7861  data: 0.0003  max mem: 20623
Train Epoch: [16]  [1850/2023]  eta: 0:05:09  lr: 0.000010  mi_loss: 3.8732  train_loss: 3.8732  time: 1.7873  data: 0.0002  max mem: 20623
Train Epoch: [16]  [1900/2023]  eta: 0:03:39  lr: 0.000010  mi_loss: 3.6875  train_loss: 3.6875  time: 1.7867  data: 0.0003  max mem: 20623
Train Epoch: [16]  [1950/2023]  eta: 0:02:10  lr: 0.000010  mi_loss: 3.7886  train_loss: 3.7886  time: 1.7834  data: 0.0001  max mem: 20623
Train Epoch: [16]  [2000/2023]  eta: 0:00:41  lr: 0.000010  mi_loss: 4.1351  train_loss: 4.1351  time: 1.7828  data: 0.0001  max mem: 20623
Train Epoch: [16]  [2022/2023]  eta: 0:00:01  lr: 0.000010  mi_loss: 3.4920  train_loss: 3.4920  time: 1.7889  data: 0.0018  max mem: 20623
Train Epoch: [16] Total time: 1:00:16 (1.7877 s / it)
Val Epoch: [16]  [  0/225]  eta: 0:04:34  mi_loss: 3.8100  val_loss: 3.8100  accMI: 0.1767  time: 1.2178  data: 0.9813  max mem: 20623
Val Epoch: [16]  [ 50/225]  eta: 0:00:44  mi_loss: 4.0600  val_loss: 4.0600  accMI: 0.1365  time: 0.2341  data: 0.0001  max mem: 20623
Val Epoch: [16]  [100/225]  eta: 0:00:30  mi_loss: 4.1879  val_loss: 4.1879  accMI: 0.1553  time: 0.2345  data: 0.0001  max mem: 20623
Val Epoch: [16]  [150/225]  eta: 0:00:18  mi_loss: 3.8421  val_loss: 3.8421  accMI: 0.1631  time: 0.2347  data: 0.0001  max mem: 20623
Val Epoch: [16]  [200/225]  eta: 0:00:05  mi_loss: 4.1549  val_loss: 4.1549  accMI: 0.1574  time: 0.2349  data: 0.0001  max mem: 20623
Val Epoch: [16]  [224/225]  eta: 0:00:00  mi_loss: 4.2262  val_loss: 4.2262  accMI: 0.1669  time: 0.2333  data: 0.0002  max mem: 20623
Val Epoch: [16] Total time: 0:00:54 (0.2429 s / it)
epoch:16, iter:34390, 2022,  train_loss: 3.4920337200164795, valid_loss: 3.9635876909891765, idiv_loss:3.9635876909891765, acc:0.165360882613394
Averaged stats: lr: 0.0000  mi_loss: 3.8627  train_loss: 3.8627
epoch 16 3.4920337200164795
Train Epoch: [17]  [   0/2023]  eta: 1:44:46  lr: 0.000010  mi_loss: 3.8437  train_loss: 3.8437  time: 3.1076  data: 1.0302  max mem: 20623
Train Epoch: [17]  [  50/2023]  eta: 0:59:33  lr: 0.000010  mi_loss: 3.8411  train_loss: 3.8411  time: 1.7859  data: 0.0001  max mem: 20623
Train Epoch: [17]  [ 100/2023]  eta: 0:57:35  lr: 0.000010  mi_loss: 3.8719  train_loss: 3.8719  time: 1.7789  data: 0.0001  max mem: 20623
Train Epoch: [17]  [ 150/2023]  eta: 0:55:58  lr: 0.000010  mi_loss: 4.1867  train_loss: 4.1867  time: 1.7864  data: 0.0001  max mem: 20623
Train Epoch: [17]  [ 200/2023]  eta: 0:54:24  lr: 0.000010  mi_loss: 3.7036  train_loss: 3.7036  time: 1.7834  data: 0.0001  max mem: 20623
Train Epoch: [17]  [ 250/2023]  eta: 0:52:52  lr: 0.000010  mi_loss: 3.6972  train_loss: 3.6972  time: 1.7841  data: 0.0001  max mem: 20623
Train Epoch: [17]  [ 300/2023]  eta: 0:51:22  lr: 0.000010  mi_loss: 3.4072  train_loss: 3.4072  time: 1.7850  data: 0.0001  max mem: 20623
Train Epoch: [17]  [ 350/2023]  eta: 0:49:52  lr: 0.000010  mi_loss: 3.5163  train_loss: 3.5163  time: 1.7872  data: 0.0001  max mem: 20623
Train Epoch: [17]  [ 400/2023]  eta: 0:48:21  lr: 0.000010  mi_loss: 3.9947  train_loss: 3.9947  time: 1.7824  data: 0.0001  max mem: 20623
Train Epoch: [17]  [ 450/2023]  eta: 0:46:52  lr: 0.000010  mi_loss: 3.6656  train_loss: 3.6656  time: 1.7846  data: 0.0001  max mem: 20623
Train Epoch: [17]  [ 500/2023]  eta: 0:45:22  lr: 0.000010  mi_loss: 3.8604  train_loss: 3.8604  time: 1.7854  data: 0.0001  max mem: 20623
Train Epoch: [17]  [ 550/2023]  eta: 0:43:52  lr: 0.000010  mi_loss: 3.3114  train_loss: 3.3114  time: 1.7900  data: 0.0001  max mem: 20623
Train Epoch: [17]  [ 600/2023]  eta: 0:42:22  lr: 0.000010  mi_loss: 3.8148  train_loss: 3.8148  time: 1.7842  data: 0.0001  max mem: 20623
Train Epoch: [17]  [ 650/2023]  eta: 0:40:53  lr: 0.000010  mi_loss: 3.7808  train_loss: 3.7808  time: 1.7872  data: 0.0001  max mem: 20623
Train Epoch: [17]  [ 700/2023]  eta: 0:39:24  lr: 0.000010  mi_loss: 3.6257  train_loss: 3.6257  time: 1.7890  data: 0.0001  max mem: 20623
Train Epoch: [17]  [ 750/2023]  eta: 0:37:54  lr: 0.000010  mi_loss: 3.9408  train_loss: 3.9408  time: 1.7866  data: 0.0001  max mem: 20623
Train Epoch: [17]  [ 800/2023]  eta: 0:36:25  lr: 0.000010  mi_loss: 3.5738  train_loss: 3.5738  time: 1.7863  data: 0.0001  max mem: 20623
Train Epoch: [17]  [ 850/2023]  eta: 0:34:56  lr: 0.000010  mi_loss: 3.7846  train_loss: 3.7846  time: 1.7866  data: 0.0001  max mem: 20623
Train Epoch: [17]  [ 900/2023]  eta: 0:33:26  lr: 0.000010  mi_loss: 3.9628  train_loss: 3.9628  time: 1.7847  data: 0.0001  max mem: 20623
Train Epoch: [17]  [ 950/2023]  eta: 0:31:57  lr: 0.000010  mi_loss: 3.8332  train_loss: 3.8332  time: 1.7830  data: 0.0001  max mem: 20623
Train Epoch: [17]  [1000/2023]  eta: 0:30:27  lr: 0.000010  mi_loss: 4.0840  train_loss: 4.0840  time: 1.7864  data: 0.0001  max mem: 20623
Train Epoch: [17]  [1050/2023]  eta: 0:28:58  lr: 0.000010  mi_loss: 4.1723  train_loss: 4.1723  time: 1.7866  data: 0.0001  max mem: 20623
Train Epoch: [17]  [1100/2023]  eta: 0:27:29  lr: 0.000010  mi_loss: 3.9356  train_loss: 3.9356  time: 1.7846  data: 0.0001  max mem: 20623
Train Epoch: [17]  [1150/2023]  eta: 0:25:59  lr: 0.000010  mi_loss: 3.5731  train_loss: 3.5731  time: 1.7833  data: 0.0001  max mem: 20623
Train Epoch: [17]  [1200/2023]  eta: 0:24:30  lr: 0.000010  mi_loss: 3.8475  train_loss: 3.8475  time: 1.7856  data: 0.0001  max mem: 20623
Train Epoch: [17]  [1250/2023]  eta: 0:23:00  lr: 0.000010  mi_loss: 3.8634  train_loss: 3.8634  time: 1.7858  data: 0.0001  max mem: 20623
Train Epoch: [17]  [1300/2023]  eta: 0:21:31  lr: 0.000010  mi_loss: 3.8696  train_loss: 3.8696  time: 1.7847  data: 0.0001  max mem: 20623
Train Epoch: [17]  [1350/2023]  eta: 0:20:02  lr: 0.000010  mi_loss: 3.8102  train_loss: 3.8102  time: 1.7818  data: 0.0001  max mem: 20623
Train Epoch: [17]  [1400/2023]  eta: 0:18:32  lr: 0.000010  mi_loss: 4.1300  train_loss: 4.1300  time: 1.7815  data: 0.0001  max mem: 20623
Train Epoch: [17]  [1450/2023]  eta: 0:17:03  lr: 0.000010  mi_loss: 4.0569  train_loss: 4.0569  time: 1.7829  data: 0.0001  max mem: 20623
Train Epoch: [17]  [1500/2023]  eta: 0:15:34  lr: 0.000010  mi_loss: 3.7491  train_loss: 3.7491  time: 1.7832  data: 0.0001  max mem: 20623
Train Epoch: [17]  [1550/2023]  eta: 0:14:04  lr: 0.000010  mi_loss: 4.0891  train_loss: 4.0891  time: 1.7852  data: 0.0001  max mem: 20623
Train Epoch: [17]  [1600/2023]  eta: 0:12:35  lr: 0.000010  mi_loss: 3.6398  train_loss: 3.6398  time: 1.7839  data: 0.0001  max mem: 20623
Train Epoch: [17]  [1650/2023]  eta: 0:11:06  lr: 0.000010  mi_loss: 3.9031  train_loss: 3.9031  time: 1.7867  data: 0.0001  max mem: 20623
Train Epoch: [17]  [1700/2023]  eta: 0:09:36  lr: 0.000010  mi_loss: 4.0018  train_loss: 4.0018  time: 1.7874  data: 0.0001  max mem: 20623
Train Epoch: [17]  [1750/2023]  eta: 0:08:07  lr: 0.000010  mi_loss: 3.6623  train_loss: 3.6623  time: 1.7852  data: 0.0001  max mem: 20623
Train Epoch: [17]  [1800/2023]  eta: 0:06:38  lr: 0.000010  mi_loss: 3.7959  train_loss: 3.7959  time: 1.7840  data: 0.0001  max mem: 20623
Train Epoch: [17]  [1850/2023]  eta: 0:05:08  lr: 0.000010  mi_loss: 4.0562  train_loss: 4.0562  time: 1.7840  data: 0.0001  max mem: 20623
Train Epoch: [17]  [1900/2023]  eta: 0:03:39  lr: 0.000010  mi_loss: 3.7616  train_loss: 3.7616  time: 1.7842  data: 0.0001  max mem: 20623
Train Epoch: [17]  [1950/2023]  eta: 0:02:10  lr: 0.000010  mi_loss: 3.6585  train_loss: 3.6585  time: 1.7850  data: 0.0002  max mem: 20623
Train Epoch: [17]  [2000/2023]  eta: 0:00:41  lr: 0.000010  mi_loss: 3.4517  train_loss: 3.4517  time: 1.7843  data: 0.0001  max mem: 20623
Train Epoch: [17]  [2022/2023]  eta: 0:00:01  lr: 0.000010  mi_loss: 3.7132  train_loss: 3.7132  time: 1.7876  data: 0.0006  max mem: 20623
Train Epoch: [17] Total time: 1:00:13 (1.7864 s / it)
Val Epoch: [17]  [  0/225]  eta: 0:04:28  mi_loss: 3.7880  val_loss: 3.7880  accMI: 0.1745  time: 1.1912  data: 0.9536  max mem: 20623
Val Epoch: [17]  [ 50/225]  eta: 0:00:44  mi_loss: 4.0598  val_loss: 4.0598  accMI: 0.1398  time: 0.2345  data: 0.0002  max mem: 20623
Val Epoch: [17]  [100/225]  eta: 0:00:30  mi_loss: 4.2311  val_loss: 4.2311  accMI: 0.1542  time: 0.2347  data: 0.0002  max mem: 20623
Val Epoch: [17]  [150/225]  eta: 0:00:18  mi_loss: 3.8104  val_loss: 3.8104  accMI: 0.1799  time: 0.2353  data: 0.0002  max mem: 20623
Val Epoch: [17]  [200/225]  eta: 0:00:05  mi_loss: 4.2257  val_loss: 4.2257  accMI: 0.1596  time: 0.2356  data: 0.0002  max mem: 20623
Val Epoch: [17]  [224/225]  eta: 0:00:00  mi_loss: 4.1889  val_loss: 4.1889  accMI: 0.1588  time: 0.2337  data: 0.0003  max mem: 20623
Val Epoch: [17] Total time: 0:00:54 (0.2433 s / it)
epoch:17, iter:36413, 2022,  train_loss: 3.7131669521331787, valid_loss: 3.9541097270117866, idiv_loss:3.9541097270117866, acc:0.16677184422810873
Averaged stats: lr: 0.0000  mi_loss: 3.8432  train_loss: 3.8432
epoch 17 3.7131669521331787
Train Epoch: [18]  [   0/2023]  eta: 1:42:44  lr: 0.000010  mi_loss: 3.8558  train_loss: 3.8558  time: 3.0473  data: 0.9596  max mem: 20623
Train Epoch: [18]  [  50/2023]  eta: 0:59:35  lr: 0.000010  mi_loss: 3.8196  train_loss: 3.8196  time: 1.7850  data: 0.0001  max mem: 20623
Train Epoch: [18]  [ 100/2023]  eta: 0:57:40  lr: 0.000010  mi_loss: 3.8130  train_loss: 3.8130  time: 1.7876  data: 0.0001  max mem: 20623
Train Epoch: [18]  [ 150/2023]  eta: 0:56:02  lr: 0.000010  mi_loss: 3.9133  train_loss: 3.9133  time: 1.7870  data: 0.0001  max mem: 20623
Train Epoch: [18]  [ 200/2023]  eta: 0:54:29  lr: 0.000010  mi_loss: 3.9544  train_loss: 3.9544  time: 1.7889  data: 0.0001  max mem: 20623
Train Epoch: [18]  [ 250/2023]  eta: 0:52:56  lr: 0.000010  mi_loss: 3.8901  train_loss: 3.8901  time: 1.7835  data: 0.0001  max mem: 20623
Train Epoch: [18]  [ 300/2023]  eta: 0:51:24  lr: 0.000010  mi_loss: 3.9901  train_loss: 3.9901  time: 1.7851  data: 0.0001  max mem: 20623
Train Epoch: [18]  [ 350/2023]  eta: 0:49:54  lr: 0.000010  mi_loss: 3.5796  train_loss: 3.5796  time: 1.7871  data: 0.0001  max mem: 20623
Train Epoch: [18]  [ 400/2023]  eta: 0:48:24  lr: 0.000010  mi_loss: 4.2497  train_loss: 4.2497  time: 1.7888  data: 0.0001  max mem: 20623
Train Epoch: [18]  [ 450/2023]  eta: 0:46:54  lr: 0.000010  mi_loss: 3.6388  train_loss: 3.6388  time: 1.7862  data: 0.0001  max mem: 20623
Train Epoch: [18]  [ 500/2023]  eta: 0:45:24  lr: 0.000010  mi_loss: 3.8993  train_loss: 3.8993  time: 1.7896  data: 0.0001  max mem: 20623
Train Epoch: [18]  [ 550/2023]  eta: 0:43:54  lr: 0.000010  mi_loss: 4.0539  train_loss: 4.0539  time: 1.7857  data: 0.0001  max mem: 20623
Train Epoch: [18]  [ 600/2023]  eta: 0:42:25  lr: 0.000010  mi_loss: 3.9194  train_loss: 3.9194  time: 1.7852  data: 0.0001  max mem: 20623
Train Epoch: [18]  [ 650/2023]  eta: 0:40:55  lr: 0.000010  mi_loss: 4.3493  train_loss: 4.3493  time: 1.7857  data: 0.0001  max mem: 20623
Train Epoch: [18]  [ 700/2023]  eta: 0:39:26  lr: 0.000010  mi_loss: 3.8180  train_loss: 3.8180  time: 1.7887  data: 0.0001  max mem: 20623
Train Epoch: [18]  [ 750/2023]  eta: 0:37:56  lr: 0.000010  mi_loss: 3.7042  train_loss: 3.7042  time: 1.7868  data: 0.0002  max mem: 20623
Train Epoch: [18]  [ 800/2023]  eta: 0:36:26  lr: 0.000010  mi_loss: 3.7512  train_loss: 3.7512  time: 1.7868  data: 0.0001  max mem: 20623
Train Epoch: [18]  [ 850/2023]  eta: 0:34:57  lr: 0.000010  mi_loss: 3.8926  train_loss: 3.8926  time: 1.7872  data: 0.0001  max mem: 20623
Train Epoch: [18]  [ 900/2023]  eta: 0:33:27  lr: 0.000010  mi_loss: 4.0174  train_loss: 4.0174  time: 1.7852  data: 0.0003  max mem: 20623
Train Epoch: [18]  [ 950/2023]  eta: 0:31:58  lr: 0.000010  mi_loss: 3.4808  train_loss: 3.4808  time: 1.7851  data: 0.0003  max mem: 20623
Train Epoch: [18]  [1000/2023]  eta: 0:30:28  lr: 0.000010  mi_loss: 3.8832  train_loss: 3.8832  time: 1.7833  data: 0.0003  max mem: 20623
Train Epoch: [18]  [1050/2023]  eta: 0:28:59  lr: 0.000010  mi_loss: 3.6793  train_loss: 3.6793  time: 1.7863  data: 0.0003  max mem: 20623
Train Epoch: [18]  [1100/2023]  eta: 0:27:29  lr: 0.000010  mi_loss: 3.7018  train_loss: 3.7018  time: 1.7846  data: 0.0002  max mem: 20623
Train Epoch: [18]  [1150/2023]  eta: 0:26:00  lr: 0.000010  mi_loss: 3.7742  train_loss: 3.7742  time: 1.7882  data: 0.0003  max mem: 20623
Train Epoch: [18]  [1200/2023]  eta: 0:24:30  lr: 0.000010  mi_loss: 4.0182  train_loss: 4.0182  time: 1.7847  data: 0.0003  max mem: 20623
Train Epoch: [18]  [1250/2023]  eta: 0:23:01  lr: 0.000010  mi_loss: 3.8716  train_loss: 3.8716  time: 1.7861  data: 0.0003  max mem: 20623
Train Epoch: [18]  [1300/2023]  eta: 0:21:32  lr: 0.000010  mi_loss: 3.9287  train_loss: 3.9287  time: 1.7820  data: 0.0003  max mem: 20623
Train Epoch: [18]  [1350/2023]  eta: 0:20:02  lr: 0.000010  mi_loss: 3.5921  train_loss: 3.5921  time: 1.7893  data: 0.0003  max mem: 20623
Train Epoch: [18]  [1400/2023]  eta: 0:18:33  lr: 0.000010  mi_loss: 3.8696  train_loss: 3.8696  time: 1.7867  data: 0.0002  max mem: 20623
Train Epoch: [18]  [1450/2023]  eta: 0:17:03  lr: 0.000010  mi_loss: 3.8882  train_loss: 3.8882  time: 1.7871  data: 0.0003  max mem: 20623
Train Epoch: [18]  [1500/2023]  eta: 0:15:34  lr: 0.000010  mi_loss: 3.8816  train_loss: 3.8816  time: 1.7849  data: 0.0003  max mem: 20623
Train Epoch: [18]  [1550/2023]  eta: 0:14:05  lr: 0.000010  mi_loss: 3.9022  train_loss: 3.9022  time: 1.7876  data: 0.0002  max mem: 20623
Train Epoch: [18]  [1600/2023]  eta: 0:12:35  lr: 0.000010  mi_loss: 3.9796  train_loss: 3.9796  time: 1.7852  data: 0.0003  max mem: 20623
Train Epoch: [18]  [1650/2023]  eta: 0:11:06  lr: 0.000010  mi_loss: 3.8461  train_loss: 3.8461  time: 1.7871  data: 0.0003  max mem: 20623
Train Epoch: [18]  [1700/2023]  eta: 0:09:37  lr: 0.000010  mi_loss: 3.8182  train_loss: 3.8182  time: 1.7850  data: 0.0003  max mem: 20623
Train Epoch: [18]  [1750/2023]  eta: 0:08:07  lr: 0.000010  mi_loss: 3.5169  train_loss: 3.5169  time: 1.7846  data: 0.0002  max mem: 20623
Train Epoch: [18]  [1800/2023]  eta: 0:06:38  lr: 0.000010  mi_loss: 3.7437  train_loss: 3.7437  time: 1.7878  data: 0.0003  max mem: 20623
Train Epoch: [18]  [1850/2023]  eta: 0:05:09  lr: 0.000010  mi_loss: 3.9448  train_loss: 3.9448  time: 1.7868  data: 0.0003  max mem: 20623
Train Epoch: [18]  [1900/2023]  eta: 0:03:39  lr: 0.000010  mi_loss: 4.0004  train_loss: 4.0004  time: 1.7839  data: 0.0003  max mem: 20623
Train Epoch: [18]  [1950/2023]  eta: 0:02:10  lr: 0.000010  mi_loss: 3.5404  train_loss: 3.5404  time: 1.7836  data: 0.0002  max mem: 20623
Train Epoch: [18]  [2000/2023]  eta: 0:00:41  lr: 0.000010  mi_loss: 3.5148  train_loss: 3.5148  time: 1.7870  data: 0.0001  max mem: 20623
Train Epoch: [18]  [2022/2023]  eta: 0:00:01  lr: 0.000010  mi_loss: 3.5744  train_loss: 3.5744  time: 1.7861  data: 0.0005  max mem: 20623
Train Epoch: [18] Total time: 1:00:16 (1.7875 s / it)
Val Epoch: [18]  [  0/225]  eta: 0:04:32  mi_loss: 3.8060  val_loss: 3.8060  accMI: 0.1711  time: 1.2133  data: 0.9749  max mem: 20623
Val Epoch: [18]  [ 50/225]  eta: 0:00:44  mi_loss: 4.0648  val_loss: 4.0648  accMI: 0.1331  time: 0.2350  data: 0.0003  max mem: 20623
Val Epoch: [18]  [100/225]  eta: 0:00:30  mi_loss: 4.2570  val_loss: 4.2570  accMI: 0.1464  time: 0.2352  data: 0.0003  max mem: 20623
Val Epoch: [18]  [150/225]  eta: 0:00:18  mi_loss: 3.8052  val_loss: 3.8052  accMI: 0.1799  time: 0.2346  data: 0.0002  max mem: 20623
Val Epoch: [18]  [200/225]  eta: 0:00:05  mi_loss: 4.1756  val_loss: 4.1756  accMI: 0.1674  time: 0.2351  data: 0.0002  max mem: 20623
Val Epoch: [18]  [224/225]  eta: 0:00:00  mi_loss: 4.1477  val_loss: 4.1477  accMI: 0.1615  time: 0.2335  data: 0.0003  max mem: 20623
Val Epoch: [18] Total time: 0:00:54 (0.2433 s / it)
epoch:18, iter:38436, 2022,  train_loss: 3.5744364261627197, valid_loss: 3.9456825711992054, idiv_loss:3.9456825711992054, acc:0.16818118035793306
Averaged stats: lr: 0.0000  mi_loss: 3.8232  train_loss: 3.8232
epoch 18 3.5744364261627197
Train Epoch: [19]  [   0/2023]  eta: 1:43:13  lr: 0.000010  mi_loss: 3.6151  train_loss: 3.6151  time: 3.0617  data: 0.9509  max mem: 20623
Train Epoch: [19]  [  50/2023]  eta: 0:59:31  lr: 0.000010  mi_loss: 3.5843  train_loss: 3.5843  time: 1.7864  data: 0.0001  max mem: 20623
Train Epoch: [19]  [ 100/2023]  eta: 0:57:36  lr: 0.000010  mi_loss: 4.0011  train_loss: 4.0011  time: 1.7854  data: 0.0001  max mem: 20623
Train Epoch: [19]  [ 150/2023]  eta: 0:55:57  lr: 0.000010  mi_loss: 3.3898  train_loss: 3.3898  time: 1.7845  data: 0.0001  max mem: 20623
Train Epoch: [19]  [ 200/2023]  eta: 0:54:23  lr: 0.000010  mi_loss: 3.8667  train_loss: 3.8667  time: 1.7847  data: 0.0001  max mem: 20623
Train Epoch: [19]  [ 250/2023]  eta: 0:52:51  lr: 0.000010  mi_loss: 3.7831  train_loss: 3.7831  time: 1.7853  data: 0.0001  max mem: 20623
Train Epoch: [19]  [ 300/2023]  eta: 0:51:20  lr: 0.000010  mi_loss: 3.7763  train_loss: 3.7763  time: 1.7835  data: 0.0001  max mem: 20623
Train Epoch: [19]  [ 350/2023]  eta: 0:49:50  lr: 0.000010  mi_loss: 3.7021  train_loss: 3.7021  time: 1.7867  data: 0.0001  max mem: 20623
Train Epoch: [19]  [ 400/2023]  eta: 0:48:21  lr: 0.000010  mi_loss: 3.2530  train_loss: 3.2530  time: 1.7869  data: 0.0001  max mem: 20623
Train Epoch: [19]  [ 450/2023]  eta: 0:46:50  lr: 0.000010  mi_loss: 3.7614  train_loss: 3.7614  time: 1.7839  data: 0.0002  max mem: 20623
Train Epoch: [19]  [ 500/2023]  eta: 0:45:20  lr: 0.000010  mi_loss: 3.6279  train_loss: 3.6279  time: 1.7838  data: 0.0001  max mem: 20623
Train Epoch: [19]  [ 550/2023]  eta: 0:43:51  lr: 0.000010  mi_loss: 3.9036  train_loss: 3.9036  time: 1.7842  data: 0.0001  max mem: 20623
Train Epoch: [19]  [ 600/2023]  eta: 0:42:21  lr: 0.000010  mi_loss: 3.7136  train_loss: 3.7136  time: 1.7847  data: 0.0001  max mem: 20623
Train Epoch: [19]  [ 650/2023]  eta: 0:40:52  lr: 0.000010  mi_loss: 3.7113  train_loss: 3.7113  time: 1.7846  data: 0.0001  max mem: 20623
Train Epoch: [19]  [ 700/2023]  eta: 0:39:22  lr: 0.000010  mi_loss: 3.8795  train_loss: 3.8795  time: 1.7872  data: 0.0001  max mem: 20623
Train Epoch: [19]  [ 750/2023]  eta: 0:37:53  lr: 0.000010  mi_loss: 3.8052  train_loss: 3.8052  time: 1.7859  data: 0.0001  max mem: 20623
Train Epoch: [19]  [ 800/2023]  eta: 0:36:23  lr: 0.000010  mi_loss: 3.8030  train_loss: 3.8030  time: 1.7809  data: 0.0001  max mem: 20623
Train Epoch: [19]  [ 850/2023]  eta: 0:34:54  lr: 0.000010  mi_loss: 3.9639  train_loss: 3.9639  time: 1.7808  data: 0.0001  max mem: 20623
Train Epoch: [19]  [ 900/2023]  eta: 0:33:25  lr: 0.000010  mi_loss: 3.6372  train_loss: 3.6372  time: 1.7887  data: 0.0002  max mem: 20623
Train Epoch: [19]  [ 950/2023]  eta: 0:31:55  lr: 0.000010  mi_loss: 3.9597  train_loss: 3.9597  time: 1.7888  data: 0.0002  max mem: 20623
Train Epoch: [19]  [1000/2023]  eta: 0:30:26  lr: 0.000010  mi_loss: 3.8234  train_loss: 3.8234  time: 1.7845  data: 0.0002  max mem: 20623
Train Epoch: [19]  [1050/2023]  eta: 0:28:57  lr: 0.000010  mi_loss: 3.9674  train_loss: 3.9674  time: 1.7848  data: 0.0002  max mem: 20623
Train Epoch: [19]  [1100/2023]  eta: 0:27:27  lr: 0.000010  mi_loss: 3.6433  train_loss: 3.6433  time: 1.7829  data: 0.0002  max mem: 20623
Train Epoch: [19]  [1150/2023]  eta: 0:25:58  lr: 0.000010  mi_loss: 3.9859  train_loss: 3.9859  time: 1.7814  data: 0.0002  max mem: 20623
Train Epoch: [19]  [1200/2023]  eta: 0:24:29  lr: 0.000010  mi_loss: 3.7884  train_loss: 3.7884  time: 1.7869  data: 0.0002  max mem: 20623
Train Epoch: [19]  [1250/2023]  eta: 0:22:59  lr: 0.000010  mi_loss: 3.7266  train_loss: 3.7266  time: 1.7832  data: 0.0002  max mem: 20623
Train Epoch: [19]  [1300/2023]  eta: 0:21:30  lr: 0.000010  mi_loss: 4.0253  train_loss: 4.0253  time: 1.7830  data: 0.0002  max mem: 20623
Train Epoch: [19]  [1350/2023]  eta: 0:20:01  lr: 0.000010  mi_loss: 3.7602  train_loss: 3.7602  time: 1.7808  data: 0.0002  max mem: 20623
Train Epoch: [19]  [1400/2023]  eta: 0:18:32  lr: 0.000010  mi_loss: 3.8187  train_loss: 3.8187  time: 1.7837  data: 0.0002  max mem: 20623
Train Epoch: [19]  [1450/2023]  eta: 0:17:02  lr: 0.000010  mi_loss: 4.1405  train_loss: 4.1405  time: 1.7794  data: 0.0002  max mem: 20623
Train Epoch: [19]  [1500/2023]  eta: 0:15:33  lr: 0.000010  mi_loss: 3.7051  train_loss: 3.7051  time: 1.7839  data: 0.0002  max mem: 20623
Train Epoch: [19]  [1550/2023]  eta: 0:14:04  lr: 0.000010  mi_loss: 3.9748  train_loss: 3.9748  time: 1.7801  data: 0.0002  max mem: 20623
Train Epoch: [19]  [1600/2023]  eta: 0:12:34  lr: 0.000010  mi_loss: 3.9150  train_loss: 3.9150  time: 1.7855  data: 0.0001  max mem: 20623
Train Epoch: [19]  [1650/2023]  eta: 0:11:05  lr: 0.000010  mi_loss: 3.6876  train_loss: 3.6876  time: 1.7826  data: 0.0002  max mem: 20623
Train Epoch: [19]  [1700/2023]  eta: 0:09:36  lr: 0.000010  mi_loss: 3.9604  train_loss: 3.9604  time: 1.7858  data: 0.0002  max mem: 20623
Train Epoch: [19]  [1750/2023]  eta: 0:08:07  lr: 0.000010  mi_loss: 3.7509  train_loss: 3.7509  time: 1.7816  data: 0.0001  max mem: 20623
Train Epoch: [19]  [1800/2023]  eta: 0:06:37  lr: 0.000010  mi_loss: 3.7440  train_loss: 3.7440  time: 1.7814  data: 0.0002  max mem: 20623
Train Epoch: [19]  [1850/2023]  eta: 0:05:08  lr: 0.000010  mi_loss: 3.4622  train_loss: 3.4622  time: 1.7820  data: 0.0002  max mem: 20623
Train Epoch: [19]  [1900/2023]  eta: 0:03:39  lr: 0.000010  mi_loss: 3.9568  train_loss: 3.9568  time: 1.7827  data: 0.0002  max mem: 20623
Train Epoch: [19]  [1950/2023]  eta: 0:02:10  lr: 0.000010  mi_loss: 3.7741  train_loss: 3.7741  time: 1.7811  data: 0.0002  max mem: 20623
Train Epoch: [19]  [2000/2023]  eta: 0:00:41  lr: 0.000010  mi_loss: 4.1982  train_loss: 4.1982  time: 1.7841  data: 0.0002  max mem: 20623
Train Epoch: [19]  [2022/2023]  eta: 0:00:01  lr: 0.000010  mi_loss: 3.9614  train_loss: 3.9614  time: 1.7828  data: 0.0006  max mem: 20623
Train Epoch: [19] Total time: 1:00:11 (1.7850 s / it)
Val Epoch: [19]  [  0/225]  eta: 0:04:40  mi_loss: 3.8260  val_loss: 3.8260  accMI: 0.1779  time: 1.2471  data: 1.0090  max mem: 20623
Val Epoch: [19]  [ 50/225]  eta: 0:00:44  mi_loss: 4.0349  val_loss: 4.0349  accMI: 0.1353  time: 0.2341  data: 0.0002  max mem: 20623
Val Epoch: [19]  [100/225]  eta: 0:00:30  mi_loss: 4.1983  val_loss: 4.1983  accMI: 0.1575  time: 0.2345  data: 0.0001  max mem: 20623
Val Epoch: [19]  [150/225]  eta: 0:00:18  mi_loss: 3.8395  val_loss: 3.8395  accMI: 0.1844  time: 0.2347  data: 0.0002  max mem: 20623
Val Epoch: [19]  [200/225]  eta: 0:00:05  mi_loss: 4.1486  val_loss: 4.1486  accMI: 0.1629  time: 0.2350  data: 0.0001  max mem: 20623
Val Epoch: [19]  [224/225]  eta: 0:00:00  mi_loss: 4.0933  val_loss: 4.0933  accMI: 0.1615  time: 0.2333  data: 0.0002  max mem: 20623
Val Epoch: [19] Total time: 0:00:54 (0.2430 s / it)
epoch:19, iter:40459, 2022,  train_loss: 3.9614009857177734, valid_loss: 3.9347984398735893, idiv_loss:3.9347984398735893, acc:0.16933220581875907
Averaged stats: lr: 0.0000  mi_loss: 3.8043  train_loss: 3.8043
epoch 19 3.9614009857177734
Training time 20:24:36
ai-platform-wlf1-ge10-1:33508:33549 [2] NCCL INFO [Service thread] Connection closed by localRank 2
ai-platform-wlf1-ge10-1:33508:33508 [2] NCCL INFO comm 0x427895e0 rank 2 nranks 4 cudaDev 2 busId 81000 - Abort COMPLETE
ai-platform-wlf1-ge10-1:33506:33548 [0] NCCL INFO [Service thread] Connection closed by localRank 0
ai-platform-wlf1-ge10-1:33506:33506 [0] NCCL INFO comm 0x42b44e70 rank 0 nranks 4 cudaDev 0 busId 1000 - Abort COMPLETE
ai-platform-wlf1-ge10-1:33507:33550 [1] NCCL INFO [Service thread] Connection closed by localRank 1
ai-platform-wlf1-ge10-1:33507:33507 [1] NCCL INFO comm 0x44e6cfd0 rank 1 nranks 4 cudaDev 1 busId 24000 - Abort COMPLETE
ai-platform-wlf1-ge10-1:33510:33547 [3] NCCL INFO [Service thread] Connection closed by localRank 3
ai-platform-wlf1-ge10-1:33510:33510 [3] NCCL INFO comm 0x42a82a10 rank 3 nranks 4 cudaDev 3 busId e1000 - Abort COMPLETE
